{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Project 4\n",
    "\n",
    "## Python-specific learning objectives\n",
    "\n",
    "In addition to the learning objectives for this project, in this section you will learn how to convert (reshape) data from wide to long format and vice versa.\n",
    "\n",
    "## Getting started in Python\n",
    "\n",
    "For this project, you will need the following packages:\n",
    "\n",
    "- **pandas** for data analysis\n",
    "- **matplotlib** for data visualisation\n",
    "- **numpy** for numerical methods\n",
    "\n",
    "You'll also be using the **warnings**, **pathblib**, and **os** packages, but these come built-in with Python.\n",
    "\n",
    "Remember, you can install packages in Visual Studio Code's integrated terminal (click \"View > Terminal\") by running `conda install packagename` (if using the Anaconda distribution of Python) or `pip install packagename` if not.\n",
    "\n",
    "Once you have the Python packages installed, you will need to import them into your Python session‚Äîand configure any other initial settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Set the plot style for prettier charts:\n",
    "plt.style.use(\n",
    "    \"https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt\"\n",
    ")\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 3]\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "theme_set(theme_seaborn)\n",
    "\n",
    "# Ignore warnings to make nice output\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Go to the United Nations‚Äô [National Accounts Main Aggregates Database website](https://tinyco.re/7226184). On the right-hand side of the page, under ‚ÄòData Availability‚Äô, click ‚ÄòDownloads‚Äô.\n",
    "- Under the subheading ‚ÄòGDP and its breakdown at constant 2015 prices in US Dollars‚Äô, select the Excel file ‚ÄòAll countries for all years ‚Äì sorted alphabetically‚Äô.\n",
    "- Save it in a subfolder of the directory you are coding in such that that the relative path is `data/Download-GDPconstant-USD-all.xlsx`.\n",
    "\n",
    "## Python Walkthrough 4.1\n",
    "\n",
    "**Importing the Excel file (`.xlsx` or `.xls`) into Python**\n",
    "\n",
    "First, make sure you move the saved the data to a folder called `data` that is a subfolder of your working directory. The working directory is the folder that your code 'starts' in, and the one that you open when you start Visual Studio Code. Let's say you called it `core`, then the file and folder structure of your working directory would look like this:\n",
    "\n",
    "```bash\n",
    "üìÅ core\n",
    "‚îÇ‚îÄ‚îÄüìÅdata\n",
    "   ‚îî‚îÄ‚îÄDownload-GDPconstant-USD-all.xlsx\n",
    "‚îÇ‚îÄ‚îÄempirical_project_4.py\n",
    "```\n",
    "\n",
    "This is similar to what you should see in Visual Studio Code under the explorer tab (although the working directory, `core`, won't appear). You can check your working directory by running\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.getcwd()\n",
    "```\n",
    "\n",
    "in Visual Studio Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before importing the file into Python, open the file in Excel, OpenOffice, LibreOffice, or Numbers to see how the data is organized in the spreadsheet, and note that:\n",
    "\n",
    "- There is a heading that we don‚Äôt need, followed by a blank row.\n",
    "- The data we need starts on row three.\n",
    "\n",
    "Armed with this knowledge, we can import the data using the `Path` module to create the path to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(Path(\"data/Download-GDPconstant-USD-all.xlsx\"), skiprows=2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.2\n",
    "\n",
    "**Making a frequency table**\n",
    "\n",
    "We want to create a table showing how many years of `Final consumption expenditure` data are available for each country.\n",
    "\n",
    "Looking at the dataset‚Äôs current format, you can see that countries and indicators (for example, `Afghanistan` and `Final consumption expenditure`) are row variables, while year is the column variable. This data is organized in ‚Äòwide‚Äô format (each individual‚Äôs information is in a single row).\n",
    "\n",
    "For many data operations and making charts it is more convenient to have indicators as column variables, so we would like `Final consumption expenditure` to be a column variable, and year to be the row variable. Each observation would represent the value of an indicator for a particular country and year. This data is organized in ‚Äòlong‚Äô format (each individual‚Äôs information is in multiple rows). This is also called 'tidy' data and it can be recognised by having variable per column and one observation per row. Many data scientists consider keeping data in tidy format good practice.\n",
    "\n",
    "To change data from wide to long format, we use the `pd.melt` method. The `melt` method is very powerful and useful, as you will find many large datasets are in wide format. In this case, `pd.melt` takes the data from all columns not specified as being `id_vars` (via a list of column names), and uses them to create two new columns: one contains the name of the row variable created from the former column names, which is the year here; we can set that new column's name with `var_name=\"year\"`. The second new column contains the values that were in the columns we unpivoted and is automatically given the name `value`. (We could have set a new name for this column by passing `value_name=` too.)\n",
    "\n",
    "Compare `df_long` to the wider `df` to understand how the melt command works. To learn more about organizing data in Python, see the [Working with Data](https://aeturrell.github.io/coding-for-economists/data-intro.html) section of 'Coding for Economists'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.melt(\n",
    "    df, id_vars=[\"Area/CountryID\", \"Area/Country\", \"IndicatorName\"], var_name=\"year\"\n",
    ")\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the required table, we only need `Final consumption expenditure` of each country, which we extract using the `.loc` function. We'd like all columns so we pass the condition in the first position of `.loc` and leave the second entry as `:` for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = df_long.loc[df_long[\"IndicatorName\"] == \"Final consumption expenditure\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count = cons.groupby(\"Area/Country\").agg(available_years=(\"year\", \"count\"))\n",
    "year_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating the code in words: Take the variable `cons` and group the observations by area and country (`.groupby(Area/Country\")`), then take this result and aggregate `.agg` it such that a new variable called available years (`available_years=`) is created that sees the column year counted (`(\"year\", \"count\")`).\n",
    "\n",
    "Now we can establish how many of the 250 countries and areas in the dataset have complete information. A dataset is complete if it has the maximum number of available observations (given by `year_count[\"available_years\"].max()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(year_count[\"available_years\"] == year_count[\"available_years\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the full set of data are available for all countries and areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.3\n",
    "\n",
    "**Creating new variables**\n",
    "\n",
    "We will use Brazil, the US, and China as examples.\n",
    "\n",
    "Before we select these three countries, we will calculate the net exports (exports minus imports) for all countries, as we need that information in Python walkthrough 4.4. We will also shorten the names of the variables we need, to make the code easier to read. We will use a dictionary to map names into shorter formats. A dictionary is a built-in object type in Python and always has the structure `{key1: value1, key2: value2, ...}` where the keys and values could have any type (eg string, int, dataframe). In our case, both keys and values will be strings. We will use a convention for our naming that is known as \"snake case\". This means all lower case with spaces replaced by underscores (it looks a bit like a snake!). There are packages that will auto-rename long variables for you, but let's see how to do it manually here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_names_dict = {\n",
    "    \"Final consumption expenditure\": \"final_expenditure\",\n",
    "    \"Household consumption expenditure (including Non-profit institutions serving households)\": \"hh_expenditure\",\n",
    "    \"General government final consumption expenditure\": \"gov_expenditure\",\n",
    "    \"Gross capital formation\": \"capital\",\n",
    "    \"Imports of goods and services\": \"imports\",\n",
    "    \"Exports of goods and services\": \"exports\",\n",
    "}\n",
    "# rename these values\n",
    "df_long[\"IndicatorName\"] = df_long[\"IndicatorName\"].replace(short_names_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_long` still has several rows for a particular country and year (one for each indicator). We will reshape this data using the `.pivot` method to ensure that we have only one row per country/area and per year. Note that `pivot` preserves the list of columns we pass as the `index=` and pivots the columns we pass to `columns=` out so that they are wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = df_long.pivot(\n",
    "    index=[\"Area/CountryID\", \"Area/Country\", \"year\"], columns=[\"IndicatorName\"]\n",
    ")\n",
    "df_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a `net_exports` column based on the existing columns (exports - imports), and we can know that this will be a unique country/area and year combination for each row. First we need to drop the top level of the column index, which is currently called `value`: we don't need this anymore. This will allow for direct access to the `exports` and `imports` columns. We'll also reset the index to row numbers rather than those three columns we used in the pivot. We'll also remove the name of the columns as we won't need that any longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.columns = df_table.columns.droplevel()\n",
    "df_table = df_table.reset_index()\n",
    "df_table.columns.name = \"\"\n",
    "df_table[\"net_exports\"] = df_table[\"exports\"] - df_table[\"imports\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us select our three chosen countries to check that we calculated net exports correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_countries = [\"Brazil\", \"United States\", \"China\"]\n",
    "cols_to_keep = [\"Area/Country\", \"year\", \"exports\", \"imports\", \"net_exports\"]\n",
    "\n",
    "df_sel_un = df_table.loc[df_table[\"Area/Country\"].isin(sel_countries), cols_to_keep]\n",
    "df_sel_un.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.4\n",
    "\n",
    "**Plotting and annotating time series data**\n",
    "\n",
    "*Extract the relevant data*\n",
    "\n",
    "We will work with the `df_long` dataset, as the long format is well suited to produce charts with the **seaborn** package. In this example, we use the US and China (which we will now save as the dataframe `comp`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a copy of df_long that is just US and China\n",
    "comp = df_long.loc[df_long[\"Area/Country\"].isin([\"United States\", \"China\"]), :].copy()\n",
    "# Convert value to billion USD\n",
    "comp[\"value\"] = comp[\"value\"] / 1.0e9\n",
    "# Filter down to certain cols and values\n",
    "comp = comp.loc[\n",
    "    comp[\"IndicatorName\"].isin(\n",
    "        [\"gov_expenditure\", \"hh_expenditure\", \"capital\", \"imports\", \"exports\"]\n",
    "    ),\n",
    "    [\"Area/Country\", \"year\", \"IndicatorName\", \"value\"],\n",
    "]\n",
    "comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a line chart*\n",
    "\n",
    "We can now plot this data using the **seaborn** data visualisation library. We'll subset to US' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=comp[comp[\"Area/Country\"] == \"United States\"],\n",
    "    x=\"year\",\n",
    "    y=\"value\",\n",
    "    hue=\"IndicatorName\",\n",
    "    style=\"IndicatorName\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 4.2 The US‚Äôs GDP components (expenditure approach).*\n",
    "\n",
    "There are plenty of problems with this chart:\n",
    "\n",
    "- the vertical axis label is uninformative\n",
    "- there is no chart title\n",
    "- the y-axis dips below zero\n",
    "- the legend is uninformative.\n",
    "\n",
    "\n",
    "To improve this chart, we add features to the figure by creating the axis, `ax`, explicitly. We'll also use a trick where we invert the dictionary from earlier and use this to supply full names to the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO finalise\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=comp[comp[\"Area/Country\"] == \"United States\"],\n",
    "    x=\"year\",\n",
    "    y=\"value\",\n",
    "    hue=\"IndicatorName\",\n",
    "    style=\"IndicatorName\",\n",
    "    ax=ax,\n",
    ")\n",
    "# Set sensible y-lims\n",
    "ax.set_ylim(0, 1.4e4)\n",
    "# Add a y-label\n",
    "ax.set_ylabel(\"Billions USD\")\n",
    "# Add a title\n",
    "ax.set_title(\"GDP components over time\", loc=\"center\")\n",
    "# Add an annotation for a feature at a specific year\n",
    "ax.annotate(\"Great Recession\", (2008, 0.05e4), size=8, ha=\"center\")\n",
    "# Use full number notation\n",
    "ax.ticklabel_format(style=\"plain\")\n",
    "# Tweak the legend\n",
    "# first reverse the dictionary and only keep words before \"(\"\n",
    "rev_name_dict = {v: k.split(\"(\")[0] for k, v in short_names_dict.items()}\n",
    "# now set the legend with new labels by getting old labels (`ax.get_legend_handles_labels()[1]`) and\n",
    "# converting them back to original column names\n",
    "ax.legend(\n",
    "    frameon=False,\n",
    "    fontsize=8,\n",
    "    labels=[rev_name_dict[x] for x in ax.get_legend_handles_labels()[1]],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.3** The US‚Äôs GDP components (expenditure approach), amended chart.*\n",
    "\n",
    "We can make a chart for more than one country simultaneously by switching to a **seaborn** `relplot` with otherwise the same settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO finalise\n",
    "rel = sns.relplot(\n",
    "    data=comp,\n",
    "    kind=\"line\",\n",
    "    style=\"IndicatorName\",\n",
    "    x=\"year\",\n",
    "    y=\"value\",\n",
    "    hue=\"IndicatorName\",\n",
    "    col=\"Area/Country\",\n",
    "    linewidth=2.5,\n",
    ")\n",
    "rev_name_dict = {v: k.split(\"(\")[0] for k, v in short_names_dict.items()}\n",
    "\n",
    "\n",
    "for ax in rel.fig.axes:\n",
    "    ax.set_ylim(0, 1.4e4)\n",
    "    # Use full number notation\n",
    "    ax.ticklabel_format(style=\"plain\")\n",
    "\n",
    "# Change name in legend\n",
    "[t.set_text(rev_name_dict[t.get_text()]) for t in rel._legend.texts]\n",
    "\n",
    "rel.fig.axes[0].set_ylabel(\"Billions USD\")\n",
    "# remove default legend\n",
    "rel._legend.remove()\n",
    "# add custom legend\n",
    "rel.fig.axes[0].legend(\n",
    "    frameon=False,\n",
    "    fontsize=8,\n",
    "    labels=[rev_name_dict[x] for x in rel.fig.axes[0].get_legend_handles_labels()[1]],\n",
    ")\n",
    "plt.suptitle(\"GDP components over time\", y=1.03);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO finalise\n",
    "pl = (\n",
    "    ggplot(comp, aes(x=\"year\", y=\"value\", color=\"IndicatorName\"))\n",
    "    + geom_line(aes(group=\"IndicatorName\"), size=1)\n",
    "    + facet_wrap(\"Area/Country\")\n",
    "    + scale_y_continuous(name=\"Billion US$\")\n",
    "    + theme(theme_bw)\n",
    "    + scale_x_discrete(breaks=range(comp[\"year\"].min(), comp[\"year\"].max(), 10))\n",
    "    + scale_colour_discrete(\n",
    "        name=\"Components of GDP\",\n",
    "        labels=[rev_name_dict[x] for x in sorted(comp[\"IndicatorName\"].unique())],\n",
    "    )\n",
    ")\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.4** GDP components over time (China and the US).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.5\n",
    "\n",
    "**Calculating new variables and plotting time series data**\n",
    "\n",
    "*Calculate proportion of total GDP*\n",
    "\n",
    "We will use the `comp` dataset created in Python Walkthrough 4.4. First we will calculate net exports, as that contributes to GDP. As the data is currently in long format, we will reshape the data into wide format so that the variables we need are in separate columns instead of separate rows (using the `pivot` method, as in Python Walkthrough 4.3), calculate net exports, then transform the data back into long format using the `melt` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to wide format (indicators in columns)\n",
    "comp_wide = comp.pivot(index=[\"Area/Country\", \"year\"], columns=\"IndicatorName\")\n",
    "comp_wide.columns = comp_wide.columns.droplevel()\n",
    "comp_wide.columns.name = \"\"\n",
    "comp_wide = comp_wide.reset_index()\n",
    "comp_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new column for net exports = exports ‚Äì imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_wide[\"net_exports\"] = comp_wide[\"exports\"] - comp_wide[\"imports\"]\n",
    "comp_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to long format with the household expenditure, capital, and net export variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp2_wide = comp_wide.loc[\n",
    "    :, [x for x in comp_wide.columns if x not in [\"exports\", \"imports\"]]\n",
    "]\n",
    "comp2 = pd.melt(\n",
    "    comp2_wide,\n",
    "    id_vars=[\"year\", \"Area/Country\"],\n",
    "    var_name=\"indicator\",\n",
    "    value_name=\"2015_bn_usd\",\n",
    ")\n",
    "comp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a new dataframe (`props`) also containing the proportions for each GDP component (`proportion`), using method chaining to link functions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = comp2.assign(\n",
    "    proportion=comp2.groupby([\"Area/Country\", \"year\"])[\"2015_bn_usd\"].transform(\n",
    "        lambda x: x / x.sum()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, we did the following: Take the `comp2` dataframe and add in a new column called `proportion` (this bit starts with `.assign(proportion=`) that, within area and year groups (`.groupby([\"Area/Country\", \"year\"])`) takes the value (`[\"2015_bn_usd\"]`) and divides it by the total value for that group (`.transform(lambda x: x/x.sum())`). For example, the first row gives the proportion of capital for China in 1970.\n",
    "\n",
    "The result is then saved in props. Look at the props dataframe to confirm that the above command has achieved the desired result. (You can check the answer with `props.groupby([\"Area/Country\", \"year\"])[\"proportion\"].sum()`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a line chart*\n",
    "\n",
    "Now we redo the line chart from Python Walkthrough 4.4 using the variable `props`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO finalise\n",
    "\n",
    "# Update dictionary for net exports, which is new\n",
    "rev_name_dict.update({\"net_exports\": \"Net exports\"})\n",
    "\n",
    "pl = (\n",
    "    ggplot(props, aes(x=\"year\", y=\"proportion\", color=\"indicator\"))\n",
    "    + geom_line(aes(group=\"indicator\"), size=1)\n",
    "    + facet_wrap(\"Area/Country\")\n",
    "    + scale_y_continuous(name=\"%\")\n",
    "    + theme(theme_bw, subplots_adjust={\"wspace\": 0.1})\n",
    "    + scale_colour_discrete(\n",
    "        name=\"Components of GDP\",\n",
    "        labels=[rev_name_dict[x] for x in sorted(props[\"indicator\"].unique())],\n",
    "    )\n",
    ")\n",
    "pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.5** GDP component proportions over time (China and the US).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.6\n",
    "\n",
    "**Creating stacked bar charts**\n",
    "\n",
    "*Calculate proportion of total GDP*\n",
    "\n",
    "This walk-through uses the following countries (chosen at random):\n",
    "\n",
    "- developed countries: Germany, Japan, United States\n",
    "- transition countries: Albania, Russian Federation, Ukraine\n",
    "- developing countries: Brazil, China, India.\n",
    "\n",
    "The relevant data are still in the `df` dataframe. Before we select these countries, we first calculate the required proportions for all countries for capital, final expenditure, and net exports (out of those columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_track = [\"capital\", \"final_expenditure\", \"net_exports\"]\n",
    "countries_to_use = [\n",
    "    \"Germany\",\n",
    "    \"Japan\",\n",
    "    \"United States\",\n",
    "    \"Albania\",\n",
    "    \"Russian Federation\",\n",
    "    \"Ukraine\",\n",
    "    \"Brazil\",\n",
    "    \"China\",\n",
    "    \"India\",\n",
    "]\n",
    "\n",
    "# Find the proportions for these columns and create new columns called \"prop_\" + original col name\n",
    "for col in columns_to_track:\n",
    "    df_table[\"prop_\" + col] = df_table[col].divide(\n",
    "        df_table[columns_to_track].sum(axis=1)\n",
    "    )\n",
    "\n",
    "# filter this down to 2015 for the countries and cols we want\n",
    "cols_to_keep = [\"prop_\" + col for col in columns_to_track] + [\"Area/Country\", \"year\"]\n",
    "df_2015 = df_table.loc[\n",
    "    (df_table[\"Area/Country\"].isin(countries_to_use)) & (df_table[\"year\"] == 2015),\n",
    "    cols_to_keep,\n",
    "]\n",
    "df_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a stacked bar chart*\n",
    "\n",
    "Now let‚Äôs create the bar chart. First we need to melt the data into a format where each row is an observation, each column a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015_melt = pd.melt(\n",
    "    df_2015,\n",
    "    id_vars=[\"Area/Country\", \"year\"],\n",
    "    value_name=\"Percent\",\n",
    "    var_name=[\"Component\"],\n",
    ")\n",
    "df_2015_melt[\"Percent\"] = df_2015_melt[\"Percent\"] * 100\n",
    "\n",
    "\n",
    "(\n",
    "    ggplot(df_2015_melt, aes(\"Area/Country\", \"Percent\", fill=\"Component\"))\n",
    "    + geom_col()\n",
    "    + geom_bar(stat=\"identity\")\n",
    "    + coord_flip()\n",
    "    + scale_fill_discrete(\n",
    "        name=\"Components of GDP\",\n",
    "        labels=[\"Final expenditure\", \"Gross capital formation\", \"Net Exports\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.6** GDP component proportions in 2015.*\n",
    "\n",
    "Note that even when a country has a trade deficit (proportion of net exports < 0), the proportions will add up to 1, but the proportions of final expenditure and capital will add up to more than 1.\n",
    "\n",
    "We have not yet ordered the countries so that they form the pre-specified groups. To achieve this, we need to explicitly impose an ordering on the Area/Country variable by converting this column to be of type category and setting the order of those categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015_melt[\"Area/Country\"] = pd.Categorical(\n",
    "    df_2015_melt[\"Area/Country\"], categories=countries_to_use\n",
    ")\n",
    "\n",
    "(\n",
    "    ggplot(df_2015_melt, aes(\"Area/Country\", \"Percent\", fill=\"Component\"))\n",
    "    + geom_col()\n",
    "    + geom_bar(stat=\"identity\")\n",
    "    + coord_flip()\n",
    "    + scale_fill_discrete(\n",
    "        name=\"Components of GDP\",\n",
    "        labels=[\"Final expenditure\", \"Gross capital formation\", \"Net Exports\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIND OUT MORE**\n",
    "\n",
    "*The natural log: What it means, and how to calculate it in Python*\n",
    "\n",
    "The natural log turns a linear variable into a concave variable, as shown in Figure 4.9. For any value of income on the horizontal axis, the natural log of that value on the vertical axis is smaller. At first, the difference between income and log income is not that big (for example, an income of 2 corresponds to a natural log of 0.7), but the difference becomes bigger as we move rightwards along the horizontal axis (for example, when income is 100,000, the natural log is only 11.5).\n",
    "\n",
    "![](https://www.core-econ.org/doing-economics/book/images/web/figure-04-05.jpg)\n",
    "\n",
    "***Figure 4.9** Comparing income with the natural logarithm of income.*\n",
    "\n",
    "The reason why natural logs are useful in economics is because they can represent variables that have diminishing marginal returns: an additional unit of input results in a smaller increase in the total output than did the previous unit. (If you have studied production functions, then the shape of the natural log function might look familiar.)\n",
    "\n",
    "When applied to the concept of wellbeing, the ‚Äòinput‚Äô is income, and the ‚Äòoutput‚Äô is material wellbeing. It makes intuitive sense that a $100 increase in per capita income will have a much greater effect on wellbeing for a poor country compared to a rich country. Using the natural log of income incorporates this notion into the index we create. Conversely, the notion of diminishing marginal returns (the larger the value of the input, the smaller the contribution of an additional unit of input) is not captured by GDP per capita, which uses actual income and not its natural log. Doing so makes the assumption that a $100 increase in per capita income has the same effect on wellbeing for rich and poor countries.\n",
    "\n",
    "The **numpy** log function in Python calculates the natural log of a value for you. To calculate the natural log of a value, `x`, type `np.log(x)`. If you have a scientific calculator, you can check that the calculation is correct by using the ln or log key.\n",
    "\n",
    "Now that you know about the natural log, you might want to go back to Question 3(c) in Part 4.1, and create a new chart using the natural log scale. The natural log is used in economics because it approximates percentage changes i.e. log(x) ‚Äì log(y) is a close approximation to the percentage change between x and y. So, using the natural log scale, you will be able to ‚Äòread off‚Äô the relative growth rates from the slopes of the different series you have plotted. For example, a 0.01 change in the vertical axis value corresponds to a 1% change in that variable. This will allow you to compare the growth rates of the different components of GDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.7\n",
    "\n",
    "**Calculating the HDI**\n",
    "\n",
    "We will use `pd.read_excel` to import the data file, which we saved using its default name of '2020_Statistical_Annex_Table_1.xlsx‚Äô in the data folder within our working directory. Before importing, look at the Excel file so that you understand its structure and how it corresponds to the code options used below. It's a long way from being a neat and tidy dataset! We will save the imported data as the `df_hdr` dataframe. Having taken a look at the Excel file, we can see we should skip the first few rows and take row 1 as the header (which becomes our column names).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = pd.read_excel(\n",
    "    Path(\"data/2020_Statistical_Annex_Table_1.xlsx\"), skiprows=3, header=1\n",
    ")\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `df_hdr` dataframe, there are rows that have information that isn‚Äôt data (for example, all the rows with an ‚ÄòNaN‚Äô in), as well as variables/columns that do not contain data, or are a mix of 'NaN' and data.\n",
    "\n",
    "Cleaning up the dataframe can be easier to do in Excel by deleting irrelevant rows and columns, but one advantage of doing it in Python is replicability. Suppose in a year‚Äôs time you carried out the analysis again with an updated spreadsheet containing new information. If you had done the cleaning in Excel, you would have to redo it from scratch, but if you had done it in Python, you can simply rerun the code below. (This works for new data that are in the same format too.)\n",
    "\n",
    "Let's do some data cleaning.\n",
    "\n",
    "First, we rename the last column by picking up the year entry below it in order to distinguish between different years of HDI rank. Next, we replace any columns that are \"Unnamed\" with entries from the first row of observations with a list comprehension. Then we eliminate rows that do not have any numbers in the `\"HDI rank\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = df_hdr.rename(columns={\"HDI rank\": \"HDI_rank_\" + str(df_hdr.iloc[1, -1])})\n",
    "df_hdr.columns = [df_hdr.columns[i] if \"Unnamed\" not in df_hdr.columns[i] else df_hdr.iloc[0, i] for i in range(len(df_hdr.columns))]\n",
    "df_hdr = df_hdr.loc[~pd.isna(df_hdr[\"HDI rank\"]), :]\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can eliminate rows in HDI rank that do not have numbers in, and, following that, eliminate columns that contain NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = df_hdr.loc[~pd.isna(df_hdr[\"HDI_rank_2018\"]), :]\n",
    "df_hdr = df_hdr.dropna(axis=1, how=\"any\")\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's switch to shorter column names and check what datatypes we have in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\"hdi_rank\", \"country\", \"hdi\", \"life_exp\", \"exp_yrs_school\", \"mean_yrs_school\", \"gni_capita\", \"gni_hdi_rank\", \"hdi_rank_2018\"]\n",
    "df_hdr.columns = new_column_names\n",
    "df_hdr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the structure of the data, we see that **pandas** thinks that all the data are objects because the original datafile contained non-numerical entries (these rows have now been deleted). Apart from the `\"country\"` variable, which we want to be a categorical variable, all variables should be doubles or ints. Let's sort that out using a trick where we 'zip' two variables (the column names and datatypes) together into a dictionary that maps the column name into the datatype we'd it to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_datatypes = [\"int\", \"category\", \"double\", \"double\", \"double\", \"double\", \"double\", \"int\", \"int\"]\n",
    "df_hdr = df_hdr.astype({k: v for k, v in zip(new_column_names, new_column_datatypes)})\n",
    "df_hdr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nice clean dataset that we can work with.\n",
    "\n",
    "We start by calculating the three indices, using the information given. For the education index we calculate the index for expected and mean schooling separately, then take the arithmetic mean to get `i_education`. As some mean schooling observations exceed the specified ‚Äòmaximum‚Äô value of 18, the calculated index values would be larger than 1. To avoid this, we first replace these observations with 18 to obtain an index value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr.loc[df_hdr[\"exp_yrs_school\"] > 18, \"exp_yrs_school\"] = 18\n",
    "\n",
    "# Now create the indices\n",
    "df_hdr[\"i_health\"] = (df_hdr[\"life_exp\"] - 20)/ (85 - 20)\n",
    "df_hdr[\"i_education\"] = (((df_hdr[\"exp_yrs_school\"] - 0) / (18-0)) + (df_hdr[\"mean_yrs_school\"] - 0) / (15-0))/2\n",
    "df_hdr[\"i_income\"] = (np.log(df_hdr[\"gni_capita\"]) - np.log(100)) / (np.log(75000) - np.log(100))\n",
    "df_hdr[\"hdi_calc\"] = np.power(df_hdr[\"i_health\"]*df_hdr[\"i_education\"]*df_hdr[\"i_income\"], 1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the `HDI` given in the table and our calculated HDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr[[\"hdi\", \"hdi_calc\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.8\n",
    "\n",
    "**Creating your own HDI**\n",
    "\n",
    "*Merge data and calculate alternative indices*\n",
    "\n",
    "TODO: THE DATA REFERRED TO IN THE R WALKTHROUGH ARE NO LONGER AVAILABLE\n",
    "\n",
    "TODO replace this one with a HDI created from the already imported data"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13950ff975ea58bb356510d0d8f98cdd1bd1f12bd4ccce66a17a21f4f1a23379"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('core-python-wDU3726x-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
