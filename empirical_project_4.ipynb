{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Project 4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Settings\n",
    "\n",
    "Let's import the packages we'll need and also configure the settings we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "import pingouin as pg\n",
    "import warnings\n",
    "\n",
    "\n",
    "### You don't need to use these settings yourself\n",
    "### ‚Äî they are just here to make the book look nicer!\n",
    "# Set the plot style for prettier charts:\n",
    "plt.style.use(\"plot_style.txt\")\n",
    "# Make seaborn work consistently with this\n",
    "so.Plot.config.theme.update(mpl.rcParams)\n",
    "# Ignore warnings to make nice output\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Go to the United Nations‚Äô [National Accounts Main Aggregates Database website](https://tinyco.re/7226184). On the right-hand side of the page, under ‚ÄòData Availability‚Äô, click ‚ÄòDownloads‚Äô.\n",
    "- Under the subheading ‚ÄòGDP and its breakdown at constant 2015 prices in US Dollars‚Äô, select the Excel file ‚ÄòAll countries for all years ‚Äì sorted alphabetically‚Äô.\n",
    "- Save it in a subfolder of the directory you are coding in such that that the relative path is `data/Download-GDPconstant-USD-all.xlsx`.\n",
    "\n",
    "## Python Walkthrough 4.1\n",
    "\n",
    "**Importing the Excel file (`.xlsx` or `.xls`) into Python**\n",
    "\n",
    "First, make sure you move the saved the data to a folder called `data` that is a subfolder of your working directory. The working directory is the folder that your code 'starts' in, and the one that you open when you start Visual Studio Code. Let's say you called it `core`, then the file and folder structure of your working directory would look like this:\n",
    "\n",
    "```bash\n",
    "üìÅ core\n",
    "‚îÇ‚îÄ‚îÄüìÅdata\n",
    "   ‚îî‚îÄ‚îÄDownload-GDPconstant-USD-all.xlsx\n",
    "‚îÇ‚îÄ‚îÄempirical_project_4.py\n",
    "```\n",
    "\n",
    "This is similar to what you should see in Visual Studio Code under the explorer tab (although the working directory, `core`, won't appear). You can check your working directory by running\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.getcwd()\n",
    "```\n",
    "\n",
    "in Visual Studio Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before importing the file into Python, open the file in Excel, OpenOffice, LibreOffice, or Numbers to see how the data is organized in the spreadsheet, and note that:\n",
    "\n",
    "- There is a heading that we don‚Äôt need, followed by a blank row.\n",
    "- The data we need starts on row three.\n",
    "\n",
    "Armed with this knowledge, we can import the data using the `Path` module to create the path to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(Path(\"data/Download-GDPconstant-USD-all.xlsx\"), skiprows=2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.2\n",
    "\n",
    "**Making a frequency table**\n",
    "\n",
    "We want to create a table showing how many years of `Final consumption expenditure` data are available for each country.\n",
    "\n",
    "Looking at the dataset‚Äôs current format, you can see that countries and indicators (for example, `Afghanistan` and `Final consumption expenditure`) are row variables, while year is the column variable. This data is organized in ‚Äòwide‚Äô format (each individual‚Äôs information is in a single row).\n",
    "\n",
    "For many data operations and making charts it is more convenient to have indicators as column variables, so we would like `Final consumption expenditure` to be a column variable, and year to be the row variable. Each observation would represent the value of an indicator for a particular country and year. This data is organized in ‚Äòlong‚Äô format (each individual‚Äôs information is in multiple rows). This is also called 'tidy' data and it can be recognised by having variable per column and one observation per row. Many data scientists consider keeping data in tidy format good practice.\n",
    "\n",
    "To change data from wide to long format, we use the `pd.melt` method. The `melt` method is very powerful and useful, as you will find many large datasets are in wide format. In this case, `pd.melt` takes the data from all columns not specified as being `id_vars` (via a list of column names), and uses them to create two new columns: one contains the name of the row variable created from the former column names, which is the year here; we can set that new column's name with `var_name=\"year\"`. The second new column contains the values that were in the columns we unpivoted and is automatically given the name `value`. (We could have set a new name for this column by passing `value_name=` too.)\n",
    "\n",
    "Compare `df_long` to the wider `df` to understand how the melt command works. To learn more about organizing data in Python, see the [Working with Data](https://aeturrell.github.io/coding-for-economists/data-intro.html) section of 'Coding for Economists'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.melt(\n",
    "    df, id_vars=[\"Area/CountryID\", \"Area/Country\", \"IndicatorName\"], var_name=\"year\"\n",
    ")\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the required table, we only need `Final consumption expenditure` of each country, which we extract using the `.loc` function. We'd like all columns so we pass the condition in the first position of `.loc` and leave the second entry as `:` for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = df_long.loc[df_long[\"IndicatorName\"] == \"Final consumption expenditure\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count = cons.groupby(\"Area/Country\").agg(available_years=(\"year\", \"count\"))\n",
    "year_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating the code in words: Take the variable `cons` and group the observations by area and country (`.groupby(Area/Country\")`), then take this result and aggregate `.agg` it such that a new variable called available years (`available_years=`) is created that sees the column year counted (`(\"year\", \"count\")`).\n",
    "\n",
    "Now we can establish how many of the 250 countries and areas in the dataset have complete information. A dataset is complete if it has the maximum number of available observations (given by `year_count[\"available_years\"].max()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(year_count[\"available_years\"] == year_count[\"available_years\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the full set of data are available for all countries and areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.3\n",
    "\n",
    "**Creating new variables**\n",
    "\n",
    "We will use Brazil, the US, and China as examples.\n",
    "\n",
    "Before we select these three countries, we will calculate the net exports (exports minus imports) for all countries, as we need that information in Python walkthrough 4.4. We will also shorten the names of the variables we need, to make the code easier to read. We will use a dictionary to map names into shorter formats. A dictionary is a built-in object type in Python and always has the structure `{key1: value1, key2: value2, ...}` where the keys and values could have any type (eg string, int, dataframe). In our case, both keys and values will be strings. We will use a convention for our naming that is known as \"snake case\". This means all lower case with spaces replaced by underscores (it looks a bit like a snake!). There are packages that will auto-rename long variables for you, but let's see how to do it manually here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_names_dict = {\n",
    "    \"Final consumption expenditure\": \"final_expenditure\",\n",
    "    \"Household consumption expenditure (including Non-profit institutions serving households)\": \"hh_expenditure\",\n",
    "    \"General government final consumption expenditure\": \"gov_expenditure\",\n",
    "    \"Gross capital formation\": \"capital\",\n",
    "    \"Imports of goods and services\": \"imports\",\n",
    "    \"Exports of goods and services\": \"exports\",\n",
    "}\n",
    "# rename these values\n",
    "df_long[\"IndicatorName\"] = df_long[\"IndicatorName\"].replace(short_names_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_long` still has several rows for a particular country and year (one for each indicator). We will reshape this data using the `.pivot` method to ensure that we have only one row per country/area and per year. Note that `pivot` preserves the list of columns we pass as the `index=` and pivots the columns we pass to `columns=` out so that they are wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = df_long.pivot(\n",
    "    index=[\"Area/CountryID\", \"Area/Country\", \"year\"], columns=[\"IndicatorName\"]\n",
    ")\n",
    "df_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a `net_exports` column based on the existing columns (exports - imports), and we can know that this will be a unique country/area and year combination for each row. First we need to drop the top level of the column index, which is currently called `value`: we don't need this anymore. This will allow for direct access to the `exports` and `imports` columns. We'll also reset the index to row numbers rather than those three columns we used in the pivot. We'll also remove the name of the columns as we won't need that any longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.columns = df_table.columns.droplevel()\n",
    "df_table = df_table.reset_index()\n",
    "df_table.columns.name = \"\"\n",
    "df_table[\"net_exports\"] = df_table[\"exports\"] - df_table[\"imports\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us select our three chosen countries to check that we calculated net exports correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_countries = [\"Brazil\", \"United States\", \"China\"]\n",
    "cols_to_keep = [\"Area/Country\", \"year\", \"exports\", \"imports\", \"net_exports\"]\n",
    "\n",
    "df_sel_un = df_table.loc[df_table[\"Area/Country\"].isin(sel_countries), cols_to_keep]\n",
    "df_sel_un.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.4\n",
    "\n",
    "**Plotting and annotating time series data**\n",
    "\n",
    "*Extract the relevant data*\n",
    "\n",
    "We will work with the `df_long` dataset, as the long format is well suited to produce charts with the **seaborn** package. In this example, we use the US and China (which we will now save as the dataframe `comp`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a copy of df_long that is just US and China\n",
    "comp = df_long.loc[df_long[\"Area/Country\"].isin([\"United States\", \"China\"]), :].copy()\n",
    "# Convert value to billion USD\n",
    "comp[\"value\"] = comp[\"value\"] / 1.0e9\n",
    "# Filter down to certain cols and values\n",
    "comp = comp.loc[\n",
    "    comp[\"IndicatorName\"].isin(\n",
    "        [\"gov_expenditure\", \"hh_expenditure\", \"capital\", \"imports\", \"exports\"]\n",
    "    ),\n",
    "    [\"Area/Country\", \"year\", \"IndicatorName\", \"value\"],\n",
    "]\n",
    "comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a line chart*\n",
    "\n",
    "We can now plot this data using the **seaborn** data visualisation library. We'll subset to US' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    so.Plot(\n",
    "        comp.loc[comp[\"Area/Country\"] == \"United States\", :],\n",
    "        x=\"year\",\n",
    "        y=\"value\",\n",
    "        color=\"IndicatorName\",\n",
    "        linestyle=\"IndicatorName\",\n",
    "    )\n",
    "    .add(so.Line())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 4.2 The US‚Äôs GDP components (expenditure approach).*\n",
    "\n",
    "There are plenty of problems with this chart:\n",
    "\n",
    "- the vertical axis label is uninformative\n",
    "- there is no chart title\n",
    "- the y-axis dips below zero\n",
    "- the legend is uninformative.\n",
    "\n",
    "\n",
    "To improve this chart, we add features to the figure by creating the axis, `ax`, explicitly. We'll also use a trick where we invert the dictionary from earlier and use this to supply full names to the legend via a new \"indicator\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the dictionary from earlier\n",
    "rev_name_dict = {v: k.split(\"(\")[0] for k, v in short_names_dict.items()}\n",
    "# create a new col with the original names\n",
    "comp[\"Indicator\"] = comp[\"IndicatorName\"].replace(rev_name_dict)\n",
    "# plot data\n",
    "fig, ax = plt.subplots()\n",
    "ax.annotate(\"Great Recession\", (2008, 0.05e4), size=8, ha=\"center\")\n",
    "(\n",
    "    so.Plot(\n",
    "        comp.loc[comp[\"Area/Country\"] == \"United States\", :],\n",
    "        x=\"year\",\n",
    "        y=\"value\",\n",
    "        color=\"Indicator\",\n",
    "        linestyle=\"Indicator\",\n",
    "    )\n",
    "    .add(so.Line())\n",
    "    .label(y=\"Billions USD\", title=\"GDP components over time\", color=\"Component\")\n",
    "    .scale(y=so.Continuous().label(like=\"{x:,g}\"))\n",
    "    .limit(y=(0, None))\n",
    "    .on(ax)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.3** The US‚Äôs GDP components (expenditure approach), amended chart.*\n",
    "\n",
    "We can make a chart for more than one country simultaneously by switching to a **seaborn** `relplot` with otherwise the same settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    so.Plot(\n",
    "        comp,\n",
    "        x=\"year\",\n",
    "        y=\"value\",\n",
    "        color=\"Indicator\",\n",
    "        linestyle=\"Indicator\",\n",
    "    )\n",
    "    .facet(\"Area/Country\")\n",
    "    .add(so.Line())\n",
    "    .label(\n",
    "        y=\"Billions USD\",\n",
    "        # title=\"GDP components over time\",\n",
    "        color=\"GDP Component\",\n",
    "    )\n",
    "    .scale(y=so.Continuous().label(like=\"{x:,g}\"))\n",
    "    .limit(y=(0, None))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.4** GDP components over time (China and the US).*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.5\n",
    "\n",
    "**Calculating new variables and plotting time series data**\n",
    "\n",
    "*Calculate proportion of total GDP*\n",
    "\n",
    "We will use the `comp` dataset created in Python Walkthrough 4.4. First we will calculate net exports, as that contributes to GDP. As the data is currently in long format, we will reshape the data into wide format so that the variables we need are in separate columns instead of separate rows (using the `pivot` method, as in Python Walkthrough 4.3), calculate net exports, then transform the data back into long format using the `melt` method.\n",
    "\n",
    "On the way, we'll end up dropping the Indicator Names, and dropping the top level \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_wide = comp.drop(\"Indicator\", axis=1).pivot(\n",
    "    index=[\"Area/Country\", \"year\"], columns=\"IndicatorName\"\n",
    ")\n",
    "comp_wide.columns = comp_wide.columns.droplevel()\n",
    "comp_wide = comp_wide.reset_index()\n",
    "comp_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new column for net exports = exports ‚Äì imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_wide[\"net_exports\"] = comp_wide[\"exports\"] - comp_wide[\"imports\"]\n",
    "comp_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to long format with the household expenditure, capital, and net export variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp2_wide = comp_wide.loc[\n",
    "    :, [x for x in comp_wide.columns if x not in [\"exports\", \"imports\"]]\n",
    "]\n",
    "comp2 = pd.melt(\n",
    "    comp2_wide,\n",
    "    id_vars=[\"year\", \"Area/Country\"],\n",
    "    var_name=\"indicator\",\n",
    "    value_name=\"2015_bn_usd\",\n",
    ")\n",
    "comp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a new dataframe (`props`) also containing the proportions for each GDP component (`proportion`), using method chaining to link functions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = comp2.assign(\n",
    "    proportion=comp2.groupby([\"Area/Country\", \"year\"])[\"2015_bn_usd\"].transform(\n",
    "        lambda x: x / x.sum()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, we did the following: Take the `comp2` dataframe and add in a new column called `proportion` (this bit starts with `.assign(proportion=`) that, within area and year groups (`.groupby([\"Area/Country\", \"year\"])`) takes the value (`[\"2015_bn_usd\"]`) and divides it by the total value for that group (`.transform(lambda x: x/x.sum())`). For example, the first row gives the proportion of capital for China in 1970.\n",
    "\n",
    "The result is then saved in props. Look at the props dataframe to confirm that the above command has achieved the desired result. (You can check the answer with `props.groupby([\"Area/Country\", \"year\"])[\"proportion\"].sum()`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a line chart*\n",
    "\n",
    "Now we redo the line chart from Python Walkthrough 4.4 using the variable `props`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dictionary for net exports, which is new\n",
    "rev_name_dict.update({\"net_exports\": \"Net exports\"})\n",
    "props[\"Component\"] = props[\"indicator\"].map(rev_name_dict)\n",
    "(\n",
    "    so.Plot(props, x=\"year\", y=\"proportion\", color=\"Component\", linestyle=\"Component\")\n",
    "    .facet(\"Area/Country\")\n",
    "    .add(so.Line())\n",
    "    .limit(y=(0, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.5** GDP component proportions over time (China and the US).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.6\n",
    "\n",
    "**Creating stacked bar charts**\n",
    "\n",
    "*Calculate proportion of total GDP*\n",
    "\n",
    "This walk-through uses the following countries (chosen at random):\n",
    "\n",
    "- developed countries: Germany, Japan, United States\n",
    "- transition countries: Albania, Russian Federation, Ukraine\n",
    "- developing countries: Brazil, China, India.\n",
    "\n",
    "The relevant data are still in the `df` dataframe. Before we select these countries, we first calculate the required proportions for all countries for capital, final expenditure, and net exports (out of those columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_track = [\"capital\", \"final_expenditure\", \"net_exports\"]\n",
    "countries_to_use = [\n",
    "    \"Germany\",\n",
    "    \"Japan\",\n",
    "    \"United States\",\n",
    "    \"Albania\",\n",
    "    \"Russian Federation\",\n",
    "    \"Ukraine\",\n",
    "    \"Brazil\",\n",
    "    \"China\",\n",
    "    \"India\",\n",
    "]\n",
    "\n",
    "# Find the proportions for these columns and create new columns called \"prop_\" + original col name\n",
    "for col in columns_to_track:\n",
    "    df_table[\"prop_\" + col] = df_table[col].divide(\n",
    "        df_table[columns_to_track].sum(axis=1)\n",
    "    )\n",
    "\n",
    "# filter this down to 2015 for the countries and cols we want\n",
    "cols_to_keep = [\"prop_\" + col for col in columns_to_track] + [\"Area/Country\", \"year\"]\n",
    "df_2015 = df_table.loc[\n",
    "    (df_table[\"Area/Country\"].isin(countries_to_use)) & (df_table[\"year\"] == 2015),\n",
    "    cols_to_keep,\n",
    "]\n",
    "df_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a stacked bar chart*\n",
    "\n",
    "Now let‚Äôs create the bar chart. First we need to melt the data into a format where each row is an observation, each column a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015_melt = pd.melt(\n",
    "    df_2015,\n",
    "    id_vars=[\"Area/Country\", \"year\"],\n",
    "    value_name=\"Percent\",\n",
    "    var_name=[\"Component\"],\n",
    ")\n",
    "df_2015_melt[\"Percent\"] = df_2015_melt[\"Percent\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    so.Plot(df_2015_melt, y=\"Area/Country\", x=\"Percent\", color=\"Component\")\n",
    "    .add(so.Bar(), so.Stack())\n",
    "    .label(\n",
    "        color=\"Components of GDP\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.6** GDP component proportions in 2015.*\n",
    "\n",
    "Note that even when a country has a trade deficit (proportion of net exports < 0), the proportions will add up to 1, but the proportions of final expenditure and capital will add up to more than 1.\n",
    "\n",
    "We have not yet ordered the countries so that they form the pre-specified groups. To achieve this, we need to explicitly impose an ordering on the Area/Country variable by converting this column to be of type category and setting the order of those categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015_melt[\"Area/Country\"] = pd.Categorical(\n",
    "    df_2015_melt[\"Area/Country\"], categories=countries_to_use\n",
    ")\n",
    "\n",
    "(\n",
    "    so.Plot(df_2015_melt, y=\"Area/Country\", x=\"Percent\", color=\"Component\")\n",
    "    .add(so.Bar(), so.Stack())\n",
    "    .label(\n",
    "        color=\"Components of GDP\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIND OUT MORE**\n",
    "\n",
    "*The natural log: What it means, and how to calculate it in Python*\n",
    "\n",
    "The natural log turns a linear variable into a concave variable, as shown in Figure 4.9. For any value of income on the horizontal axis, the natural log of that value on the vertical axis is smaller. At first, the difference between income and log income is not that big (for example, an income of 2 corresponds to a natural log of 0.7), but the difference becomes bigger as we move rightwards along the horizontal axis (for example, when income is 100,000, the natural log is only 11.5).\n",
    "\n",
    "![](https://www.core-econ.org/doing-economics/book/images/web/figure-04-05.jpg)\n",
    "\n",
    "***Figure 4.9** Comparing income with the natural logarithm of income.*\n",
    "\n",
    "The reason why natural logs are useful in economics is because they can represent variables that have diminishing marginal returns: an additional unit of input results in a smaller increase in the total output than did the previous unit. (If you have studied production functions, then the shape of the natural log function might look familiar.)\n",
    "\n",
    "When applied to the concept of wellbeing, the ‚Äòinput‚Äô is income, and the ‚Äòoutput‚Äô is material wellbeing. It makes intuitive sense that a $100 increase in per capita income will have a much greater effect on wellbeing for a poor country compared to a rich country. Using the natural log of income incorporates this notion into the index we create. Conversely, the notion of diminishing marginal returns (the larger the value of the input, the smaller the contribution of an additional unit of input) is not captured by GDP per capita, which uses actual income and not its natural log. Doing so makes the assumption that a $100 increase in per capita income has the same effect on wellbeing for rich and poor countries.\n",
    "\n",
    "The **numpy** log function in Python calculates the natural log of a value for you. To calculate the natural log of a value, `x`, type `np.log(x)`. If you have a scientific calculator, you can check that the calculation is correct by using the ln or log key.\n",
    "\n",
    "Now that you know about the natural log, you might want to go back to Question 3(c) in Part 4.1, and create a new chart using the natural log scale. The natural log is used in economics because it approximates percentage changes i.e. log(x) ‚Äì log(y) is a close approximation to the percentage change between x and y. So, using the natural log scale, you will be able to ‚Äòread off‚Äô the relative growth rates from the slopes of the different series you have plotted. For example, a 0.01 change in the vertical axis value corresponds to a 1% change in that variable. This will allow you to compare the growth rates of the different components of GDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.7\n",
    "\n",
    "**Calculating the HDI**\n",
    "\n",
    "We will use `pd.read_excel` to import the data file, which we saved using its default name of '2020_Statistical_Annex_Table_1.xlsx‚Äô in the data folder within our working directory. Before importing, look at the Excel file so that you understand its structure and how it corresponds to the code options used below. It's a long way from being a neat and tidy dataset! We will save the imported data as the `df_hdr` dataframe. Having taken a look at the Excel file, we can see we should skip the first few rows and take row 1 as the header (which becomes our column names).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = pd.read_excel(\n",
    "    Path(\"data/2020_Statistical_Annex_Table_1.xlsx\"), skiprows=3, header=1\n",
    ")\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `df_hdr` dataframe, there are rows that have information that isn‚Äôt data (for example, all the rows with an ‚ÄòNaN‚Äô in), as well as variables/columns that do not contain data, or are a mix of 'NaN' and data.\n",
    "\n",
    "Cleaning up the dataframe can be easier to do in Excel by deleting irrelevant rows and columns, but one advantage of doing it in Python is replicability. Suppose in a year‚Äôs time you carried out the analysis again with an updated spreadsheet containing new information. If you had done the cleaning in Excel, you would have to redo it from scratch, but if you had done it in Python, you can simply rerun the code below. (This works for new data that are in the same format too.)\n",
    "\n",
    "Let's do some data cleaning.\n",
    "\n",
    "First, we rename the last column by picking up the year entry below it in order to distinguish between different years of HDI rank. Next, we replace any columns that are \"Unnamed\" with entries from the first row of observations with a list comprehension. Then we eliminate rows that do not have any numbers in the `\"HDI rank\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = df_hdr.rename(columns={\"HDI rank\": \"HDI_rank_\" + str(df_hdr.iloc[1, -1])})\n",
    "df_hdr.columns = [\n",
    "    df_hdr.columns[i] if \"Unnamed\" not in df_hdr.columns[i] else df_hdr.iloc[0, i]\n",
    "    for i in range(len(df_hdr.columns))\n",
    "]\n",
    "df_hdr = df_hdr.loc[~pd.isna(df_hdr[\"HDI rank\"]), :]\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can eliminate rows in HDI rank that do not have numbers in, and, following that, eliminate columns that contain NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = df_hdr.loc[~pd.isna(df_hdr[\"HDI_rank_2018\"]), :]\n",
    "df_hdr = df_hdr.dropna(axis=1, how=\"any\")\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's switch to shorter column names and check what datatypes we have in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\n",
    "    \"hdi_rank\",\n",
    "    \"country\",\n",
    "    \"hdi\",\n",
    "    \"life_exp\",\n",
    "    \"exp_yrs_school\",\n",
    "    \"mean_yrs_school\",\n",
    "    \"gni_capita\",\n",
    "    \"gni_hdi_rank\",\n",
    "    \"hdi_rank_2018\",\n",
    "]\n",
    "df_hdr.columns = new_column_names\n",
    "df_hdr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the structure of the data, we see that **pandas** thinks that all the data are objects because the original datafile contained non-numerical entries (these rows have now been deleted). Apart from the `\"country\"` variable, which we want to be a categorical variable, all variables should be doubles or ints. Let's sort that out using a trick where we 'zip' two variables (the column names and datatypes) together into a dictionary that maps the column name into the datatype we'd it to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_datatypes = [\n",
    "    \"int\",\n",
    "    \"category\",\n",
    "    \"double\",\n",
    "    \"double\",\n",
    "    \"double\",\n",
    "    \"double\",\n",
    "    \"double\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "]\n",
    "df_hdr = df_hdr.astype({k: v for k, v in zip(new_column_names, new_column_datatypes)})\n",
    "df_hdr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nice clean dataset that we can work with.\n",
    "\n",
    "We start by calculating the three indices, using the information given. For the education index we calculate the index for expected and mean schooling separately, then take the arithmetic mean to get `i_education`. As some mean schooling observations exceed the specified ‚Äòmaximum‚Äô value of 18, the calculated index values would be larger than 1. To avoid this, we first replace these observations with 18 to obtain an index value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr.loc[df_hdr[\"exp_yrs_school\"] > 18, \"exp_yrs_school\"] = 18\n",
    "\n",
    "# Now create the indices\n",
    "df_hdr[\"i_health\"] = (df_hdr[\"life_exp\"] - 20) / (85 - 20)\n",
    "df_hdr[\"i_education\"] = (\n",
    "    ((df_hdr[\"exp_yrs_school\"] - 0) / (18 - 0))\n",
    "    + (df_hdr[\"mean_yrs_school\"] - 0) / (15 - 0)\n",
    ") / 2\n",
    "df_hdr[\"i_income\"] = (np.log(df_hdr[\"gni_capita\"]) - np.log(100)) / (\n",
    "    np.log(75000) - np.log(100)\n",
    ")\n",
    "df_hdr[\"hdi_calc\"] = np.power(\n",
    "    df_hdr[\"i_health\"] * df_hdr[\"i_education\"] * df_hdr[\"i_income\"], 1 / 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the `HDI` given in the table and our calculated HDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr[[\"hdi\", \"hdi_calc\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDI is one way to measure wellbeing, but you may think that it does not use the most appropriate measures for the non-material aspects of wellbeing (health and education).\n",
    "\n",
    "https://databank.worldbank.org/\n",
    "\n",
    "Now we will use the same method to create our own index of non-material wellbeing (an ‚Äòalternative HDI‚Äô), using different indicators. You can find alternative indicators to measure health and education on the [World Bank data bank](https://databank.worldbank.org).\n",
    "\n",
    "https://databank.worldbank.org/Human-development-index/id/363d401b#\n",
    "\n",
    "1. Create an alternative index of wellbeing. In particular, propose alternative Education and Health indices in (a) and (b), then combine these with the existing Income index in (c) to calculate an alternative HDI. Examine whether the changes caused substantial changes in country rankings in (d).\n",
    "\n",
    "a) Choose two to three indicators to measure health, and two to three indicators to measure education. Explain why you have chosen these indicators.\n",
    "\n",
    "b) Carefully merge the data into your existing data. Choose a reasonable maximum and minimum value for each indicator and justify your choices.\n",
    "\n",
    "c) Calculate your alternative versions of the education and health dimension indices. Since you have chosen more than one indicator for this dimension, make sure to average the dimension indices as done in Question 3\n",
    "(b). Also ensure that higher indicator values always represent better outcomes. Now calculate the alternative HDI as done in Questions 3 and 4. Remember to combine your alternative education and health indices with the existing income index from Question 2.\n",
    "\n",
    "d) Create a new variable showing each country‚Äôs rank according to your alternative HDI, where 1 is assigned to the country with the highest value. Compare your ranking to the HDI rank. Are the rankings generally similar, or very different? (See R walk-through 4.8 on how to do this.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.8\n",
    "\n",
    "**Creating your own HDI**\n",
    "\n",
    "*Merge data and calculate alternative indices*\n",
    "\n",
    "This example uses the following indicators:\n",
    "\n",
    "- Education: Literacy rate, adult (% ages 15 and older); Gross enrolment ratio, tertiary (% of tertiary school-age population); Primary school teachers trained to teach (%)\n",
    "\n",
    "- Health: Child malnutrition, stunting (moderate or severe) (% under age 5); Mortality rate, female adult (per 1,000 people); Mortality rate, male adult (per 1,000 people).\n",
    "\n",
    "First, we import the data and check that it has been imported correctly. You can see that each row represents a different country, and each column represents a different year-indicator combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hdr = pd.read_csv(Path(\"data/HDR21-22_Composite_indices_complete_time_series.csv\"))\n",
    "all_hdr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we follow the same process as in Python Walkthrough 4.7‚Äîgetting the data for the indicators we want, reshaping it so that each indicator is in a different column (instead of a different row), and giving each indicator a shorter name. We will save this data as `hdr_wide`. Note that the variable 9999 refers to the latest year available, or the average taken over a range of years (the Excel file contains information on which year(s) were used).\n",
    "\n",
    "We can use the convenience **pandas** function `wide_to_long`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26c4b888616894cf8360ee3d370b6a41732ef00c8f1d61e869c42a8428cf1ac1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
