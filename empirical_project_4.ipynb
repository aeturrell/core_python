{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Project 4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Settings\n",
    "\n",
    "Let's import the packages we'll need and also configure the settings we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pingouin as pg\n",
    "from skimpy import skim\n",
    "from lets_plot import *\n",
    "\n",
    "LetsPlot.setup_html(no_js=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Go to the United Nations‚Äô [National Accounts Main Aggregates Database website](https://tinyco.re/7226184). On the right-hand side of the page, under ‚ÄòData Availability‚Äô, click ‚ÄòDownloads‚Äô.\n",
    "- Under the subheading ‚ÄòGDP and its breakdown at constant 2015 prices in US Dollars‚Äô, select the Excel file ‚ÄòAll countries for all years ‚Äì sorted alphabetically‚Äô.\n",
    "- Save it in a subfolder of the directory you are coding in such that that the relative path is `data/Download-GDPconstant-USD-all.xlsx`.\n",
    "\n",
    "## Python Walkthrough 4.1\n",
    "\n",
    "**Importing the Excel file (`.xlsx` or `.xls`) into Python**\n",
    "\n",
    "First, make sure you move the saved the data to a folder called `data` that is a subfolder of your working directory. The working directory is the folder that your code 'starts' in, and the one that you open when you start Visual Studio Code. Let's say you called it `core`, then the file and folder structure of your working directory would look like this:\n",
    "\n",
    "```bash\n",
    "üìÅ core\n",
    "‚îÇ‚îÄ‚îÄüìÅdata\n",
    "   ‚îî‚îÄ‚îÄDownload-GDPconstant-USD-all.xlsx\n",
    "‚îÇ‚îÄ‚îÄempirical_project_4.py\n",
    "```\n",
    "\n",
    "This is similar to what you should see in Visual Studio Code under the explorer tab (although the working directory, `core`, won't appear). You can check your working directory by running\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.getcwd()\n",
    "```\n",
    "\n",
    "in Visual Studio Code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before importing the file into Python, open the file in Excel, OpenOffice, LibreOffice, or Numbers to see how the data is organized in the spreadsheet, and note that:\n",
    "\n",
    "- There is a heading that we don‚Äôt need, followed by a blank row.\n",
    "- The data we need starts on row three.\n",
    "\n",
    "Armed with this knowledge, we can import the data using the `Path` module to create the path to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(Path(\"data/Download-GDPconstant-USD-all.xlsx\"), skiprows=2)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.2\n",
    "\n",
    "**Making a frequency table**\n",
    "\n",
    "We want to create a table showing how many years of `Final consumption expenditure` data are available for each country.\n",
    "\n",
    "Looking at the dataset‚Äôs current format, you can see that countries and indicators (for example, `Afghanistan` and `Final consumption expenditure`) are row variables, while year is the column variable. This data is organized in ‚Äòwide‚Äô format (each individual‚Äôs information is in a single row).\n",
    "\n",
    "For many data operations and making charts it is more convenient to have indicators as column variables, so we would like `Final consumption expenditure` to be a column variable, and year to be the row variable. Each observation would represent the value of an indicator for a particular country and year. This data is organized in ‚Äòlong‚Äô format (each individual‚Äôs information is in multiple rows). This is also called 'tidy' data and it can be recognised by having variable per column and one observation per row. Many data scientists consider keeping data in tidy format good practice.\n",
    "\n",
    "To change data from wide to long format, we use the `pd.melt` method. The `melt` method is very powerful and useful, as you will find many large datasets are in wide format. In this case, `pd.melt` takes the data from all columns not specified as being `id_vars` (via a list of column names), and uses them to create two new columns: one contains the name of the row variable created from the former column names, which is the year here; we can set that new column's name with `var_name=\"year\"`. The second new column contains the values that were in the columns we unpivoted and is automatically given the name `value`. (We could have set a new name for this column by passing `value_name=` too.)\n",
    "\n",
    "Compare `df_long` to the wider `df` to understand how the melt command works. To learn more about organizing data in Python, see the [Working with Data](https://aeturrell.github.io/coding-for-economists/data-intro.html) section of 'Coding for Economists'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = pd.melt(\n",
    "    df, id_vars=[\"Area/CountryID\", \"Area/Country\", \"IndicatorName\"], var_name=\"year\"\n",
    ")\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the required table, we only need `Final consumption expenditure` of each country, which we extract using the `.loc` function. We'd like all columns so we pass the condition in the first position of `.loc` and leave the second entry as `:` for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = df_long.loc[df_long[\"IndicatorName\"] == \"Final consumption expenditure\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count = cons.groupby(\"Area/Country\").agg(available_years=(\"year\", \"count\"))\n",
    "year_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating the code in words: Take the variable `cons` and group the observations by area and country (`.groupby(Area/Country\")`), then take this result and aggregate `.agg` it such that a new variable called available years (`available_years=`) is created that sees the column year counted (`(\"year\", \"count\")`).\n",
    "\n",
    "Now we can establish how many of the 250 countries and areas in the dataset have complete information. A dataset is complete if it has the maximum number of available observations (given by `year_count[\"available_years\"].max()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(year_count[\"available_years\"] == year_count[\"available_years\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the full set of data are available for all countries and areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.3\n",
    "\n",
    "**Creating new variables**\n",
    "\n",
    "We will use Brazil, the US, and China as examples.\n",
    "\n",
    "Before we select these three countries, we will calculate the net exports (exports minus imports) for all countries, as we need that information in Python walkthrough 4.4. We will also shorten the names of the variables we need, to make the code easier to read. We will use a dictionary to map names into shorter formats. A dictionary is a built-in object type in Python and always has the structure `{key1: value1, key2: value2, ...}` where the keys and values could have any type (eg string, int, dataframe). In our case, both keys and values will be strings. We will use a convention for our naming that is known as \"snake case\". This means all lower case with spaces replaced by underscores (it looks a bit like a snake!). There are packages that will auto-rename long variables for you, but let's see how to do it manually here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_names_dict = {\n",
    "    \"Final consumption expenditure\": \"final_expenditure\",\n",
    "    \"Household consumption expenditure (including Non-profit institutions serving households)\": \"hh_expenditure\",\n",
    "    \"General government final consumption expenditure\": \"gov_expenditure\",\n",
    "    \"Gross capital formation\": \"capital\",\n",
    "    \"Imports of goods and services\": \"imports\",\n",
    "    \"Exports of goods and services\": \"exports\",\n",
    "}\n",
    "# rename these values\n",
    "df_long[\"IndicatorName\"] = df_long[\"IndicatorName\"].replace(short_names_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df_long` still has several rows for a particular country and year (one for each indicator). We will reshape this data using the `.pivot` method to ensure that we have only one row per country/area and per year. Note that `pivot` preserves the list of columns we pass as the `index=` and pivots the columns we pass to `columns=` out so that they are wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = df_long.pivot(\n",
    "    index=[\"Area/CountryID\", \"Area/Country\", \"year\"], columns=[\"IndicatorName\"]\n",
    ")\n",
    "df_table.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `df_long` previously had information in 5 columns, \"Area/CountryID\", \"Area/Country\", and \"year\", which are still columns as we declared them as the index of the new dataframe; \"IndicatorName\", the content of which has now become the variable names; and data in the \"values\" column. That last column was not mentioned in the previous line of code (it was clear to **pandas** what had been left out) but it is the values of that column that now appear in the body of the `df_table` dataframe. See what happens if you leave two of the original columns unmentioned, for instance take \"year\" out of the index list. You should find that you get an error message because it is then ambigious as to how to create `df_table` from `df_long`.\n",
    "\n",
    "\n",
    "Now we create a `net_exports` column based on the existing columns (exports - imports), and we can know that this will be a unique country/area and year combination for each row. First we need to drop the top level of the column index, which is currently called `value`: we don't need this anymore. This will allow for direct access to the `exports` and `imports` columns. We'll also reset the index to row numbers rather than those three columns we used in the pivot. We'll also remove the name of the columns as we won't need that any longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.columns = df_table.columns.droplevel()\n",
    "df_table = df_table.reset_index()\n",
    "df_table.columns.name = \"\"\n",
    "df_table[\"net_exports\"] = df_table[\"exports\"] - df_table[\"imports\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us select our three chosen countries to check that we calculated net exports correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_countries = [\"Brazil\", \"United States\", \"China\"]\n",
    "cols_to_keep = [\"Area/Country\", \"year\", \"exports\", \"imports\", \"net_exports\"]\n",
    "\n",
    "df_sel_un = df_table.loc[df_table[\"Area/Country\"].isin(sel_countries), cols_to_keep]\n",
    "df_sel_un.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.4\n",
    "\n",
    "**Plotting and annotating time series data**\n",
    "\n",
    "*Extract the relevant data*\n",
    "\n",
    "We will work with the `df_long` dataset, as the long format is well suited to produce charts with the **lets-plot** package. In this example, we use the US and China (which we will now save as the dataframe `comp`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a copy of df_long that is just US and China\n",
    "comp = df_long.loc[df_long[\"Area/Country\"].isin([\"United States\", \"China\"]), :].copy()\n",
    "# Convert value to billion USD\n",
    "comp[\"value\"] = comp[\"value\"] / 1.0e9\n",
    "# Filter down to certain cols and values\n",
    "comp = comp.loc[\n",
    "    comp[\"IndicatorName\"].isin(\n",
    "        [\"gov_expenditure\", \"hh_expenditure\", \"capital\", \"imports\", \"exports\"]\n",
    "    ),\n",
    "    [\"Area/Country\", \"year\", \"IndicatorName\", \"value\"],\n",
    "]\n",
    "comp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a line chart*\n",
    "\n",
    "We can now plot this data using the **lets-plot** data visualisation library. We'll subset to US data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(\n",
    "        comp.loc[comp[\"Area/Country\"] == \"United States\", :],\n",
    "        aes(\n",
    "            x=\"year\",\n",
    "            y=\"value\",\n",
    "            color=\"IndicatorName\",\n",
    "            linetype=\"IndicatorName\",\n",
    "        ),\n",
    "    )\n",
    "    + geom_line(size=1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 4.2 The US‚Äôs GDP components (expenditure approach).*\n",
    "\n",
    "There are plenty of problems with this chart:\n",
    "\n",
    "- the vertical axis label is uninformative\n",
    "- there is no chart title\n",
    "- the y-axis dips below zero\n",
    "- the legend is uninformative.\n",
    "\n",
    "\n",
    "To improve this chart, we add features to the figure by creating the axis, `ax`, explicitly. We'll also use a trick where we invert the dictionary from earlier and use this to supply full names to the legend via a new \"indicator\" column.\n",
    "\n",
    "Earlier, we had a dictionary, `short_names_dict`, that mapped the long names, eg \"Final consumption expenditure\", into short names, eg \"final_expenditure\". For making our plot, we'd like to reverse this. The first line of code helps with this by reversing the earlier dictionary‚Äîyou don't need to understand the details of this, just know that it's an inversion so that \"final_expenditure\" maps into \"Final consumption expenditure\" and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the dictionary from earlier\n",
    "rev_name_dict = {v: k.split(\"(\")[0] for k, v in short_names_dict.items()}\n",
    "# create a new col with the original names\n",
    "comp[\"Indicator\"] = comp[\"IndicatorName\"].replace(rev_name_dict)\n",
    "\n",
    "(\n",
    "    ggplot(\n",
    "        comp.loc[comp[\"Area/Country\"] == \"United States\", :],\n",
    "        aes(x=\"year\", y=\"value\", color=\"Indicator\", linetype=\"Indicator\"),\n",
    "    )\n",
    "    + geom_line(size=2)\n",
    "    + labs(\n",
    "        y=\"Billions USD\",\n",
    "        title=\"GDP components over time\",\n",
    "        color=\"Indicator\",\n",
    "    )\n",
    "    + ggsize(800, 500)\n",
    "    + scale_x_continuous(format=\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.3** The US‚Äôs GDP components (expenditure approach), amended chart.*\n",
    "\n",
    "We can make a chart for more than one country simultaneously using `comp` rather than just a slice of `comp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(comp, aes(x=\"year\", y=\"value\", color=\"Indicator\", linetype=\"Indicator\"))\n",
    "    + geom_line(size=2)\n",
    "    + labs(\n",
    "        y=\"Billions USD\",\n",
    "        title=\"GDP components over time\",\n",
    "        color=\"Indicator\",\n",
    "    )\n",
    "    + ggsize(1000, 500)\n",
    "    + scale_x_continuous(format=\"\")\n",
    "    + facet_wrap(facets=\"Area/Country\", ncol=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.4** GDP components over time (China and the US).*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.5\n",
    "\n",
    "**Calculating new variables and plotting time series data**\n",
    "\n",
    "*Calculate proportion of total GDP*\n",
    "\n",
    "We will use the `comp` dataset created in Python Walkthrough 4.4. First we will calculate net exports, as that contributes to GDP. As the data is currently in long format, we will reshape the data into wide format so that the variables we need are in separate columns instead of separate rows (using the `pivot` method, as in Python Walkthrough 4.3), calculate net exports, then transform the data back into long format using the `melt` method.\n",
    "\n",
    "On the way, we'll end up dropping the Indicator Names, and dropping the top level \"value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_wide = comp.drop(\"Indicator\", axis=1).pivot(\n",
    "    index=[\"Area/Country\", \"year\"], columns=\"IndicatorName\"\n",
    ")\n",
    "comp_wide.columns = comp_wide.columns.droplevel()\n",
    "comp_wide = comp_wide.reset_index()\n",
    "comp_wide.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to figure out what `.droplevel()` and `.reset_index()` do, you should comment out each of the two lines in turn and re-run the block of code to see what difference these lines make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new column for net exports = exports ‚Äì imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_wide[\"net_exports\"] = comp_wide[\"exports\"] - comp_wide[\"imports\"]\n",
    "comp_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to long format with the household expenditure, capital, and net export variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp2 = pd.melt(\n",
    "    comp_wide.drop([\"exports\", \"imports\"], axis=1),\n",
    "    id_vars=[\"year\", \"Area/Country\"],\n",
    "    var_name=\"indicator\",\n",
    "    value_name=\"2015_bn_usd\",\n",
    ")\n",
    "comp2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a new dataframe (`props`) also containing the proportions for each GDP component (`proportion`), using method chaining to link functions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = comp2.assign(\n",
    "    proportion=comp2.groupby([\"Area/Country\", \"year\"])[\"2015_bn_usd\"].transform(\n",
    "        lambda x: x / x.sum()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, we did the following: Take the `comp2` dataframe and add in a new column called `proportion` (this bit starts with `.assign(proportion=`) that, within area and year groups (`.groupby([\"Area/Country\", \"year\"])`) takes the value (`[\"2015_bn_usd\"]`) and divides it by the total value for that group (`.transform(lambda x: x/x.sum())`). For example, the first row gives the proportion of capital for China in 1970.\n",
    "\n",
    "The result is then saved in props. Look at the props dataframe to confirm that the above command has achieved the desired result. (You can check the answer with `props.groupby([\"Area/Country\", \"year\"])[\"proportion\"].sum()`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a line chart*\n",
    "\n",
    "Now we redo the line chart from Python Walkthrough 4.4 using the variable `props`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dictionary for net exports, which is new\n",
    "rev_name_dict.update({\"net_exports\": \"Net exports\"})\n",
    "props[\"Component\"] = props[\"indicator\"].map(rev_name_dict)\n",
    "(\n",
    "    ggplot(\n",
    "        props, aes(x=\"year\", y=\"proportion\", color=\"Component\", linetype=\"Component\")\n",
    "    )\n",
    "    + geom_line(size=2)\n",
    "    + facet_wrap(facets=\"Area/Country\", ncol=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.5** GDP component proportions over time (China and the US).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.6\n",
    "\n",
    "**Creating stacked bar charts**\n",
    "\n",
    "*Calculate proportion of total GDP*\n",
    "\n",
    "This walk-through uses the following countries (chosen at random):\n",
    "\n",
    "- developed countries: Germany, Japan, United States\n",
    "- transition countries: Albania, Russian Federation, Ukraine\n",
    "- developing countries: Brazil, China, India.\n",
    "\n",
    "The relevant data are still in the `df` dataframe. Before we select these countries, we first calculate the required proportions for all countries for capital, final expenditure, and net exports (out of those columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_track = [\"capital\", \"final_expenditure\", \"net_exports\"]\n",
    "countries_to_use = [\n",
    "    \"Germany\",\n",
    "    \"Japan\",\n",
    "    \"United States\",\n",
    "    \"Albania\",\n",
    "    \"Russian Federation\",\n",
    "    \"Ukraine\",\n",
    "    \"Brazil\",\n",
    "    \"China\",\n",
    "    \"India\",\n",
    "]\n",
    "\n",
    "# Find the proportions for these columns and create new columns called \"prop_\" + original col name\n",
    "for col in columns_to_track:\n",
    "    df_table[\"prop_\" + col] = df_table[col].divide(\n",
    "        df_table[columns_to_track].sum(axis=1)\n",
    "    )\n",
    "\n",
    "# filter this down to 2015 for the countries and cols we want\n",
    "cols_to_keep = [\"prop_\" + col for col in columns_to_track] + [\"Area/Country\", \"year\"]\n",
    "df_2015 = df_table.loc[\n",
    "    (df_table[\"Area/Country\"].isin(countries_to_use)) & (df_table[\"year\"] == 2015),\n",
    "    cols_to_keep,\n",
    "]\n",
    "df_2015.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot a stacked bar chart*\n",
    "\n",
    "Now let‚Äôs create the bar chart. First we need to melt the data into a format where each row is an observation, each column a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015_melt = pd.melt(\n",
    "    df_2015,\n",
    "    id_vars=[\"Area/Country\", \"year\"],\n",
    "    value_name=\"Percent\",\n",
    "    var_name=[\"Component\"],\n",
    ")\n",
    "df_2015_melt[\"Percent\"] = df_2015_melt[\"Percent\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(df_2015_melt, aes(x=\"Area/Country\", y=\"Percent\", fill=\"Component\"))\n",
    "    + geom_bar(stat=\"identity\")\n",
    "    + coord_flip()\n",
    "    + labs(fill=\"Components of GDP\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 4.6** GDP component proportions in 2015.*\n",
    "\n",
    "Note that even when a country has a trade deficit (proportion of net exports < 0), the proportions will add up to 1, but the proportions of final expenditure and capital will add up to more than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIND OUT MORE**\n",
    "\n",
    "*The natural log: What it means, and how to calculate it in Python*\n",
    "\n",
    "The natural log turns a linear variable into a concave variable, as shown in Figure 4.9. For any value of income on the horizontal axis, the natural log of that value on the vertical axis is smaller. At first, the difference between income and log income is not that big (for example, an income of 2 corresponds to a natural log of 0.7), but the difference becomes bigger as we move rightwards along the horizontal axis (for example, when income is 100,000, the natural log is only 11.5).\n",
    "\n",
    "![](https://www.core-econ.org/doing-economics/book/images/web/figure-04-05.jpg)\n",
    "\n",
    "***Figure 4.9** Comparing income with the natural logarithm of income.*\n",
    "\n",
    "The reason why natural logs are useful in economics is because they can represent variables that have diminishing marginal returns: an additional unit of input results in a smaller increase in the total output than did the previous unit. (If you have studied production functions, then the shape of the natural log function might look familiar.)\n",
    "\n",
    "When applied to the concept of wellbeing, the ‚Äòinput‚Äô is income, and the ‚Äòoutput‚Äô is material wellbeing. It makes intuitive sense that a $100 increase in per capita income will have a much greater effect on wellbeing for a poor country compared to a rich country. Using the natural log of income incorporates this notion into the index we create. Conversely, the notion of diminishing marginal returns (the larger the value of the input, the smaller the contribution of an additional unit of input) is not captured by GDP per capita, which uses actual income and not its natural log. Doing so makes the assumption that a $100 increase in per capita income has the same effect on wellbeing for rich and poor countries.\n",
    "\n",
    "The **numpy** log function in Python calculates the natural log of a value for you. To calculate the natural log of a value, `x`, type `np.log(x)`. If you have a scientific calculator, you can check that the calculation is correct by using the ln or log key.\n",
    "\n",
    "Now that you know about the natural log, you might want to go back to Question 3(c) in Part 4.1, and create a new chart using the natural log scale. The natural log is used in economics because it approximates percentage changes i.e. log(x) ‚Äì log(y) is a close approximation to the percentage change between x and y. So, using the natural log scale, you will be able to ‚Äòread off‚Äô the relative growth rates from the slopes of the different series you have plotted. For example, a 0.01 change in the vertical axis value corresponds to a 1% change in that variable. This will allow you to compare the growth rates of the different components of GDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.7\n",
    "\n",
    "**Calculating the HDI**\n",
    "\n",
    "We will use `pd.read_excel` to import the data file, which we saved using its default name of '2020_Statistical_Annex_Table_1.xlsx‚Äô in the data folder within our working directory. Before importing, look at the Excel file so that you understand its structure and how it corresponds to the code options used below. It's a long way from being a neat and tidy dataset! We will save the imported data as the `df_hdr` dataframe. Having taken a look at the Excel file, we can see we should skip the first few rows and take row 1 as the header (which becomes our column names).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = pd.read_excel(\n",
    "    Path(\"data/2020_Statistical_Annex_Table_1.xlsx\"), skiprows=3, header=1\n",
    ")\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `df_hdr` dataframe, there are rows that have information that isn‚Äôt data (for example, all the rows with an ‚ÄòNaN‚Äô in), as well as variables/columns that do not contain data, or are a mix of 'NaN' and data.\n",
    "\n",
    "Cleaning up the dataframe can be easier to do in Excel by deleting irrelevant rows and columns, but one advantage of doing it in Python is replicability. Suppose in a year‚Äôs time you carried out the analysis again with an updated spreadsheet containing new information. If you had done the cleaning in Excel, you would have to redo it from scratch, but if you had done it in Python, you can simply rerun the code below. (This works for new data that are in the same format too.)\n",
    "\n",
    "Let's do some data cleaning.\n",
    "\n",
    "First, we rename the last column by picking up the year entry below it in order to distinguish between different years of HDI rank. Next, we replace any columns that are \"Unnamed\" with entries from the first row of observations with a list comprehension. Then we eliminate rows that do not have any numbers in the `\"HDI rank\"` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = df_hdr.rename(columns={\"HDI rank\": \"HDI_rank_\" + str(df_hdr.iloc[1, -1])})\n",
    "df_hdr.columns = [\n",
    "    df_hdr.columns[i] if \"Unnamed\" not in df_hdr.columns[i] else df_hdr.iloc[0, i]\n",
    "    for i in range(len(df_hdr.columns))\n",
    "]\n",
    "df_hdr = df_hdr.loc[~pd.isna(df_hdr[\"HDI rank\"]), :]\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can eliminate rows in HDI rank that do not have numbers in, and, following that, eliminate columns that contain NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = df_hdr.loc[~pd.isna(df_hdr[\"HDI_rank_2018\"]), :]\n",
    "df_hdr = df_hdr.dropna(axis=1, how=\"any\")\n",
    "df_hdr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's switch to shorter column names and check what datatypes we have in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\n",
    "    \"hdi_rank\",\n",
    "    \"country\",\n",
    "    \"hdi\",\n",
    "    \"life_exp\",\n",
    "    \"exp_yrs_school\",\n",
    "    \"mean_yrs_school\",\n",
    "    \"gni_capita\",\n",
    "    \"gni_hdi_rank\",\n",
    "    \"hdi_rank_2018\",\n",
    "]\n",
    "df_hdr.columns = new_column_names\n",
    "df_hdr.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the structure of the data, we see that **pandas** thinks that all the data are objects because the original datafile contained non-numerical entries (these rows have now been deleted). Apart from the `\"country\"` variable, which we want to be a categorical variable, all variables should be doubles or ints. \n",
    "\n",
    "Now, we could do this individually, by running:\n",
    "\n",
    "```python\n",
    "df_hdr = df_hdr.astype({\"hdi_rank\": \"int32\"})\n",
    "```\n",
    "\n",
    "and similar for each name and its corresponding type. Note that the structure `{ ... : ...}` is a dictionary, which has *keys* and *values*.\n",
    "\n",
    "However, it would be quite tedious to have to retype the same line over and over again. Instead we can pass on dictionary with key-value pairs for all mappings of names to data types we'd like to have in our dataframe.\n",
    "\n",
    "To do this, we'll use a trick where we 'zip' two variables (the column names and datatypes) together into a dictionary that maps the column name into the datatype we'd it to have. The zip function brings an element of two lists together in turn, a bit like a zipper on your clothes brings two interlocking plastic teeth together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_datatypes = [\n",
    "    \"int\",\n",
    "    \"category\",\n",
    "    \"double\",\n",
    "    \"double\",\n",
    "    \"double\",\n",
    "    \"double\",\n",
    "    \"double\",\n",
    "    \"int\",\n",
    "    \"int\",\n",
    "]\n",
    "dict_of_names_to_types = {k: v for k, v in zip(new_column_names, new_column_datatypes)}\n",
    "dict_of_names_to_types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary of all names and their associated types that we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr = df_hdr.astype(dict_of_names_to_types)\n",
    "df_hdr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a nice clean dataset that we can work with.\n",
    "\n",
    "We start by calculating the three indices, using the information given. For the education index we calculate the index for expected and mean schooling separately, then take the arithmetic mean to get `i_education`. As some mean schooling observations exceed the specified ‚Äòmaximum‚Äô value of 18, the calculated index values would be larger than 1. To avoid this, we first replace these observations with 18 to obtain an index value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr.loc[df_hdr[\"exp_yrs_school\"] > 18, \"exp_yrs_school\"] = 18\n",
    "\n",
    "# Now create the indices\n",
    "df_hdr[\"i_health\"] = (df_hdr[\"life_exp\"] - 20) / (85 - 20)\n",
    "df_hdr[\"i_education\"] = (\n",
    "    ((df_hdr[\"exp_yrs_school\"] - 0) / (18 - 0))\n",
    "    + (df_hdr[\"mean_yrs_school\"] - 0) / (15 - 0)\n",
    ") / 2\n",
    "df_hdr[\"i_income\"] = (np.log(df_hdr[\"gni_capita\"]) - np.log(100)) / (\n",
    "    np.log(75000) - np.log(100)\n",
    ")\n",
    "df_hdr[\"hdi_calc\"] = np.power(\n",
    "    df_hdr[\"i_health\"] * df_hdr[\"i_education\"] * df_hdr[\"i_income\"], 1 / 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the `HDI` given in the table and our calculated HDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdr[[\"hdi\", \"hdi_calc\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDI is one way to measure wellbeing, but you may think that it does not use the most appropriate measures for the non-material aspects of wellbeing (health and education).\n",
    "\n",
    "https://databank.worldbank.org/\n",
    "\n",
    "Now we will use the same method to create our own index of non-material wellbeing (an ‚Äòalternative HDI‚Äô), using different indicators. You can find alternative indicators to measure health and education on the [World Bank data bank](https://databank.worldbank.org) under \"World Development Indicators\". Use the interactive menus to select all years, all countries, and then the indicators from the Walkthrough below.\n",
    "\n",
    "1. Create an alternative index of wellbeing. In particular, propose alternative Education and Health indices in (a) and (b), then combine these with the existing Income index in (c) to calculate an alternative HDI. Examine whether the changes caused substantial changes in country rankings in (d).\n",
    "\n",
    "a) Choose two to three indicators to measure health, and two to three indicators to measure education. Explain why you have chosen these indicators.\n",
    "\n",
    "b) Carefully merge the data into your existing data. Choose a reasonable maximum and minimum value for each indicator and justify your choices.\n",
    "\n",
    "c) Calculate your alternative versions of the education and health dimension indices. Since you have chosen more than one indicator for this dimension, make sure to average the dimension indices as done in Question 3\n",
    "(b). Also ensure that higher indicator values always represent better outcomes. Now calculate the alternative HDI as done in Questions 3 and 4. Remember to combine your alternative education and health indices with the existing income index from Question 2.\n",
    "\n",
    "d) Create a new variable showing each country‚Äôs rank according to your alternative HDI, where 1 is assigned to the country with the highest value. Compare your ranking to the HDI rank. Are the rankings generally similar, or very different? (See R walk-through 4.8 on how to do this.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 4.8\n",
    "\n",
    "**Creating your own HDI**\n",
    "\n",
    "*Merge data and calculate alternative indices*\n",
    "\n",
    "This example uses the following indicators:\n",
    "\n",
    "- Education: Literacy rate, adult (% ages 15 and older); School enrolment, tertiary (% gross); Trained teachers in primary education (%)\n",
    "\n",
    "- Health: Prevalence of stunting, height for age (% under age 5); Mortality rate, female adult (per 1,000 female adults); Mortality rate, male adult (per 1,000 male adults).\n",
    "\n",
    "- Income: GNP per capita (in constant 2015 USD).\n",
    "\n",
    "You'll need to select these indices and then use the 'Download Options' buttons to get them as a CSV file.\n",
    "\n",
    "First, we import the data and check that it has been imported correctly. You can see that each row represents a different country, and each column represents a different year-indicator combination. Note that the data come with a good number of empty rows at the end before some meta-data that have been inserted, so we drop those rows that have NaNs in the Country Code column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hdr = pd.read_csv(Path(\"data/world_bank_data.csv\")).dropna(\n",
    "    subset=[\"Country Code\"], axis=0\n",
    ")\n",
    "all_hdr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to filter to just the year we want. We'll do this in a new data frame. Note that two dots (`..`) are a string that indicates a missing variable. Unfortunately, if we have a mix of strings and other data types (like double), **pandas** isn't sure how to treat the column. So we need to replace the code for missing values in a string, \"..\", with \"proper\" numeric missing values which will allow us to do operations like arithmetic on the column. To do this, we use the `pd.to_numeric` function‚Äîonce it has been applied using the `errors=\"coerce\"` option, you'll see that any occurrences of `..` have been converted to `NaN`, which means not a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_2018 = all_hdr.loc[\n",
    "    :, [\"Country Name\", \"Country Code\", \"Series Name\", \"Series Code\", \"2018 [YR2018]\"]\n",
    "]\n",
    "hdr_2018[\"2018 [YR2018]\"] = pd.to_numeric(\n",
    "    hdr_2018[\"2018 [YR2018]\"], errors=\"coerce\"\n",
    ").fillna(np.nan)\n",
    "hdr_2018.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease, we'll introduce some short name versions of the series names and we will do some additional data typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_shorter_hdi_names = {\n",
    "    \"SE.TER.ENRR\": \"enrollment\",\n",
    "    \"SE.PRM.TCAQ.ZS\": \"trained_teachers\",\n",
    "    \"SE.ADT.LITR.ZS\": \"literacy\",\n",
    "    \"SP.DYN.AMRT.MA\": \"mortality_m\",\n",
    "    \"SP.DYN.AMRT.FE\": \"mortality_f\",\n",
    "    \"SH.STA.STNT.ZS\": \"stunting\",\n",
    "    \"NY.GNP.PCAP.KD\": \"income\",\n",
    "}\n",
    "\n",
    "dict_of_types = {\n",
    "    \"Country Name\": \"category\",\n",
    "    \"Country Code\": \"category\",\n",
    "    \"Series Code\": \"category\",\n",
    "    \"indicator\": \"category\",\n",
    "    \"Series Name\": \"string\",\n",
    "}\n",
    "\n",
    "hdr_2018[\"indicator\"] = hdr_2018[\"Series Code\"].map(dict_to_shorter_hdi_names)\n",
    "hdr_2018 = hdr_2018.astype(dict_of_types)\n",
    "hdr_2018.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at some summary statistics; we'll use the **skimpy** package.\n",
    "\n",
    "You will need to install skimpy on the command line of your computer by typing\n",
    "\n",
    "```bash\n",
    "pip install skimpy\n",
    "```\n",
    "\n",
    "before running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skim(hdr_2018)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can calculate indices, we need to set minimum and maximum values, which we base on the minimum and maximum values in the sample. Let's look at the max and min by indicator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = hdr_2018.groupby([\"indicator\"])[\"2018 [YR2018]\"].agg([\"min\", \"max\"])\n",
    "min_max"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want observations to be within the interval of min to max. Let's do this systematically for all of the indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_2018[\"index_indicator\"] = hdr_2018.groupby([\"indicator\"])[\n",
    "    \"2018 [YR2018]\"\n",
    "].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "hdr_2018.head(7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see that the first row is the original value for literacy for Sri Lanka with the minimum literacy subtracted divided by the difference between the max and min for literacy.\n",
    "\n",
    "Now we'll build the two new indicators based on subsets of the existing indicators. To do this, we'll aggregate over existing indicators‚Äîso we're going to end up a smaller data frame that only has each country in it once. For neatness, we can also pack these into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create educ index\n",
    "educ_index_indicators = [\"literacy\", \"enrollment\", \"trained_teachers\"]\n",
    "hdr_2018_educ = (\n",
    "    hdr_2018.loc[hdr_2018[\"indicator\"].isin(educ_index_indicators), :]\n",
    "    .groupby([\"Country Code\"])[\"index_indicator\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# create health index. These indicators are less good when the number is high\n",
    "# so we invert this measure\n",
    "health_index_indicators = [\"stunting\", \"mortality_m\", \"mortality_f\"]\n",
    "hdr_2018_health = (\n",
    "    1.0\n",
    "    - hdr_2018.loc[hdr_2018[\"indicator\"].isin(health_index_indicators), :]\n",
    "    .groupby([\"Country Code\"])[\"index_indicator\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# create income indicator\n",
    "hdr_2018_income = (\n",
    "    hdr_2018.loc[hdr_2018[\"indicator\"].isin([\"income\"]), :]\n",
    "    .groupby([\"Country Code\"])[\"index_indicator\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# combine them all\n",
    "hdr_2018_long = pd.concat([hdr_2018_educ, hdr_2018_health, hdr_2018_income], axis=1)\n",
    "# give each sensible names (they both inherit column name of \"index_indicator\" by default)\n",
    "hdr_2018_long.columns = [\"educ_index\", \"health_index\", \"income_index\"]\n",
    "skim(hdr_2018_long.reset_index())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to merge our two new indicators back into the main dataset on the same basis as the existing ones. Our new indices are in a different shape, so to do this we have to either reshape `hdr_2018` to be in wide format or reshape `hdr_2018_long` to be in wide format. Let's try the latter, adding in the \"missing\" columns as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_indices_wide = pd.melt(\n",
    "    hdr_2018_long.reset_index(),\n",
    "    id_vars=\"Country Code\",\n",
    "    var_name=\"indicator\",\n",
    "    value_name=\"index_indicator\",\n",
    ")\n",
    "country_code_to_name = dict(\n",
    "    zip(\n",
    "        hdr_2018[\"Country Code\"].drop_duplicates(),\n",
    "        hdr_2018[\"Country Name\"].drop_duplicates(),\n",
    "    )\n",
    ")\n",
    "new_indices_wide[\"Series Name\"] = new_indices_wide[\"indicator\"]\n",
    "new_indices_wide[\"Series Code\"] = new_indices_wide[\"indicator\"]\n",
    "new_indices_wide[\"Country Name\"] = new_indices_wide[\"Country Code\"].map(\n",
    "    country_code_to_name\n",
    ")\n",
    "hdr_2018 = pd.concat([new_indices_wide, hdr_2018], axis=0)\n",
    "hdr_2018"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have added both of the alternative indicators in, we are now in a position to compute our own HDI indicator. First, we'll flip the data to wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_2018_w = hdr_2018.pivot(\n",
    "    index=[\"Country Code\", \"Country Name\"],\n",
    "    columns=\"indicator\",\n",
    "    values=\"index_indicator\",\n",
    ")\n",
    "hdr_2018_w = hdr_2018_w.assign(\n",
    "    hdi_own=lambda x: np.power(\n",
    "        x[\"educ_index\"] * x[\"health_index\"] * x[\"income_index\"], 1 / 3\n",
    "    )\n",
    ")\n",
    "skim(hdr_2018_w)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our shiny new HDI has 80 missing values (37% of all entries) because of the high number of entries that feed into it being invalid too."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Ranks\n",
    "\n",
    "To compare the ranks of the two indices (the original HDI and our alternative HDI), we should only rank the countries that have observations for both indices. We will create a dataframe called `HDR2018_sub` that contains this subset of countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we're not now interested in\n",
    "hdr_2018_w_sub = hdr_2018_w.dropna(subset=[\"hdi_own\"]).drop(\n",
    "    health_index_indicators + educ_index_indicators + [\"income\"], axis=1\n",
    ")\n",
    "hdr_2018_w_sub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we retain 137 countries for which there is sufficient information to produce our special HDI.\n",
    "\n",
    "We'll now attempt to merge `hdr_2018_w_sub`, which has our hdi indicator in it, with the one we prepared earlier. The only overlapping column is one that contains country name‚Äîthough this is called something different in each dataset, so we must rename the relevant column in at least one of the two dataframes when merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_merged = pd.merge(\n",
    "    hdr_2018_w_sub.reset_index().rename(columns={\"Country Name\": \"country\"}),\n",
    "    df_hdr.drop(\n",
    "        [\"mean_yrs_school\", \"life_exp\", \"exp_yrs_school\", \"gni_capita\"], axis=1\n",
    "    ),\n",
    "    on=[\"country\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "hdr_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a succesful inner merge for 120 countries. Finally, we're going to compute a rank of countries based on our own hdi in 2018. We use the `ascending=False` option in the `.rank` method so as to make the \"best\" value have rank 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_merged[\"hdi_own_rank\"] = hdr_merged[\"hdi_own\"].rank(ascending=False).astype(\"int\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this done, we can make a scatter plot of the ranks compared with one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(hdr_merged, aes(x=\"hdi_rank_2018\", y=\"hdi_own_rank\")) + geom_point(size=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 4.10 Scatterplot of ranks for HDI and alternative HDI index.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve this chart quite a bit. One particularly useful addition would be some text showing some key values. Using **lets-plots**' system for mapping variables onto layers of a chart, we can do this fairly easily using long format data by adding a column called \"text\" that has an entry only every ten values (otherwise it would overwhelm the chart). We first sort by the 2018 official HDI rank, then create an empty text column, and then add the real country name into the text column only for every tenth entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_merged = hdr_merged.sort_values(\"hdi_rank_2018\")\n",
    "hdr_merged[\"text\"] = \"\"\n",
    "hdr_merged.iloc[::10, -1] = hdr_merged.iloc[::10, 1]\n",
    "hdr_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(hdr_merged, aes(x=\"hdi_rank_2018\", y=\"hdi_own_rank\"))\n",
    "    + geom_point(size=3)\n",
    "    + geom_text(aes(label=\"text\"), size=5)\n",
    "    + labs(\n",
    "        x=\"Rank HDI (2018)\",\n",
    "        y=\"Rank of custom HDI (2018)\",\n",
    "        title=\"Comparing two human development indices\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 4.11 Scatterplot of ranks for HDI and alternative HDI index with text.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in general the rankings are similar. If they were identical, the points in the scatterplot would form a straight upward-sloping line. They do not form a straight line, but there is a very strong positive correlation. There are, however, a few countries where the alternative definitions have caused a change in ranking, so let's look at where the alternative definitions have caused a big change in ranking using a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_merged = hdr_merged.assign(\n",
    "    rank_diff=hdr_merged[[\"hdi_own_rank\", \"hdi_rank_2018\"]].diff(axis=1)[\n",
    "        \"hdi_rank_2018\"\n",
    "    ]\n",
    ").sort_values(\"rank_diff\", ascending=False)\n",
    "hdr_merged.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr_merged.tail(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can see that countries such as Djibouti, Sudan, Niger, and Burkina Faso do much better on the custom ranking. Meanwhile, Hungary, Czechia, and Georgia have fared somewhat less well on this new ranking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter used the following packages where *sys* is the Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "26c4b888616894cf8360ee3d370b6a41732ef00c8f1d61e869c42a8428cf1ac1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
