{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Project 3\n",
    "\n",
    "## Python-specific learning objectives\n",
    "\n",
    "In addition to the learning objectives for this project, in this section you will learn how to TODO\n",
    "\n",
    "## Getting started in Python\n",
    "\n",
    "TODO (list packages needed here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 3.1\n",
    "\n",
    "**Importing the datafile into Python**\n",
    "\n",
    "First, we need to import the packages and settings we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "import seaborn as sns\n",
    "import pingouin as pg\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Set the plot style for prettier charts:\n",
    "plt.style.use(\n",
    "    \"https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt\"\n",
    ")\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 3]\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "# Ignore warnings to make nice output\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored in `.xlsx` format, so we use the `pd.read_excel` function from the **pandas** package to import it. We also import the **matplotlib** library as this includes packages that we will use later for drawing charts.\n",
    "\n",
    "If you open the data in Microsoft's Excel, Apple's Numbers, or LibreOffice's Calc (free), you will see that there are two tabs: ‘Data Dictionary’, which contains some information about the variables, and ‘Data’, which contains the actual data. Let’s import both into Python, so that we do not need to refer back to Excel for the additional information about variables.\n",
    "\n",
    "Remember to store the downloaded file in a subdirectory of your current working directory called `data`. Let's load up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = Path(\"data/Dataset Project 3.xlsx\")\n",
    "\n",
    "var_info = pd.read_excel(path_to_data, sheet_name=\"Data Dictionary\")\n",
    "data = pd.read_excel(path_to_data, sheet_name=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `info()` method to ensure that all the variables were given the correct *datatypes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python classified all the variables containing numbers as either `int64` (integers) or `float64` (real numbers). However, other variables got labelled as `object` because they don't numbers, and some of the columns labelled as numbers are actually categorical variables.\n",
    "\n",
    "Let's ensure that `type`, `taxed`, `supp`, `store_id`, `store_type`, `type2` and `product_id` are represented as categorical variables. We'll use the `astype` function to convert these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"type\", \"taxed\", \"supp\", \"store_id\", \"store_type\", \"type2\", \"product_id\"]:\n",
    "    data[col] = data[col].astype(\"category\")\n",
    "\n",
    "# rename some of the categories\n",
    "data[\"taxed\"] = data[\"taxed\"].cat.rename_categories([\"not taxed\", \"taxed\"])\n",
    "data[\"supp\"] = data[\"supp\"].cat.rename_categories([\"standard\", \"supplemental\"])\n",
    "data[\"store_type\"] = data[\"store_type\"].cat.rename_categories([\"Large supermarket\", \"Small supermarket\", \"Pharmacy\", \"Gas station\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we used `.cat.rename_categories` to give more meaningful names to different categories (where they are clearly defined).\n",
    "\n",
    "There is another variable, `time`, which is classified as having object datatype, but should be a categorical variable. Before we do this, we use the unique command to check the categories of this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"time\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the timeline in the [Silver et al. (2017) paper](https://doi.org/10.1371/journal.pmed.1002283), you will notice that the third price survey was in March 2016, not in March 2015, so the data has been labelled incorrectly. We shall therefore change all the values `MAR2015` to `MAR2016`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"time\"]==\"MAR2015\", \"time\"] = \"MAR2016\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now change `time` into a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"time\"] = data[\"time\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 3.2\n",
    "\n",
    "**Counting the number of unique elements in a variable**\n",
    "\n",
    "We use two functions here: unique to obtain a list of the unique elements of the variables of interest (`data[\"store_id\"]` and `data[\"product_id\"]`), then `len` to count how long the list is. We will create two variables, `no_stores` and `no_products`, that contain the number of stores and products respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stores = len(data[\"store_id\"].unique())\n",
    "no_products = len(data[\"product_id\"].unique())\n",
    "print(f\"The {no_stores=} while the {no_products=}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 3.3\n",
    "\n",
    "**Creating frequency tables**\n",
    "\n",
    "### Frequency table for store type and time period.\n",
    "\n",
    "We use the `pd.crosstab` function to perform a cross-tabulation count of `store_type` by `time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=data[\"store_type\"], columns=data[\"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are fewer observations taken from gas stations and pharmacies and more from small supermarkets.\n",
    "\n",
    "### Frequency table for store type and taxed\n",
    "\n",
    "Now we repeat the steps above to make a frequency table with `store_type` as the row variable and `taxed` as the column variable. We pass a list of dataframe columns to the `pd.crosstab` function because we also want separate values for each time period. To get the total row each row and column, we add `margins=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=data[\"store_type\"], columns=[data[\"time\"], data[\"taxed\"]], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency table for product type and time period\n",
    "\n",
    "Now we make a frequency table with product type (`type`) as the row variable and time period (`time`) as the column variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=data[\"type\"], columns=data[\"time\"], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows that there were no observations for the category `Sport-diet` in March 2016. As this is a drink which even in the other months has very few observations, it may be a product that is offered only in one shop, and it is possible that this shop was not visited in March 2016. The product may also be a seasonal product that is not available in March. It is also likely that Water-sweet is offered in only one shop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 3.4\n",
    "\n",
    "**Calculating conditional means**\n",
    "\n",
    "Calculating conditional, or group, means is a common data cleaning operation you will encounter. It can be a bit confusing, but, in the typical use, case we might just be asking what the mean is for each period (instead of the mean for all periods). Furthermore, we must choose whether to bring the mean back into our data with the *original index* (and so the original number of rows) or with a new index that reflects what the mean is conditional on. We'll now see one way to do this.\n",
    "\n",
    "In order to identify products (`\"product_id\"`) that have observations for all three periods (`\"DEC2014\"`, `\"JUN2015\"` and `\"MAR2016\"`), we will first create a new variable called `period_test`, which takes the value `True` if we have observations in all periods for a product in a particular store, and `False` otherwise. These true/false variables are called ‘boolean variables’.\n",
    "\n",
    "This is going to be a complex operation, so hold on to your hats. First we need to group everything by `\"product_id\"` and `\"store_id\"` using a `groupby` operation. Then we'll select the column we want, `\"time\"`. Then we'll use `transform`: this returns a series with the same number of rows as the original index, so will fit nicely into our existing data frame (as opposed to having the index be set to the number of unique groups from the groupby). Within our transform, we'll ask for the number of unique (`nunique`) categories of time. For entries that exist, this will be 1, 2, or 3 depending on whether the product is available in 1, 2, or 3 of the periods. For entries that don't exist, this comes back as `NaN`, which means not a number. So we then use `.fillna(0)` to set these to zero. Finally we compare this object against the known unique number of different periods (3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby([\"product_id\", \"store_id\"])[\"time\"].transform(lambda x: x.nunique()).fillna(0) == data[\"time\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tip* The [**pandas** documentation](https://pandas.pydata.org/docs/index.html) provides lots of detailed examples of all of its extensive functionality, including the [`transform`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html?highlight=transform#pandas.core.groupby.DataFrameGroupBy.transform) function.\n",
    "\n",
    "Let's store this in the dataframe as a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"period_test\"] = data.groupby([\"product_id\", \"store_id\"])[\"time\"].transform(lambda x: x.nunique()).fillna(0) == data[\"time\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check this worked by picking out the first unique product_id, store_id combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data[\"product_id\"] == 29) & (data[\"store_id\"] == 16), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`period_test` reports `True` for this combination, and we cn see that all three periods are present. Let's try another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data[\"product_id\"] == 29) & (data[\"store_id\"] == 10), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the product-store cell is only there for two out of the three periods and so gets a `False` entry for its `\"period_test\"` column. \n",
    "\n",
    "Now we can use the `\"period_test\"` variable to remove all products that have not been observed in all three periods. We'll also filter by standard in the `\"supp\"` column. We will store the remaining products in a new dataframe, `data_c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_c = data.loc[ data[\"period_test\"] & (data[\"supp\"] == \"standard\")]\n",
    "data_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the means of `price_per_oz` by grouping the data according to `store_type`, `taxed`, and `time`. We'll use a technique called method chaining to do this: this just means chaining many successive methods together one after another. The first operation groups all of the variables together by `\"taxed\"`, `\"store_type\"`, and `\"time\"`. The second produces some aggregates via `.agg`, namely new columns `avg_price` and `n` that are given via the `(\"input column\", \"function to be applied\")` syntax. The new dataframe has the groupby variables as its index, so we reset the index (to row numbers) ready to perform a `pivot` to bring the data in the shape that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_res = (data_c\n",
    "    .groupby([\"taxed\", \"store_type\", \"time\"])\n",
    "    .agg(avg_price = (\"price_per_oz\", \"mean\"), n=(\"price_per_oz\", \"count\"))\n",
    "    .reset_index()\n",
    "    .pivot(index=[\"taxed\", \"store_type\", \"n\"], columns=\"time\", values=\"avg_price\")\n",
    ")\n",
    "table_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method chaining is a useful technique for data analysis, but it has pros and cons. If you know what a code is doing, and that it works already, it can be substantially easier to read than successive lines of logic using assignment via `=`. However, if something is going wrong within a long method chain, it can be hard to know where and why, and they don't always encourage modular code written via functions.\n",
    "\n",
    "You can find a deeper dive into method chaining in the [relevant section](https://aeturrell.github.io/coding-for-economists/data-intro.html#advanced-method-chaining) of [Coding for Economists](https://aeturrell.github.io/coding-for-economists).\n",
    "\n",
    "Use the [S3 Table](https://tinyco.re/9687764) in the journal paper to check how closely your summary data match those in the paper. You should find that your results for Large Supermarkets and Pharmacies match, but the other store types have discrepancies. In Python walk-through 3.5 we will discuss these differences in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 3.5\n",
    "\n",
    "**Making a column chart to compare two groups**\n",
    "\n",
    "### Calculate price differences by store type\n",
    "\n",
    "Let’s calculate the two price differences (June 2015 minus December 2014 and March 2016 minus December 2014), and store them as `\"d1\"` and `\"d2\"` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_res[\"d1\"] = table_res[\"JUN2015\"] - table_res[\"DEC2014\"]\n",
    "table_res[\"d2\"] = table_res[\"MAR2016\"] - table_res[\"DEC2014\"]\n",
    "table_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a column chart for average price changes\n",
    "\n",
    "To display `\"d1\"` and `\"d2\"` in a column chart, we will use the **seaborn** and **matplotlib** packages, which we loaded at the start, in Python Walkthrough 3.1. This Let’s start with displaying the average price change from December 2014 to June 2015 (which is stored in `\"d1\"`).\n",
    "\n",
    "We will use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(table_res.reset_index(), \n",
    "  aes(fill = \"taxed\", y = \"d1\", x = \"store_type\")) +\n",
    "  geom_bar(position = \"dodge\", stat = \"identity\") +\n",
    "  # Add the axes labels\n",
    "  labs(y = \"Price change (US$/oz)\", x = \"Store type\") +\n",
    "  # Add the title and legend labels\n",
    "  scale_fill_discrete(name = \"Beverages\") +\n",
    "  ggtitle(\"Average price change from Dec 2014 to Jun 2015\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3.2** Average price change from December 2014 to June 2015.\n",
    "\n",
    "Now we do the same for the price change from Dec 2014 to Mar 2016:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(table_res.reset_index(), \n",
    "  aes(fill = \"taxed\", y = \"d2\", x = \"store_type\")) +\n",
    "  geom_bar(position = \"dodge\", stat = \"identity\") +\n",
    "  # Add the axes labels\n",
    "  labs(y = \"Price change (US$/oz)\", x = \"Store type\") +\n",
    "  # Add the title and legend labels\n",
    "  scale_fill_discrete(name = \"Beverages\") +\n",
    "  ggtitle(\"Average price change from Dec 2014 to Mar 2016\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3.3** Average price change from December 2014 to March 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Python Walkthrough 3.6\n",
    "\n",
    "**Calculating the p-value for price changes**\n",
    "\n",
    "In this walk-through, we show the calculations required to obtain the p- values in Table S3 of the Silver et al. (2017) paper. Since the p-values are already provided, this walk-through is only for those who want to see how these p-values were calculated. For the categories of Large Supermarkets and Pharmacies, we conduct a hypothesis test, which tests the null hypothesis that the price difference between June 2015 and December 2014 (and March 2016 and December 2014) for the taxed and untaxed beverages in the two store types could be due to chance.\n",
    "\n",
    "We are interested in whether the difference in average price between JUN2015 and DEC2014 (or MAR2016 and DEC2014) for one group (say, Large Supermarket and taxed) is zero (i.e. there is no difference in the means of the two populations). Note that we are dealing with paired observations (the same product in both time periods).\n",
    "\n",
    "Let’s use the price difference between June 2015 and December 2014 in Large Supermarkets for taxed beverages as an example. First, we extract the prices for both periods (the vectors `p_1` and `p_2`) and then calculate the difference, element by element (stored as `\"Δ_t\"`). Because *all operations assume the same index*, we must also reset the indexes of these sets of values that have been drawn from different rows (dropping the original indexes as we go)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_1 = data_c.query(\"store_type == 'Large supermarket' & taxed == 'taxed' & time == 'DEC2014'\")[\"price_per_oz\"].reset_index(drop=True)\n",
    "p_2 = data_c.query(\"store_type == 'Large supermarket' & taxed == 'taxed' & time == 'JUN2015'\")[\"price_per_oz\"].reset_index(drop=True)\n",
    "\n",
    "# Price difference for taxed products in large supermarkets\n",
    "Δ_t = p_2 - p_1\n",
    "Δ_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three new variables are vectors with 36 elements. For `Δ_t` to correctly represent the price difference for a particular product in a particular store, we need to be certain that each element in both vectors corresponds to the same product in the same store. \n",
    "\n",
    "An alternative to the above, where one just hopes for the best on an ordered dataset, is to extract\n",
    "\n",
    "To check that the elements match, we will extract the store and product IDs along with the prices, and compare the ordering in p1_alt and p2_alt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt = (data_c.loc[(data_c[\"store_type\"] == \"Large supermarket\") & (data_c[\"taxed\"] == \"taxed\"), :]\n",
    "             .pivot([\"store_id\", \"product_id\"], columns=\"time\", values=\"price_per_oz\"))\n",
    "alt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply subtract columns, preserving index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_1_alt, p_2_alt = alt[\"DEC2014\"], alt[\"JUN2015\"]\n",
    "Δ_t_alt = p_2_alt - p_1_alt\n",
    "Δ_t_alt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case, we can also see which combination produced this particular value of the `price_per_oz`. This approach guarantees identical ordering because the subtraction would fail if the indices were different.\n",
    "\n",
    "The average value of the price difference is 0.1312, and our task is to evaluate whether this is likely to be due to sampling variation (given the assumption that there is no difference between the populations) or not. To do this, we can use the `ttest` function, which provides the associated p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognise that the differences come from paired samples.\n",
    "pg.ttest(p_2, p_1, paired=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can calculate the respective test statistic manually, which also requires us to calculate the standard error of this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Δ_t_alt.mean() / np.sqrt((Δ_t_alt.var()/len(Δ_t_alt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Another alternative* is to use the ttest function directly on `Δ_t`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the results to the journal paper, look at the extract from Table S3 (the section on Large Supermarkets) shown in Figure 3.4 below. The cell with ‘**’ shows the mean price difference of 1.31 cents ($0.0131)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Large supermarkets<br>(n = 6)        \t| Taxed beverage price<br>(36 sets) \t| Untaxed beverage price<br>(36 sets) \t| Taxed – untaxed difference \t|\n",
    "|--------------------------------------\t|:---------------------------------:\t|:-----------------------------------:\t|:--------------------------:\t|\n",
    "|                                      \t| cents/oz                          \t|                            cents/oz \t|                   cents/oz \t|\n",
    "| Round 1: December 2014               \t|                             15.62 \t|                               11.19 \t|                            \t|\n",
    "| Round 2: June 2015                   \t|                             16.93 \t|                               11.48 \t|                            \t|\n",
    "| Round 3: March 2016                  \t|                             16.68 \t|                               11.70 \t|                            \t|\n",
    "| Mean change<br>(March 2016–Dec 2014) \t|                    1.07, (p=0.01) \t|                      0.51, (p=0.01) \t|             0.56, (p=0.22) \t|\n",
    "| Mean change<br>(June 2015–Dec 2014)  \t|                 1.31, (p<0.001)** \t|                      0.29, (p=0.08) \t|            1.02, (p=0.002) \t|\n",
    "\n",
    "**Figure 3.4** Table S3 in Silver et al. (2017), showing means and confidence intervals. n = number of stores of each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our test output we get a very small p-value (0.000032) which in the table is indicated by the double asterisk.\n",
    "\n",
    "Tests for other store types are calculated similarly, by changing the data extracted to `p_1` and `p_2`. Let’s do that for one more example: the price difference between June 2015 and December 2014 in Large Supermarkets for untaxed beverages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_nt = (data_c.loc[(data_c[\"store_type\"] == \"Large supermarket\") & (data_c[\"taxed\"] == \"not taxed\"), :]\n",
    "             .pivot([\"store_id\", \"product_id\"], columns=\"time\", values=\"price_per_oz\"))\n",
    "Δ_nt_alt = alt_nt[\"JUN2015\"] - alt_nt[\"DEC2014\"]\n",
    "print(f\"Mean difference is {100*Δ_nt_alt.mean():.2f} cents/oz\")\n",
    "pg.ttest(Δ_nt_alt, y=0, paired=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to recognize the mean difference and the p-value in the excerpt of Table S3 provided in Figure 3.4.\n",
    "\n",
    "Let’s also replicate the last section of Table S3, which shows the difference between the price changes in taxed and untaxed products, that is, we want to know whether `Δ_t_alt` *and* `Δ_nt_alt` have different means. We will apply the two sample hypothesis tests, but this time for unpaired data, as the products differ across samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.ttest(Δ_t_alt, Δ_nt_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again you should be able to identify the corresponding entries in Table S3 shown in Figure 3.4. The main entry in the table is 1.02, indicating that the means of the two groups differ by 1.02 cents. This is confirmed in our calculations, as $0.01312 − $0.00288 is about $0.0102 or 1.02 cents. The p-value of 0.002 is also the same as the one in Table S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 3.7\n",
    "\n",
    "**Importing data from a Stata file and plotting a line chart**\n",
    "\n",
    "## Import data and create a table of average monthly prices\n",
    "\n",
    "To import data from a Stata file (`.dta` format) we need the **pandas** package's `pd.read_stata` function. Remember to download the data from [this link](https://www.globalfoodresearchprogram.org/wp-content/uploads/2021/10/Berkeley_data.zip) and save the file with extension `.dta` in the `data/` folder of your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_stata_data = Path(\"data/public_use_weighted_prices.dta\")\n",
    "posd = pd.read_stata(path_to_stata_data)\n",
    "posd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that for each month and location (Berkeley or Non-Berkeley), there are prices for a variety of beverage categories, and we know whether the category is taxed or not. For any particular time-location-tax status combination we want the average price of all products.\n",
    "\n",
    "To make the summary table, we use method chaining again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_test = (posd\n",
    "                .groupby([\"year\", \"month\", \"location\", \"tax\"])[\"price\"]\n",
    "                .agg(\"mean\")\n",
    "                .reset_index())\n",
    "table_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used a quick trick to pivot the data so that location appears in two separate columns according to its two separate values. The trick is called `unstack` and it takes values from the index and pastes them across columns. `unstack(2)` takes the position 2 level of the index and turns it into columns (in this case, that's location).\n",
    "\n",
    "We'll now prepare the data for plotting in a line chart.\n",
    "\n",
    "First, we reset the index so that it is just composed of row numbers. We'd also like to add a column that tracks time as a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_test[\"date\"] = table_test.apply(lambda row: pd.to_datetime(str(int(row[\"year\"])) + \"-\" + str(int(row[\"month\"])), format=\"%Y-%m\"), axis=1) + pd.offsets.MonthEnd()\n",
    "table_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use an integer version of the year and month together in a string and pass these to `pd.to_datetime` along with the recipe for the format, `%Y-%m` means YYYY-MM format. This produces a date at the start of each month, `pd.offsets.MonthEnd()` finds the last day in the month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a line chart\n",
    "\n",
    "Now we can create a line chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "# zorder sets which layer appears on top\n",
    "sns.lineplot(data=table_test, x=\"date\", y=\"price\", hue=\"tax\", style=\"location\", ax=ax, zorder=2)\n",
    "# Add vertical lines\n",
    "ax.axvline(\"2015-01-01\", color=\"k\", alpha=0.3, zorder=1, linestyle=\"--\")\n",
    "ax.axvline(\"2015-03-01\", color=\"k\", alpha=0.3, zorder=1, linestyle=\"--\")\n",
    "ax.legend(ncol=2, frameon=False)\n",
    "ax.set_ylabel(\"Average price\")\n",
    "# \\n puts text on a new line\n",
    "ax.set_title(\"Average price of taxed and non-taxed beverages \\nin Berkeley and non-Berkeley areas\")\n",
    "ax.annotate(\"Pre-tax\", xy=(\"2014-09-01\", 4.5))\n",
    "ax.annotate(\"Post-tax\", xy=(\"2015-06-01\", 4.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3.6** Average price of taxed and non-taxed beverages in Berkeley and non-Berkeley areas."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13950ff975ea58bb356510d0d8f98cdd1bd1f12bd4ccce66a17a21f4f1a23379"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('core-python-wDU3726x-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
