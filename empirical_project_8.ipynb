{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Project 8\n",
    "\n",
    "---\n",
    "**Download the code**\n",
    "\n",
    "To download the code used in this project as a notebook that can be run in Visual Studio Code, Google Colab, or Jupyter Notebook, right click [here]() and select 'Save Link As', then save it as a `.ipynb` file.\n",
    "\n",
    "Don’t forget to also download the data into your working directory by following the steps in this project.\n",
    "\n",
    "---\n",
    "\n",
    "## Getting started in Python\n",
    "\n",
    "For this project, you will need the following packages:\n",
    "\n",
    "- **pandas** for data analysis\n",
    "- **matplotlib** for data visualisation\n",
    "- **numpy** for numerical methods\n",
    "- **statsmodels** for an extra statistics function\n",
    "\n",
    "You'll also be using the **warnings** and **pathlib** packages, but these come built-in with Python.\n",
    "\n",
    "Remember, you can install packages in Visual Studio Code's integrated terminal (click \"View > Terminal\") by running `conda install packagename` (if using the Anaconda distribution of Python) or `pip install packagename` if not.\n",
    "\n",
    "Once you have the Python packages installed, you will need to import them into your Python session—and configure any other initial settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Set the plot style for prettier charts:\n",
    "plt.style.use(\n",
    "    \"https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt\"\n",
    ")\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 3]\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "# Ignore warnings to make nice output\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.1\n",
    "\n",
    "**Importing data into Python**\n",
    "\n",
    "As we are importing an Excel file, we use the `pd.read_excel` function provided by the **pandas** package. The file is called Project-8-datafile.xlsx and is saved into a subfolder of our working directory called 'data'. The file contains four worksheets that contain data, and these are named ‘Wave 1’ through to ‘Wave 4’. We will load the worksheets one by one and add them to the previous worksheets using the `pd.concat` function, which concatenates (combines) dataframes. \n",
    "\n",
    "The final output is called `lifesat_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sheetnames = [\"Wave \" + str(i) for i in range(1, 5)]\n",
    "list_of_dataframes = [\n",
    "    pd.read_excel(Path(\"data/Project-8-datafile.xlsx\"), sheet_name=x)\n",
    "    for x in list_of_sheetnames\n",
    "]\n",
    "lifesat_data = pd.concat(list_of_dataframes, axis=0)\n",
    "lifesat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that reading in data from lots of Excel worksheets is quite slow. For large datasets, parquet is a very efficient format and works across programming languages.\n",
    "\n",
    "The variable names provided in the spreadsheet are not very specific (a combination of letters and numbers that don’t tell us what the variable measures). To make it easier to keep track we could two approaches:\n",
    "\n",
    "1. use a multi-index for our columns; this is an index with more than one entry per column, with multiple column names stacked on top of each other. We would create a multi-index that includes the original codes, then has labels, and then has a short description.\n",
    "\n",
    "2. Work with what we have, but keep hold of an easy way to convert the codes into either labels or a short description should we need to.\n",
    "\n",
    "\n",
    "Using a multi-index for columns (option 1) is convenient in some ways but it also has some downsides. The main downside is extra complexity when doing operations on columns because we'll need to specify *all* of the different names of a column in some ways. This is so that there's no ambiguity in the case where the some of the column names are repeated at one level off the multi-index. You can see why this might be needed—you could have a case where some column names are repeated on some levels of the multi-index. So, although using the syntax\n",
    "\n",
    "```python\n",
    "lifesat_data[\"A009\"]\n",
    "```\n",
    "\n",
    "will work most of the time, for *some* operations we would have to use\n",
    "\n",
    "```python\n",
    "lifesat_data[(\"A009\", \"Health\", \"State of health (subjective)\")]\n",
    "```\n",
    "\n",
    "instead to access the health column. As you can see, we are removing any ambiguity about which data we refer to by specifying all three of its possible names in an object enclosed by curvy brackets (this object is called a `tuple` and behaves a lot like a list, except you can't modify individual values within it).\n",
    "\n",
    "Option 2 has the downside that on typical dataframe operations, we will only have the codes to go on and will have to look those codes up if we need to remind ourselves of what they represent. The easiest way to do this is using dictionaries.\n",
    "\n",
    "In this tutorial, we'll go for option 2 as it's simpler, and there's a lot to be said for making life simpler. However, if you do want to go down the option 1 route, you can and the first step would be to create the multi-index column object like so:\n",
    "\n",
    "```python\n",
    "# option 1 only\n",
    "index = pd.MultiIndex.from_tuples(\n",
    "    tuple(zip(lifesat_data.columns, labels, short_description)),\n",
    "    names=[\"code\", \"label\", \"description\"],\n",
    ")\n",
    "lifesat_data.columns = index\n",
    "```\n",
    "\n",
    "where `labels` and `short_descriptions` are lists of strings and the zip function turns the three lists of details (codenames, labels, and short descriptions) into a tuple.\n",
    "\n",
    "Going back to option 2: let's first create our neat mapping of codes into labels and codes into short descriptions using dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"EVS-wave\",\n",
    "    \"Country/region\",\n",
    "    \"Respondent number\",\n",
    "    \"Health\",\n",
    "    \"Life satisfaction\",\n",
    "    \"Work Q1\",\n",
    "    \"Work Q2\",\n",
    "    \"Work Q3\",\n",
    "    \"Work Q4\",\n",
    "    \"Work Q5\",\n",
    "    \"Sex\",\n",
    "    \"Age\",\n",
    "    \"Marital status\",\n",
    "    \"Number of children\",\n",
    "    \"Education\",\n",
    "    \"Employment\",\n",
    "    \"Monthly household income\",\n",
    "]\n",
    "\n",
    "short_descriptions = [\n",
    "    \"EVS-wave\",\n",
    "    \"Country/region\",\n",
    "    \"Original respondent number\",\n",
    "    \"State of health (subjective)\",\n",
    "    \"Satisfaction with your life\",\n",
    "    \"To develop talents you need to have a job\",\n",
    "    \"Humiliating to receive money w/o working for it\",\n",
    "    \"People who don't work become lazy\",\n",
    "    \"Work is a duty towards society\",\n",
    "    \"Work comes first even if it means less spare time\",\n",
    "    \"Sex\",\n",
    "    \"Age\",\n",
    "    \"Marital status\",\n",
    "    \"How many living children do you have\",\n",
    "    \"Educational level (ISCED-code one digit)\",\n",
    "    \"Employment status\",\n",
    "    \"Monthly household income (x 1,000s PPP euros)\",\n",
    "]\n",
    "\n",
    "labels_dict = dict(zip(lifesat_data.columns, labels))\n",
    "descrp_dict = dict(zip(lifesat_data.columns, short_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just check these work looking at the example of health again, which has code `\"A009\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_dict[\"A009\"])\n",
    "print(descrp_dict[\"A009\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this project we will refer to the variables using their original names, the codes, but you can see the extra info when you need to by passing those codes into these dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.2\n",
    "\n",
    "**Cleaning data and splitting variables**\n",
    "\n",
    "*Inspect the data and recode missing values*\n",
    "\n",
    "Python's **pandas** package stores variables as different types depending on the kind of information the variable represents. For categorical data, where, as the name suggests, data is divided into a number of groups, such as country or occupation, the variables can be stored as the `\"category\"`. Numerical data (numbers that do not represent categories) can be stored as integers, `\"int\"`, or real numbers, usually `\"double\"`. There are other datatypes too, for example `\"datetime64[ns]\"` for datetimes in nano-second increments. Text is of type `\"string\"`. There's also a 'not quite sure' datatype, `\"object\"`, which is typically used for data that doesn't clearly fall into a bucket.\n",
    "\n",
    "However, **pandas** is quite conservative about deciding on data types for you, so you do have to be careful to check the datatypes are what you want when they are read in. The classic example is of numbers being read in as type `\"object\"`.\n",
    "\n",
    "The `.info()` method tells us what data types are being used in a **pandas** dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of `\"object\"` columns, so it's clear that a lot of the columns haven't been read in as what they should be.\n",
    "\n",
    "Looking back at our data, we can see that there are a LOT of `\".a\"` values and, reading the original data source, it looks like these represent missing values. Let's replace those with the proper missing value indicator, `pd.NA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data = lifesat_data.replace(\".a\", pd.NA)\n",
    "lifesat_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't the only way to deal with those pesky `\".a\"` values. When we read each file in, we could have replaced the value for missing data used in the file, `\".a\"`, with **pandas** built-in representation of missing numbers. This is achieved via the `na_values=\".a\"` keyword in the `pd.read_excel` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recode the life satisfaction variable*\n",
    "\n",
    "To recode the life satisfaction variable (`\"A170\"`), we can use a dictionary to map ‘Dissatisfied’ or ‘Satisfied’ into 1 or 10 respectively. This variable was imported as an object column. After changing the text into numerical values, we use the `astype(\"Int32\")` method to convert the variable into a 32-bit integer (these can represent any integer between -$2^{31}$ and $2^{31}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_satisfaction = \"A170\"\n",
    "lifesat_data[col_satisfaction] = (\n",
    "    lifesat_data[col_satisfaction]\n",
    "    .replace({\"Satisfied\": 10, \"Dissatisfied\": 1})\n",
    "    .astype(\"Int32\")\n",
    ")\n",
    "lifesat_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recode the variable for number of children*\n",
    "\n",
    "We repeat this process for the variable indicating the number of children (`\"X011_01\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_num_children = \"X011_01\"\n",
    "\n",
    "lifesat_data[col_num_children] = (\n",
    "    lifesat_data[col_num_children].replace({\"No children\": 0}).astype(\"Int32\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace text with numbers for multiple variables*\n",
    "\n",
    "When we have to recode multiple variables with the same mapping of text to numerical value, we can take a bit of a short-cut to recode multiple columns at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_codes = [\"C036\", \"C037\", \"C038\", \"C039\", \"C041\"]\n",
    "\n",
    "lifesat_data[col_codes] = (\n",
    "    lifesat_data[col_codes]\n",
    "    .replace(\n",
    "        {\n",
    "            \"Strongly disagree\": 1,\n",
    "            \"Disagree\": 2,\n",
    "            \"Neither agree nor disagree\": 3,\n",
    "            \"Agree\": 4,\n",
    "            \"Strongly agree\": 5,\n",
    "        }\n",
    "    )\n",
    "    .astype(\"Int32\")\n",
    ")\n",
    "\n",
    "# This one needs a different mapping\n",
    "\n",
    "health_code = \"A009\"\n",
    "lifesat_data[health_code] = (\n",
    "    lifesat_data[health_code]\n",
    "    .replace({\"Very poor\": 1, \"Poor\": 2, \"Fair\": 3, \"Good\": 4, \"Very good\": 5})\n",
    "    .astype(\"Int32\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split a variable containing numbers and text*\n",
    "\n",
    "To split the education variable `\"X025A\"` into two new columns, we use the `.explode` method, which creates two new variables called Education_1 and Education_2 containing the numeric value and the text description respectively. Then we use the mutate_at function to convert Education_1 into a numeric variable.\n",
    "\n",
    "Because we're still using a multi-layered column system, we'll need to specify precise which combination of column names we're using in a tuple (as we just did above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_code = \"X025A\"\n",
    "lifesat_data[education_code].str.split(\" : \", expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this again but save it back into our dataframe under two new column names. We'll pass these back in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_num, ed_sch = [education_code + suffix for suffix in [\"_num\", \"_sch\"]]\n",
    "\n",
    "print(ed_num)\n",
    "print(ed_sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass them back in as a list (note the extra square brackets) so that they map up to the two new columns on the right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data[[ed_num, ed_sch]] = lifesat_data[education_code].str.split(\n",
    "    \" : \", expand=True\n",
    ")\n",
    "lifesat_data[ed_num] = pd.to_numeric(lifesat_data[ed_num]).astype(\"Int32\")\n",
    "lifesat_data.sample(5, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the two extra columns for education at the end of the dataframe.\n",
    "\n",
    "There's just one more column to convert: monthly income, which is a real number rather than an integer. Let's do that, and then let's have a final look at our object types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data[\"X047D\"] = pd.to_numeric(lifesat_data[\"X047D\"])\n",
    "lifesat_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.3\n",
    "\n",
    "**Dropping specific observations**\n",
    "\n",
    "As not all questions were asked in all waves, we have to be careful when dropping observations with missing values for certain questions, to avoid accidentally dropping an entire wave of data. For example, information on self-reported health (`\"A009\"`) was not recorded in Wave 3, and questions on work attitudes (`\"C036\"` to `\"C041\"`) and information on household income are only asked in Waves 3 and 4. Furthermore, information on the number of children (`\"X011_01\"`) and education (`\"X025A\"`) are only collected in the final wave.\n",
    "\n",
    "We will first use the `.dropna()` function to find only those observations with complete information on variables present in all waves (`\"X003\"`, `\"A170\"`, `\"X028\"`, `\"X007\"`, and `\"X001\"`, which we will store in a list named `include`). Combining with `.index` will enable us to find the index values for rows that have complete information. But we must also be wary that our index is currently not unique, so we'll do a reset of the index first to ensure that there is one and only one index value for each observation (this is generally good practice!) using `.reset_index()` with the keyword argument `drop=True` because we don't wish to keep the current index in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include = [\"X003\", \"A170\", \"X028\", \"X007\", \"X001\"]\n",
    "lifesat_data = lifesat_data.reset_index(drop=True)\n",
    "lifesat_data = lifesat_data.loc[lifesat_data[include].dropna().index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at variables that were only present in some waves. For each variable/group of variables, we have to only look at the particular wave(s) in which the question was asked, then keep the observations with complete information on those variables. As before, we make lists of variables that only feature in Waves 1, 2, and 4 (`\"A009\"`—stored in `include_wave_1_2_4`), Waves 3 and 4 (`\"C036\"` to `\"C041\"`, `\"X047D\"`—stored in `include_wave_3_4`), and Wave 4 only (`\"X011_01\"` and `\"X025A\"`—stored in `include_wave_4`). Again we will use the `.dropna()` method, but combine it with the logical OR operator, `|`, to include all observations for waves that did not ask that question, along with the complete cases for that question in the other waves.\n",
    "\n",
    "First, we put together some useful background info on what questions were only included in which waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A009 is not in Wave 3.\n",
    "# Note that even though it's just one entry, we use square brackets to make it a list\n",
    "include_in_wave_1_2_4 = [\"A009\"]\n",
    "# Work attitudes and income are in Waves 3 and 4.\n",
    "include_in_wave_3_4 = [\"C036\", \"C037\", \"C038\", \"C039\", \"C041\", \"X047D\"]\n",
    "# Number of children and education are in Wave 4.\n",
    "include_in_wave_4 = [\"X011_01\", \"X025A\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the cases for these waves, successively refining the data to just those with ...\n",
    "\n",
    "For example, in the first refinement of the data below we will first pick out any row for which the column `\"A009\"` has an entry *or* (represented by `|`) `\"S002EVS\"` takes the values `\"1981-1984\"` or `\"1990-1993\"`. This keeps observations if they are in Wave 1 (1981-1984) or Wave 2 (1990-1993), or they are in Waves 3 or 4 with complete information. Because this will create two columns worth of boolean values, we will then use the row-wise (`axis=1`) `.any()` method to create a single boolean value, `True` or `False`, for every row. Then we will use those boolean values to filter down the `lifesat_data` to only the rows that satisfy the given condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Wave 1 (1981-1984) or Wave 2 (1990-1993), or they are in Waves 3 or 4 with complete information\n",
    "condition_wave_3_4 = (lifesat_data[include_in_wave_3_4].notna()).all(axis=1) | (\n",
    "    lifesat_data[\"S002EVS\"].isin([\"1981-1984\", \"1990-1993\"])\n",
    ")\n",
    "lifesat_data = lifesat_data.loc[condition_wave_3_4, :]\n",
    "\n",
    "# in Wave 4 with complete information on the questions specific to that wave or not in Wave 4\n",
    "condition_wave_4 = (\n",
    "    lifesat_data[include_in_wave_4].notna().all(axis=1)\n",
    ") | ~lifesat_data[\"S002EVS\"].isin([\"2008-2010\"])\n",
    "lifesat_data = lifesat_data.loc[condition_wave_4, :]\n",
    "\n",
    "# in Waves 1, 2, or 4 with complete information on the questions specific to those waves, or in Wave 3\n",
    "condition_wave_1_2_4 = (lifesat_data[include_in_wave_1_2_4].notna().all(axis=1)) | (\n",
    "    lifesat_data[\"S002EVS\"].isin([\"1999-2001\"])\n",
    ")\n",
    "lifesat_data = lifesat_data.loc[condition_wave_1_2_4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.4\n",
    "\n",
    "**Calculating averages and percentiles**\n",
    "\n",
    "*Calculate average work ethic score*\n",
    "\n",
    "We use the `.mean(axis=1)` (remember it's `axis=0` to aggregate over index and `axis=1` to aggregrate over columns) method to calculate the average work ethic score for each observation (`\"workethic\"`) based on the five survey questions related to working attitudes (`\"C036\"` to `\"C041\"`). As we're still using a multi-level column naming convention, we need to specify three levels of column names to create a new column—but they can all be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data[\"work_ethic\"] = lifesat_data.loc[\n",
    "    :, [\"C036\", \"C037\", \"C038\", \"C039\", \"C041\"]\n",
    "].mean(axis=1)\n",
    "lifesat_data.sample(5, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating averages and percentiles**\n",
    "\n",
    "Regression package **statsmodels** provides a handy method (`\"ECDF\"`) to obtain an individual’s relative income as a percentile. As we want a separate income distribution for each wave, we use the `.groupby` method. Finally, we tidy up the results by converting each value into a percentage and rounding to a single decimal place.\n",
    "\n",
    "The ecdf function will not work if there is missing data. Since we don’t have income data for Waves 1 and 2, we will have to split the data when working out the percentiles. First we store all observations from Waves 3 and 4 separately in a temporary dataset (df.new), only keep observations that have income values (!is.na(X047D)), calculate the percentile values with the ecdf function, and save the values in a variable called percentile (which will be ‘NA’ for Waves 1 and 2). Then we recombine this data with the original observations from Waves 1 and 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# create empty col for new variable\n",
    "lifesat_data[\"inc_percentile\"] = pd.NA\n",
    "\n",
    "# fill it for waves 3 and 4 with relevant data\n",
    "condition_inc_percentile = (\n",
    "    lifesat_data[\"S002EVS\"].isin([\"1999-2001\", \"2008-2010\"])\n",
    ") & (lifesat_data[\"X047D\"].notna())\n",
    "\n",
    "lifesat_data.loc[condition_inc_percentile, \"inc_percentile\"] = (\n",
    "    lifesat_data.loc[\n",
    "        condition_inc_percentile, :\n",
    "    ]  # Select waves 3 & 4 without missing income data\n",
    "    .groupby(\"S002EVS\")[\"X047D\"]  # groupby wave  # select income variable\n",
    "    .transform(\n",
    "        lambda x: np.round(ECDF(x)(x) * 100, 1)\n",
    "    )  # compute ecdf as % round to 1 decimal place\n",
    ")\n",
    "\n",
    "# see the dataframe with the new column\n",
    "lifesat_data.sample(5, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.5\n",
    "\n",
    "**Calculating summary statistics**\n",
    "\n",
    "*Create a table showing employment status, by country*\n",
    "\n",
    "One of the most useful features of **pandas** is its composability. We can stack up multiple methods to create just the statistics we want. In this example, we're going to use a succession of methods to create a table showing the employment status (as a percentage) of each country's labour force. The steps are:\n",
    "\n",
    "- Select the data for Wave 4\n",
    "- Group it by employment type (`\"X028\"`) and country (`\"S003\"`). Order will matter later when we use `unstack`; whichever variable is last in the groupby command will be switched from the index to the columns when we call unstack.\n",
    "- Select the column to take observations from. In this case, it makes sense to use employment again.\n",
    "- Count the number of observations\n",
    "- Unstack so that we have a table instead of a long list (with countries as columns)\n",
    "- Transform the numbers into percentages that sum to 100 for each country\n",
    "- Round the values in the table\n",
    "- Because we have more countries than employment statuses, transpose the columns and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_table = (\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X028\", \"S003\"])[  # Group by employment and country\n",
    "        \"X028\"\n",
    "    ]  # Select employment column\n",
    "    .count()  # Count number of observations in each category\n",
    "    .unstack()  # Turn countries from an index into columns (countries as last groupby variable)\n",
    "    .transform(lambda x: x * 100 / x.sum())  # Compute a percentage\n",
    "    .round(2)  # Round to 2 decimal places\n",
    "    .T  # Tranpose so countries are the index, employment types the columns\n",
    ")\n",
    "\n",
    "sum_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then wanted to export this table for further use elsewhere, we would export it with `sum_table.to_html(filename)`, `sum_table.to_excel(filename)`, `sum_table.to_string(filename)`, `sum_table.to_latex(filename)`, or many other options that you can find [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculate summary statistics by gender*\n",
    "\n",
    "We can also obtain summary statistics on a number of variables at the same time using the `apply` function. To obtain the mean value for each of the required variables, grouped by the gender variable (`\"X001\"`), we can compose methods again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X001\"])[  # Group by gender\n",
    "        \"A009\", \"A170\", \"work_ethic\", \"X003\", \"X025A_num\", \"X011_01\"\n",
    "    ]  # Select columns\n",
    "    .mean()\n",
    "    .round(2)  # Round to 2 decimal places\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the standard deviation is as simple as replacing `mean()` with `std()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X001\"])[  # Group by gender\n",
    "        \"A009\", \"A170\", \"work_ethic\", \"X003\", \"X025A_num\", \"X011_01\"\n",
    "    ]  # Select columns\n",
    "    .std()\n",
    "    .round(2)  # Round to 2 decimal places\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want *both*!? We can have that too, using the `agg` (short for aggregate) method and a list of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X001\"])[  # Group by gender\n",
    "        \"A009\", \"A170\", \"work_ethic\", \"X003\", \"X025A_num\", \"X011_01\"\n",
    "    ]  # Select columns\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .round(2)  # Round to 2 decimal places\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in this case, we have a multi-level column object in our dataframe. If you want to flip down the last level of columns to be an index, try using `.stack()`.\n",
    "\n",
    "If you're exporting your results, you're not going to want to use the code names though. So you'll probably want to export your table with the nice names substituted in. You can do this using the dictionaries we created right at the start. Let's use the `label_dict` with a stacked version of the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = (\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X001\"])[  # Group by gender\n",
    "        \"A009\", \"A170\", \"work_ethic\", \"X003\", \"X025A_num\", \"X011_01\"\n",
    "    ]  # Select columns\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .round(2)  # Round to 2 decimal places\n",
    "    .stack()  # bring mean and std into the index\n",
    "    .rename(labels_dict, axis=1)  # rename the columns\n",
    ")\n",
    "\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't rename `\"work_ethic\"` and `\"X025A_num\"`, because we created those variables *after* the labels dictionary was created (so there are no labels for them). And `\"X001\"` is not a column heading, it's the index name, so we'll have to change that separately.\n",
    "\n",
    "Let's update our dictionary with some new names and a new index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict.update({\"work_ethic\": \"Work Ethic\", \"X025A_num\": \"Education Level\"})\n",
    "tab = tab.rename(labels_dict, axis=1)\n",
    "tab.index.names = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "]  # Set index names empty (two levels because two column levels)\n",
    "\n",
    "tab"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13950ff975ea58bb356510d0d8f98cdd1bd1f12bd4ccce66a17a21f4f1a23379"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('core-python-wDU3726x-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
