{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Project 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started in Python\n",
    "\n",
    "Head to the \"Getting Started in Python\" page for help and advice on setting up a Python session to work with. Remember, you can run any page from this book as a *notebook* by downloading the relevant file from this [repository](https://github.com/aeturrell/core_python) and running it on your own computer. Alternatively, you can run pages online in your browser over at [Binder](https://mybinder.org/v2/gh/aeturrell/core_python/HEAD)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Settings\n",
    "\n",
    "Let's import the packages we'll need and also configure the settings we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pingouin as pg\n",
    "from lets_plot import *\n",
    "\n",
    "LetsPlot.setup_html(no_js=True)\n",
    "\n",
    "### You don't need to use these settings yourself\n",
    "### — they are just here to make the book look nicer!\n",
    "# Set the plot style for prettier charts:\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6.1: Looking for Patterns in Survey Data\n",
    "\n",
    "### Learning objectives for this part\n",
    "\n",
    "- explain how survey data is collected, and describe measures that can increase the reliability and validity of survey data\n",
    "- use column charts and box and whisker plots to compare distributions\n",
    "- calculate conditional means for one or more conditions, and compare them on a bar chart\n",
    "- use line charts to describe the behaviour of real-world variables over time.\n",
    "\n",
    "### Downloading the Data\n",
    "\n",
    "First download the data used in the paper to understand how this information was collected. The data is publicly available and free of charge, but you will need to create a user account in order to access it.\n",
    "\n",
    "- Go to the [World Management Survey pages](https://worldmanagementsurvey.org/).\n",
    "- Click download survey data, then register, and fill in the form.\n",
    "- An account activation link will be sent to the email you provided. Click on it to activate your account.\n",
    "- Now go to the World Management Survey data download page.\n",
    "- In the subsection ‘Download the public WMS data now’, click the ‘Download Now’ button.\n",
    "- In the ‘Login’ section, enter your account’s email and password, then click ‘Login’.\n",
    "- Under the heading ‘Manufacturing: 2004–2010 combined survey data (AMP)’, click the ‘Download’ button.\n",
    "- Unzip the files in the downloaded zip folder into the data/ folder within your working directory.\n",
    "- You may also find it helpful to download the Bloom et al. paper [‘Management practices across firms and countries’](https://tinyco.re/6438551)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 6.1\n",
    "\n",
    "**Importing data into Python and creating tables and charts**\n",
    "\n",
    "Before opening an Excel or csv file using Python, you can open the file in spreadsheet software (such as Excel) to understand how it's structured. From looking at the file, we learn that:\n",
    "\n",
    "* the variable names are in the first row (no need to use the `skiprows` keyword argument)\n",
    "* missing values are represented by empty cells\n",
    "* the last variable is in Column S, with short variable descriptions in Column U: it is easier to import everything first and remove the unnecessary data afterwards.\n",
    "\n",
    "We will call our imported data `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(\"data/AMP_graph_manufacturing.csv\"))\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the penultimate column with values, Column T, was imported as `\"Unnamed: 19\"` and only contains `NaN`s. The final column of values has been imported into **pandas** comes from Column U in the spreadsheet and contains information about the variables (named `\"storage display value\"`).\n",
    "\n",
    "Let's extract the information about the variables in a new **pandas** series called `man_varinfo` and then remove both of these columns from the dataset. To make it easier to see the `man_varinfo`, we'll temporarily override **pandas** column width limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_varinfo = df.iloc[:, -1].dropna()\n",
    "\n",
    "with pd.option_context(\"display.max_colwidth\", 80):\n",
    "    print(man_varinfo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to drop the last two columns using slicing. The syntax is `.iloc[:, :-i]` where `i` is the number of rows we wish to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, :-2]\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few of the variables that have been imported as numbers are actually categorical variables: `\"mne_f\"` , `\"mne_d\"`, and `\"competition2004\"`. **pandas** doesn't automatically know what datatypes different variables should have. However, we can set the type of these variables as categorical and we can use labels to define what each of the numbers in the variables represents.\n",
    "\n",
    "The first two have quite clear labels. For the third, we'll use some string manipulation tools to grab the labels directly from the `man_varinfo` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_mne_f = [\"No MNE_f\", \"MNE_f\"]\n",
    "lab_mne_d = [\"No MNE_d\", \"MNE_d\"]\n",
    "lab_comp2004 = man_varinfo.iloc[16].split(\"  \")[-1].split(\",\")\n",
    "print(lab_comp2004)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third line of code above is doing a lot of work here. When you do coding you will often use someone else's code as a starting point for your code, and trying to figure out what some code does is therefore a very important skill. Below you can see a dissection of what each part of this line does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_1 = man_varinfo.iloc[16]\n",
    "print(explain_1)\n",
    "print(\"\\n\")  # prints new line\n",
    "explain_2 = man_varinfo.iloc[16].split(\"  \")\n",
    "print(explain_2)\n",
    "print(\"\\n\")  # prints new line\n",
    "explain_3 = man_varinfo.iloc[16].split(\"  \")[-1]\n",
    "print(explain_3)\n",
    "print(\"\\n\")  # prints new line\n",
    "explain_4 = man_varinfo.iloc[16].split(\"  \")[-1].split(\",\")\n",
    "print(explain_4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now encode these variables as categoricals, with suitable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"mne_f\", \"mne_d\", \"competition2004\"]:\n",
    "    df[col] = pd.Categorical(df[col])\n",
    "\n",
    "df[\"mne_f\"] = df[\"mne_f\"].cat.rename_categories(lab_mne_f)\n",
    "df[\"mne_d\"] = df[\"mne_d\"].cat.rename_categories(lab_mne_d)\n",
    "df[\"competition2004\"] = df[\"competition2004\"].cat.rename_categories(lab_comp2004)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you create new labels, check that they have been attached to the correct entries. Although we set them as a list here using, for example, `[\"No MNE_f\", \"MNE_f\"]`, we could have also used a dictionary, for example `{0: \"No MNE_f\", 1: \"MNE_f\"}`, mapping old values into new."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the tables, we can use a technique called method chaining. Rather than have a series of separate assignment commands using `=`, method chaining \"chains\" together a series of methods (these are preceeded by `.`). This approach has pros and cons: it can be easier to read, but harder to debug in case of errors. \n",
    "\n",
    "First, we will group data by country (using `.groupby`), then calculate the required aggregate statistics for each of these groups (using `.agg`), then order the countries according to their overall score (highest to lowest) (`.sort_values`).\n",
    "\n",
    "`.groupby` groups the data according to a given column.\n",
    "\n",
    "`.agg` aggregates data, and returns it with a different index. In combination with `.groupby`, it will return an index based on what column(s) was/were passed to the groupby operation. There are many ways to use `.agg`, including just setting `.agg.mean()` and other functions such as `count()`, `median()`, `sum()`, and `std()`. However, to change the column names of the returned series, you can either pass a dictionary, say `{\"columnname\": \"mean\"}`, or pass an object called a tuple that species the new column name, old column name, and the aggregation function, for example `newcolumnname = (\"columnname\", \"count\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mean = (\n",
    "    df.groupby(\"country\")\n",
    "    .agg(\n",
    "        obs=(\"management\", \"count\"),\n",
    "        m_overall=(\"management\", \"mean\"),\n",
    "        m_monitor=(\"monitor\", \"mean\"),\n",
    "        m_target=(\"target\", \"mean\"),\n",
    "        m_incentives=(\"people\", \"mean\"),\n",
    "    )\n",
    "    .sort_values(by=\"m_overall\", ascending=False)\n",
    ")\n",
    "\n",
    "table_mean.round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make the table showing the ranks. We can use the `.agg` function again, but in this case it keeps the same index as before because we're not using a groupby.\n",
    "\n",
    "We will also drop the `\"obs\"` column, and rename all of the columns so that they begin with `\"r\"` (for rank) rather than `\"m\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rank = table_mean.agg(\"rank\", ascending=False).drop(\"obs\", axis=1)\n",
    "table_rank.columns = [x.replace(\"m_\", \"r_\") for x in table_rank.columns]\n",
    "table_rank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use **lets-plot** to create a bar chart of the `\"m_overall\"` value in `table_mean`. To present countries in order of their management score, we will first re-order the dataframe using `sort_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mean = table_mean.sort_values(by=\"m_overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(table_mean.reset_index(), aes(y=\"country\", x=\"m_overall\"))\n",
    "    + geom_bar(stat=\"identity\", orientation=\"y\")\n",
    "    + labs(\n",
    "        x=\"Average management practice score\",\n",
    "        y=\"Country\",\n",
    "        title=\"Management practices in manufacturing firms\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 6.3** *Management practices in manufacturing firms around the world.*\n",
    "\n",
    "\n",
    "If you want to switch the order of the bars, use `table_mean.sort_values(by=\"m_overall\", ascending=False)` before creating the plot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 6.2\n",
    "\n",
    "**Obtaining frequency counts and plotting overlapping histograms**\n",
    "\n",
    "To get frequency counts, use the `pd.cut` function. This will count the number of observations that fall within the intervals specified in using a list of bins given by **numpy**'s `arange` function; this accepts arguments of the form `arange(start, stop, step)` to create intervals.\n",
    "\n",
    "We store this information in the **pandas** series `chile_intervals`. This gives the appropriate interval for each entry in the series. To return the counts, we need to aggregate this information to the total counts per bin, which we can do with `value_counts`. To keep the order of the intervals, we'll specify `value_counts(sort=False)` too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chile_intervals = pd.cut(\n",
    "    df.loc[df[\"country\"] == \"Chile\", \"management\"], bins=np.arange(0, 5, 0.2)\n",
    ")\n",
    "chile_counts = chile_intervals.value_counts(sort=False)\n",
    "chile_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how to get hold of a histogram in the form of data. If we want to jump straight to plotting a histogram, we have a few options.\n",
    "\n",
    "If we just want a quick look, we can use **pandas** built-in histogram function, `.plot.hist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_hist = \"Chile\"\n",
    "df.loc[df[\"country\"] == country_to_hist, \"management\"].plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course put this on an existing (**matplotlib**) plot if you like, and turn it into a neater figure. Here's an example of how you'd do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val, max_val, step = 0, 5, 0.2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df.loc[df[\"country\"] == country_to_hist, \"management\"].hist(\n",
    "    bins=np.arange(min_val, max_val, step)\n",
    ").plot(ax=ax)\n",
    "ax.set_xlabel(\"Management score\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlim(min_val, max_val)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_title(f\"Histogram of management scores for {country_to_hist}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a figure and chart axes (called `ax`) first, then put the information on them by calling `.plot(ax=ax)`. Then we added contextual information such as a title and axes labels.\n",
    "\n",
    "An alternative way of generating histograms is using **lets-plot**. **matplotlib** is great for when you need lots of flexibility and customisation, but as histograms are such common chart types, they're covered by **lets-plot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ggplot(df.loc[df[\"country\"] == country_to_hist, :], aes(x=\"management\"))\n",
    "    + geom_histogram(color=\"black\", alpha=0.5)\n",
    "    + labs(\n",
    "        x=\"Management Score\",\n",
    "        y=\"Counts\",\n",
    "        title=f\"Histogram of management scores for {country_to_hist}\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **matplotlib**, it is possible to add a second country to the same chart too, but in this case we're going to use **lets-plot**. If you did want to do that, then you could try looking up how to do it on the internet. Everyone, no matter how expert they are at coding, uses the internet to grab snippets that help them achieve what they want. In this case, you could try searching for \"Plot two histograms on the same graph matplotlib\".\n",
    "\n",
    "**lets-plot** will allow us to have two histograms on the same chart too.\n",
    "\n",
    "We're going to demonstrate this by using similar code to above to plot a second country too, the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_hist = \"Chile\"\n",
    "scnd_country_hist = \"United States\"\n",
    "\n",
    "(\n",
    "    ggplot(\n",
    "        df.loc[df[\"country\"].isin([\"Chile\", \"United States\"]), :],\n",
    "        aes(x=\"management\", fill=\"country\"),\n",
    "    )\n",
    "    + geom_histogram(alpha=0.6, color=\"black\", position=\"identity\")\n",
    "    + labs(\n",
    "        x=\"Management score\",\n",
    "        title=f\"Management scores for {country_to_hist} and {scnd_country_hist}\",\n",
    "        caption=\"Source: World Management Survey\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would want to put these two countries on the same chart but *normalised* because we're more interested in the distribution of scores than the absolute numbers of surveys. Unfortunately though, **lets-plot** doesn't yet have a `geom_histogram` option that will normalise the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 6.6** *Comparing the distribution of management scores for the US and Chile.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 6.3\n",
    "\n",
    "**Creating box plots**\n",
    "\n",
    "We will use a somewhat similar code structure as we did for the overlapping histograms, this time plotting countries on the horizontal axis and management scores on the vertical axis. Instead of laboriously specifying a different box and whisker plot for each country independently, though, we will simply ask for them for every country in a list using `df[\"country\"].isin()` to filter the dataframe with all the data in and then `boxplot(..., by=\"country\", ...)` to make the chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_include = [\"Chile\", \"United States\", \"Brazil\", \"Germany\", \"UK\"]\n",
    "\n",
    "(\n",
    "    ggplot(\n",
    "        df.loc[df[\"country\"].isin(countries_to_include), :],\n",
    "        aes(x=\"country\", y=\"management\"),\n",
    "    )\n",
    "    + geom_boxplot()\n",
    "    + labs(\n",
    "        y=\"Management score\", x=\"Country\", title=\"Management scores grouped by country\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 6.4\n",
    "\n",
    "**Calculating confidence intervals and adding them to a chart**\n",
    "\n",
    "As in Python walkthrough 6.1, we use method chaining to do this. First, we take management data dataframe, `df`, and extract the countries we need using `df.loc`. Then, we group the data by country (using a `group_by`), and calculate some of the required aggregate measures (`agg`) mapping the management into three new variables; the mean, the standard deviation, and the number of observations (using the `len` function).\n",
    "\n",
    "The final step is to compute an error from the new columns and, finally, to sort the data according to the values of `\"mean_m\"`. We save the final result in `table_stats`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_include = [\"Chile\", \"United States\", \"Brazil\", \"Germany\", \"UK\"]\n",
    "\n",
    "table_stats = (\n",
    "    df.loc[df[\"country\"].isin(countries_to_include), :]\n",
    "    .groupby(\"country\")\n",
    "    .agg(\n",
    "        mean_m=(\"management\", \"mean\"),\n",
    "        sd_m=(\"management\", \"std\"),\n",
    "        obs=(\"management\", len),\n",
    "    )\n",
    "    .assign(m_err=lambda x: 1.96 * np.sqrt(x[\"sd_m\"] ** 2 / (x[\"obs\"] - 1)))\n",
    "    .sort_values(\"mean_m\", ascending=False)\n",
    ")\n",
    "\n",
    "table_stats.round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this as the basis of a bar chart in **matplotlib** to quickly see mean management scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "table_stats.plot.bar(ax=ax, y=\"mean_m\", yerr=\"m_err\", rot=0, capsize=5)\n",
    "ax.set_ylim(2, 4)\n",
    "ax.legend([])  # Turns legend off\n",
    "ax.set_title(\"Mean management score across selected countries\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 6.10** *Bar chart of mean management score in manufacturing firms for a selection of countries, with 95% confidence intervals.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 6.5\n",
    "\n",
    "**Calculating and adding conditional summary statistics and confidence intervals to a chart**\n",
    "\n",
    "To do this, we will use many techniques encountered previously, but first we have to create a new variable that indicates whether a firm is larger or smaller than a certain threshold; we'll call this `\"size\"`. A firm with a value of `\"lemp_firm\"` greater than 5.8 is considered larger. We use a new categorical column (based on a boolean check on the condition) to make this distinction, and rename the falses to be `\"smaller\"` and the trues to be `\"larger\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"size\"] = pd.Categorical(df[\"lemp_firm\"] > 5.8).rename_categories(\n",
    "    {False: \"smaller\", True: \"larger\"}\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at Canada, Brazil, and the United States. Again, we use method chaining to make the table. In the `groupby` command, we group the variables by size and ownership, as we did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_include = [\"Canada\", \"United States\", \"Brazil\"]\n",
    "\n",
    "table_stats2 = (\n",
    "    df.loc[df[\"country\"].isin(countries_to_include), :]\n",
    "    .groupby([\"country\", \"ownership\", \"size\"])\n",
    "    .agg(\n",
    "        mean_m=(\"management\", \"mean\"),\n",
    "        sd_m=(\"management\", \"std\"),\n",
    "        obs=(\"management\", len),\n",
    "    )\n",
    ")\n",
    "\n",
    "table_stats2.round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the variable `\"size\"` as a column variable, so that we can see the summary statistics in two blocks of columns (separately for larger and smaller firms). Because `\"size\"` is the innermost index column of our three index columns (they are the three columns we passed to the groupby variable), we can use the `unstack` method to split the main table by the unique values in the `\"size\"` index (we also round the values to 2 decimal places using `.round(2)`). The 'smaller' and 'larger' categories now appear under the three headings `mean_m`, `sd_m`, and `obs` separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_stats2.unstack().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter used the following package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "26c4b888616894cf8360ee3d370b6a41732ef00c8f1d61e869c42a8428cf1ac1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
