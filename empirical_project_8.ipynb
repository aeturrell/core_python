{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Project 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Settings\n",
    "\n",
    "Let's import the packages we'll need and also configure the settings we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pingouin as pg\n",
    "\n",
    "\n",
    "### You don't need to use these settings yourself\n",
    "### — they are just here to make the book look nicer!\n",
    "# Set the plot style for prettier charts:\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.1\n",
    "\n",
    "**Importing data into Python**\n",
    "\n",
    "As we are importing an Excel file, we use the `pd.read_excel` function provided by the **pandas** package. The file is called doing-economics-datafile-working-in-excel-project-8.xlsx and needs to be saved into a subfolder of your working directory called 'data'. The file contains four worksheets that contain data, and these are named ‘Wave 1’ through to ‘Wave 4’. We will load the worksheets one by one and add them to the previous worksheets using the `pd.concat` function, which concatenates (combines) dataframes. \n",
    "\n",
    "The final, combined data frame is called `lifesat_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sheetnames = [\"Wave \" + str(i) for i in range(1, 5)]\n",
    "list_of_dataframes = [\n",
    "    pd.read_excel(\n",
    "        Path(\"data/doing-economics-datafile-working-in-excel-project-8.xlsx\"),\n",
    "        sheet_name=x,\n",
    "    )\n",
    "    for x in list_of_sheetnames\n",
    "]\n",
    "lifesat_data = pd.concat(list_of_dataframes, axis=0)\n",
    "lifesat_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that reading in data from lots of Excel worksheets is quite slow. For large datasets, [parquet is a very efficient format](https://ursalabs.org/blog/2019-10-columnar-perf/) and works across programming languages.\n",
    "\n",
    "The variable names provided in the spreadsheet are not very specific (a combination of letters and numbers that don’t tell us what the variable measures). To make it easier to keep track we could two approaches:\n",
    "\n",
    "1. use a multi-index for our columns; this is an index with more than one entry per column, with multiple column names stacked on top of each other. We would create a multi-index that includes the original codes, then has labels, and then has a short description.\n",
    "\n",
    "2. Work with what we have, but keep hold of an easy way to convert the codes into either labels or a short description should we need to.\n",
    "\n",
    "\n",
    "Using a multi-index for columns (option 1) is convenient in some ways but it also has some downsides. The main downside is extra complexity when doing operations on columns because we'll need to specify *all* of the different names of a column in some ways. This is so that there's no ambiguity in the case where the some of the column names are repeated at one level off the multi-index. You can see why this might be needed—you could have a case where some column names are repeated on some levels of the multi-index. So, although using the syntax\n",
    "\n",
    "```python\n",
    "lifesat_data[\"A009\"]\n",
    "```\n",
    "\n",
    "will work most of the time, for *some* operations we would have to use\n",
    "\n",
    "```python\n",
    "lifesat_data[(\"A009\", \"Health\", \"State of health (subjective)\")]\n",
    "```\n",
    "\n",
    "instead to access the health column. As you can see, we are removing any ambiguity about which data we refer to by specifying all three of its possible names in an object enclosed by curvy brackets (this object is called a `tuple` and behaves a lot like a list, except you can't modify individual values within it).\n",
    "\n",
    "Option 2 has the downside that, on typical dataframe operations, we will only have the codes to go on and will have to look those codes up if we need to remind ourselves of what they represent. The easiest way to do this is using dictionaries.\n",
    "\n",
    "In this tutorial, we'll go for option 2 as it's simpler, and there's a lot to be said for making life simpler. However, if you do want to go down the option 1 route, you can and the first step would be to create the multi-index column object like so:\n",
    "\n",
    "```python\n",
    "# option 1 only\n",
    "index = pd.MultiIndex.from_tuples(\n",
    "    tuple(zip(lifesat_data.columns, labels, short_description)),\n",
    "    names=[\"code\", \"label\", \"description\"],\n",
    ")\n",
    "lifesat_data.columns = index\n",
    "```\n",
    "\n",
    "where `labels` and `short_descriptions` are lists of strings and the zip function turns the three lists of details (codenames, labels, and short descriptions) into a tuple.\n",
    "\n",
    "Going back to option 2: let's first create our neat mapping of codes into labels and codes into short descriptions using dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"EVS-wave\",\n",
    "    \"Country/region\",\n",
    "    \"Respondent number\",\n",
    "    \"Health\",\n",
    "    \"Life satisfaction\",\n",
    "    \"Work Q1\",\n",
    "    \"Work Q2\",\n",
    "    \"Work Q3\",\n",
    "    \"Work Q4\",\n",
    "    \"Work Q5\",\n",
    "    \"Sex\",\n",
    "    \"Age\",\n",
    "    \"Marital status\",\n",
    "    \"Number of children\",\n",
    "    \"Education\",\n",
    "    \"Employment\",\n",
    "    \"Monthly household income\",\n",
    "]\n",
    "\n",
    "short_descriptions = [\n",
    "    \"EVS-wave\",\n",
    "    \"Country/region\",\n",
    "    \"Original respondent number\",\n",
    "    \"State of health (subjective)\",\n",
    "    \"Satisfaction with your life\",\n",
    "    \"To develop talents you need to have a job\",\n",
    "    \"Humiliating to receive money w/o working for it\",\n",
    "    \"People who don't work become lazy\",\n",
    "    \"Work is a duty towards society\",\n",
    "    \"Work comes first even if it means less spare time\",\n",
    "    \"Sex\",\n",
    "    \"Age\",\n",
    "    \"Marital status\",\n",
    "    \"How many living children do you have\",\n",
    "    \"Educational level (ISCED-code one digit)\",\n",
    "    \"Employment status\",\n",
    "    \"Monthly household income (x 1,000s PPP euros)\",\n",
    "]\n",
    "\n",
    "labels_dict = dict(zip(lifesat_data.columns, labels))\n",
    "descrp_dict = dict(zip(lifesat_data.columns, short_descriptions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just check these work looking at the example of health again, which has code `\"A009\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_dict[\"A009\"])\n",
    "print(descrp_dict[\"A009\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this project we will refer to the variables using their original names, the codes, but you can see the extra info when you need to by passing those codes into these dictionaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.2\n",
    "\n",
    "**Cleaning data and splitting variables**\n",
    "\n",
    "*Inspect the data and recode missing values*\n",
    "\n",
    "Python's **pandas** package stores variables as different types depending on the kind of information the variable represents. For categorical data, where, as the name suggests, data is divided into a number of groups, such as country or occupation, the variables can be stored as the `\"category\"`. Numerical data (numbers that do not represent categories) can be stored as integers, `\"int\"`, or real numbers, usually `\"double\"`. There are other datatypes too, for example `\"datetime64[ns]\"` for datetimes in nano-second increments. Text is of type `\"string\"`. There's also a 'not quite sure' datatype, `\"object\"`, which is typically used for data that doesn't clearly fall into a bucket.\n",
    "\n",
    "However, **pandas** is quite conservative about deciding on data types for you, so you do have to be careful to check the datatypes are what you want when they are read in. The classic example is of numbers being read in as type `\"object\"`.\n",
    "\n",
    "The `.info()` method tells us what data types are being used in a **pandas** dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of `\"object\"` columns, so it's clear that a lot of the columns haven't been read in as what they should be.\n",
    "\n",
    "Looking back at our data, we can see that there are a LOT of `\".a\"` values and, reading the original data source, it looks like these represent missing values. Let's replace those with the proper missing value indicator, `pd.NA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data = lifesat_data.replace(\".a\", pd.NA)\n",
    "lifesat_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't the only way to deal with those pesky `\".a\"` values. When we read each file in, we could have replaced the value for missing data used in the file, `\".a\"`, with **pandas** built-in representation of missing numbers. This is achieved via the `na_values=\".a\"` keyword in the `pd.read_excel` function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recode the life satisfaction variable*\n",
    "\n",
    "To recode the life satisfaction variable (`\"A170\"`), we can use a dictionary to map ‘Dissatisfied’ or ‘Satisfied’ into 1 or 10 respectively. This variable was imported as an object column. After changing the text into numerical values, we use the `astype(\"Int32\")` method to convert the variable into a 32-bit integer (these can represent any integer between -$2^{31}$ and $2^{31}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_satisfaction = \"A170\"\n",
    "lifesat_data[col_satisfaction] = (\n",
    "    lifesat_data[col_satisfaction]\n",
    "    .replace({\"Satisfied\": 10, \"Dissatisfied\": 1})\n",
    "    .astype(\"Int32\")\n",
    ")\n",
    "lifesat_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recode the variable for number of children*\n",
    "\n",
    "We repeat this process for the variable indicating the number of children (`\"X011_01\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_num_children = \"X011_01\"\n",
    "\n",
    "lifesat_data[col_num_children] = (\n",
    "    lifesat_data[col_num_children].replace({\"No children\": 0}).astype(\"Int32\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Replace text with numbers for multiple variables*\n",
    "\n",
    "When we have to recode multiple variables with the same mapping of text to numerical value, we can take a bit of a short-cut to recode multiple columns at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_codes = [\"C036\", \"C037\", \"C038\", \"C039\", \"C041\"]\n",
    "\n",
    "lifesat_data[col_codes] = (\n",
    "    lifesat_data[col_codes]\n",
    "    .replace(\n",
    "        {\n",
    "            \"Strongly disagree\": 1,\n",
    "            \"Disagree\": 2,\n",
    "            \"Neither agree nor disagree\": 3,\n",
    "            \"Agree\": 4,\n",
    "            \"Strongly agree\": 5,\n",
    "        }\n",
    "    )\n",
    "    .astype(\"Int32\")\n",
    ")\n",
    "\n",
    "# This one needs a different mapping\n",
    "\n",
    "health_code = \"A009\"\n",
    "lifesat_data[health_code] = (\n",
    "    lifesat_data[health_code]\n",
    "    .replace({\"Very poor\": 1, \"Poor\": 2, \"Fair\": 3, \"Good\": 4, \"Very good\": 5})\n",
    "    .astype(\"Int32\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split a variable containing numbers and text*\n",
    "\n",
    "To split the education variable `\"X025A\"` into two new columns, we use the `.explode` method, which creates two new variables called `X025A_num` and `X025A_sch` containing the numeric value and the text description respectively. Then we will convert `X025A_num` into a numeric variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_code = \"X025A\"\n",
    "lifesat_data[education_code].str.split(\" : \", expand=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this again but save it back into our dataframe under two new column names. We'll pass these back in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed_num, ed_sch = [education_code + suffix for suffix in [\"_num\", \"_sch\"]]\n",
    "\n",
    "print(ed_num)\n",
    "print(ed_sch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pass them back in as a list (note the extra square brackets) so that they map up to the two new columns on the right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data[[ed_num, ed_sch]] = lifesat_data[education_code].str.split(\n",
    "    \" : \", expand=True\n",
    ")\n",
    "lifesat_data[ed_num] = pd.to_numeric(lifesat_data[ed_num]).astype(\"Int32\")\n",
    "lifesat_data.sample(5, random_state=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the two extra columns for education at the end of the dataframe.\n",
    "\n",
    "There's just one more column to convert: monthly income, which is a real number rather than an integer. Let's do that, and then let's have a final look at our object types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data[\"X047D\"] = pd.to_numeric(lifesat_data[\"X047D\"])\n",
    "lifesat_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.3\n",
    "\n",
    "**Dropping specific observations**\n",
    "\n",
    "As not all questions were asked in all waves, we have to be careful when dropping observations with missing values for certain questions, to avoid accidentally dropping an entire wave of data. For example, information on self-reported health (`\"A009\"`) was not recorded in Wave 3, and questions on work attitudes (`\"C036\"` to `\"C041\"`) and information on household income are only asked in Waves 3 and 4. Furthermore, information on the number of children (`\"X011_01\"`) and education (`\"X025A\"`) are only collected in the final wave.\n",
    "\n",
    "We will first use the `.dropna()` function to find only those observations with complete information on variables present in all waves (`\"X003\"`, `\"A170\"`, `\"X028\"`, `\"X007\"`, and `\"X001\"`, which we will store in a list named `include`). Combining with `.index` will enable us to find the index values for rows that have complete information. But we must also be wary that our index is currently not unique, so we'll do a reset of the index first to ensure that there is one and only one index value for each observation (this is generally good practice!) using `.reset_index()` with the keyword argument `drop=True` because we don't wish to keep the current index in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include = [\"X003\", \"A170\", \"X028\", \"X007\", \"X001\"]\n",
    "lifesat_data = lifesat_data.reset_index(drop=True)\n",
    "lifesat_data = lifesat_data.loc[lifesat_data[include].dropna().index, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at variables that were only present in some waves. For each variable/group of variables, we have to only look at the particular wave(s) in which the question was asked, then keep the observations with complete information on those variables. As before, we make lists of variables that only feature in Waves 1, 2, and 4 (`\"A009\"`—stored in `include_wave_1_2_4`), Waves 3 and 4 (`\"C036\"` to `\"C041\"`, `\"X047D\"`—stored in `include_wave_3_4`), and Wave 4 only (`\"X011_01\"` and `\"X025A\"`—stored in `include_wave_4`).\n",
    "\n",
    "First, we put together some useful background info on what questions were only included in which waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A009 is not in Wave 3.\n",
    "# Note that even though it's just one entry, we use square brackets to make it a list\n",
    "include_in_wave_1_2_4 = [\"A009\"]\n",
    "# Work attitudes and income are in Waves 3 and 4.\n",
    "include_in_wave_3_4 = [\"C036\", \"C037\", \"C038\", \"C039\", \"C041\", \"X047D\"]\n",
    "# Number of children and education are in Wave 4.\n",
    "include_in_wave_4 = [\"X011_01\", \"X025A\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the cases for these waves, successively refining the data to just those we wish to keep.\n",
    "\n",
    "Again we will use the `.dropna()` method, but combine it with the logical OR operator, `|`, to include all observations for waves that did not ask the relevant question, along with the complete cases for that question in the other waves.\n",
    "\n",
    "As a concrete example, in the first refinement of the data below we will first pick out any row for which the column `\"A009\"` has an entry *or* (represented by `|`) `\"S002EVS\"` takes the values `\"1981-1984\"` or `\"1990-1993\"`. This keeps observations if they are in Wave 1 (1981-1984) or Wave 2 (1990-1993) or if they are in Waves 3 or 4 with complete information. As `lifesat_data[include_in_wave_3_4].notna()` will create six columns worth of boolean values (one for each variable in `include_in_wave_3_4`), we will then use the row-wise (`axis=1`) `.all()` method to create a single boolean value (`True` or `False`) for every row. This is then combined with the test whether a row is part of Wave 1 or 2 `lifesat_data[\"S002EVS\"].isin([\"1981-1984\", \"1990-1993\"])` in an OR (`|`) test. As a result we get one value (`True` or `False`) for each row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Wave 1 (1981-1984) or Wave 2 (1990-1993), or they are in Waves 3 or 4 with complete information\n",
    "condition_wave_3_4 = (lifesat_data[include_in_wave_3_4].notna()).all(axis=1) | (\n",
    "    lifesat_data[\"S002EVS\"].isin([\"1981-1984\", \"1990-1993\"])\n",
    ")\n",
    "lifesat_data = lifesat_data.loc[condition_wave_3_4, :]\n",
    "\n",
    "# in Wave 4 with complete information on the questions specific to that wave or not in Wave 4\n",
    "condition_wave_4 = (\n",
    "    lifesat_data[include_in_wave_4].notna().all(axis=1)\n",
    ") | ~lifesat_data[\"S002EVS\"].isin([\"2008-2010\"])\n",
    "lifesat_data = lifesat_data.loc[condition_wave_4, :]\n",
    "\n",
    "# in Waves 1, 2, or 4 with complete information on the questions specific to those waves, or in Wave 3\n",
    "condition_wave_1_2_4 = (lifesat_data[include_in_wave_1_2_4].notna().all(axis=1)) | (\n",
    "    lifesat_data[\"S002EVS\"].isin([\"1999-2001\"])\n",
    ")\n",
    "lifesat_data = lifesat_data.loc[condition_wave_1_2_4, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.4\n",
    "\n",
    "**Calculating averages and percentiles**\n",
    "\n",
    "*Calculate average work ethic score*\n",
    "\n",
    "We use the `.mean(axis=1)` (remember it's `axis=0` to aggregate over index and `axis=1` to aggregrate over columns) method to calculate the average work ethic score for each observation (`\"workethic\"`) based on the five survey questions related to working attitudes (`\"C036\"` to `\"C041\"`). As we're still using a multi-level column naming convention, we need to specify three levels of column names to create a new column—but they can all be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data[\"work_ethic\"] = lifesat_data.loc[\n",
    "    :, [\"C036\", \"C037\", \"C038\", \"C039\", \"C041\"]\n",
    "].mean(axis=1)\n",
    "lifesat_data.sample(5, random_state=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating averages and percentiles**\n",
    "\n",
    "Regression package **statsmodels** provides a handy method (`\"ECDF\"`) to obtain an individual’s relative income as a percentile. We do this in the following steps:\n",
    "\n",
    "- We create a new column \"inc_percentile\" and fill it with nans (`np.nan`) for now\n",
    "- We then create a Boolean value (`condition_inc_percentile`) for the relevant years with information in the relevant column\n",
    "- Then we use this to filter the rows in `lifesat_data` that we want to work on. The computation on the right-hand side is:\n",
    "  - groupby the range of years (`\"S002EVS\"`)\n",
    "  - select the income variable (`[\"X047D\"]`)\n",
    "  - use the transform method, which returns a column with the same dimensions as the input data (as opposed to apply, which returns data with only as many dimensions as there are categories in the grouped-by column)\n",
    "  - use a lambda function to apply the ECDF function to every row, and round it using `np.round`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "# create empty col for new variable\n",
    "lifesat_data[\"inc_percentile\"] = np.nan\n",
    "\n",
    "# fill it for waves 3 and 4 with relevant data\n",
    "condition_inc_percentile = (\n",
    "    lifesat_data[\"S002EVS\"].isin([\"1999-2001\", \"2008-2010\"])\n",
    ") & (lifesat_data[\"X047D\"].notna())\n",
    "\n",
    "lifesat_data.loc[condition_inc_percentile, \"inc_percentile\"] = (\n",
    "    lifesat_data.loc[\n",
    "        condition_inc_percentile, :\n",
    "    ]  # Select waves 3 & 4 without missing income data\n",
    "    .groupby(\"S002EVS\")[\"X047D\"]  # groupby wave  # select income variable\n",
    "    .transform(\n",
    "        lambda x: np.round(ECDF(x)(x) * 100, 1)\n",
    "    )  # compute ecdf as % round to 1 decimal place\n",
    ")\n",
    "\n",
    "# see the dataframe with the new column\n",
    "lifesat_data.sample(5, random_state=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.5\n",
    "\n",
    "**Calculating summary statistics**\n",
    "\n",
    "*Create a table showing employment status, by country*\n",
    "\n",
    "One of the most useful features of **pandas** is its composability. We can stack up multiple methods to create just the statistics we want. In this example, we're going to use a succession of methods to create a table showing the employment status (as a percentage) of each country's labour force. The steps are:\n",
    "\n",
    "- Select the data for Wave 4\n",
    "- Group it by employment type (`\"X028\"`) and country (`\"S003\"`). Order will matter later when we use `unstack`; whichever variable is last in the groupby command will be switched from the index to the columns when we call unstack.\n",
    "- Select the column to take observations from. In this case, it makes sense to use employment again.\n",
    "- Count the number of observations\n",
    "- Unstack so that we have a table instead of a long list (with countries as columns)\n",
    "- Transform the numbers into percentages that sum to 100 for each country\n",
    "- Round the values in the table\n",
    "- Because we have more countries than employment statuses, transpose the columns and index\n",
    "\n",
    "Note that when we get to the `.transform` line, we are left with a table that has employment status in the *rows* (which is indexed) and countries in the columns. This means that each value in the table represents the counts of observation of employment statuses in a particular country. The application of the lambda function, `x: x*100/x.sum()`, then computes the proportion of employment type as a fraction of all employment in that particular country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_table = (\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X028\", \"S003\"])[  # Group by employment and country\n",
    "        \"X028\"\n",
    "    ]  # Select employment column\n",
    "    .count()  # Count number of observations in each category (employment-country)\n",
    "    .unstack()  # Turn countries from an index into columns (countries because they are the last groupby variable)\n",
    "    .transform(lambda x: x * 100 / x.sum())  # Compute a percentage\n",
    "    .round(2)  # Round to 2 decimal places\n",
    "    .T  # Tranpose so countries are the index, employment types the columns\n",
    ")\n",
    "\n",
    "sum_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then wanted to export this table for further use elsewhere, we would export it with `sum_table.to_html(filename)`, `sum_table.to_excel(filename)`, `sum_table.to_string(filename)`, `sum_table.to_latex(filename)`, or many other options that you can find [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculate summary statistics by gender*\n",
    "\n",
    "We can also obtain summary statistics on a number of variables at the same time using the `apply` function. To obtain the mean value for each of the required variables, grouped by the gender variable (`\"X001\"`), we can compose methods again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X001\"])[  # Group by gender\n",
    "        [\"A009\", \"A170\", \"work_ethic\", \"X003\", \"X025A_num\", \"X011_01\"]\n",
    "    ]  # Select columns\n",
    "    .mean(numeric_only=True)\n",
    "    .round(2)  # Round to 2 decimal places\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the standard deviation is as simple as replacing `mean()` with `std()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X001\"])[  # Group by gender\n",
    "        [\"A009\", \"A170\", \"work_ethic\", \"X003\", \"X025A_num\", \"X011_01\"]\n",
    "    ]  # Select columns\n",
    "    .std()\n",
    "    .round(2)  # Round to 2 decimal places\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want *both*!? We can have that too, using the `agg` (short for aggregate) method and a list of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X001\"])[  # Group by gender\n",
    "        [\"A009\", \"A170\", \"work_ethic\", \"X003\", \"X025A_num\", \"X011_01\"]\n",
    "    ]  # Select columns\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .round(2)  # Round to 2 decimal places\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in this case, we have a multi-level column object in our dataframe. If you want to flip down the last level of columns to be an index, try using `.stack()`.\n",
    "\n",
    "If you're exporting your results, you're not going to want to use the code names though. So you'll probably want to export your table with the nice names substituted in. You can do this using the dictionaries we created right at the start. Let's use the `label_dict` with a stacked version of the table above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = (\n",
    "    lifesat_data.loc[\n",
    "        lifesat_data[\"S002EVS\"] == \"2008-2010\", :\n",
    "    ]  # Wave 4 only, all columns\n",
    "    .groupby([\"X001\"])[  # Group by gender\n",
    "        [\"A009\", \"A170\", \"work_ethic\", \"X003\", \"X025A_num\", \"X011_01\"]\n",
    "    ]  # Select columns\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .round(2)  # Round to 2 decimal places\n",
    "    .stack()  # bring mean and std into the index\n",
    "    .rename(labels_dict, axis=1)  # rename the columns\n",
    ")\n",
    "\n",
    "tab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't rename `\"work_ethic\"` and `\"X025A_num\"`, because we created those variables *after* the labels dictionary was created (so there are no labels for them). And `\"X001\"` is not a column heading, it's the index name, so we'll have to change that separately.\n",
    "\n",
    "Let's update our dictionary with a new name for `\"X025A_num\"` and a new index name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict.update({\"X025A_num\": \"Education Level\"})\n",
    "tab = tab.rename(labels_dict, axis=1)\n",
    "tab.index.names = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "]  # Set index names empty (two levels because two column levels)\n",
    "\n",
    "tab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.6\n",
    "\n",
    "**Calculating frequencies and percentages**\n",
    "\n",
    "First we need to create a frequency table of the `work_ethic` variable for each wave. This variable only takes values from 1.0 to 5.0 in increments of 0.2 (since it is an average of five whole numbers), so we can group by each value and count the number of observations in each group using the `count` function. Once we have counted the number of observations that have each value (separately for each wave), we compute the percentages by dividing these numbers by the total number of observations *for that wave* using `transform`. For example, if there are 50 observations between 1 and 1.2, and 1,000 observations in that wave, the percentage would be 5%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waves = [\"1999-2001\", \"2008-2010\"]\n",
    "country = \"Germany\"\n",
    "condition = (lifesat_data[\"S002EVS\"].isin(waves)) & (  # Select Waves 3 and 4\n",
    "    lifesat_data[\"S003\"] == country\n",
    ")  # Only select DE for this example\n",
    "\n",
    "# Create a new dataframe with counts by wave and work ethic score\n",
    "ethic_pct = (\n",
    "    lifesat_data.loc[condition, :]\n",
    "    .groupby([\"S002EVS\", \"work_ethic\"])[\"work_ethic\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the counts into a within-wave percentage using 'transform'\n",
    "ethic_pct[\"percentage\"] = (\n",
    "    100 * ethic_pct[\"count\"] / ethic_pct.groupby([\"S002EVS\"])[\"count\"].transform(\"sum\")\n",
    ")\n",
    "ethic_pct.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequencies and percentages are saved in a new dataframe called `ethic_pct`. If you want to look at it, you can type the name into an empty code cell in a Jupyter or Colab Notebook, or, in Visual Studio Code, use the 'Variables' button and navigate to `ethic_pct`.\n",
    "\n",
    "Now that we have the percentages and frequency data, we use **matplotlib** to plot a column chart. To overlay the column charts for both waves and make sure that the plot for each wave is visible, we use the alpha option in the `ax.bar` function to set the transparency level (try changing the transparency to see how it affects your chart’s appearance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for wave in waves:\n",
    "    sub_df = ethic_pct.loc[\n",
    "        ethic_pct[\"S002EVS\"] == wave, :\n",
    "    ]  # For convenience, subset the dataframe\n",
    "    ax.bar(sub_df[\"work_ethic\"], sub_df[\"percentage\"], width=0.2, alpha=0.7, label=wave)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Work ethic\")\n",
    "ax.set_ylabel(\"Percent\")\n",
    "ax.set_title(f\"Distribution of work ethic for {country}\", loc=\"left\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 8.2** Distribution of work ethic scores for Germany.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.7\n",
    "\n",
    "**Plotting multiple lines on a chart**\n",
    "\n",
    "*Calculate average life satisfaction, by wave and country*\n",
    "\n",
    "Before we can plot the line charts, we have to calculate the average life satisfaction for each country in each wave.\n",
    "\n",
    "In Python Walkthrough 8.5 we produced summary tables, grouped by country and employment status. We will copy this process, but now we only require mean values. Countries that do not report the life satisfaction variable for all waves will have an average life satisfaction of ‘NA’. Since each country is represented by a row in the summary table, we use the rowwise and na.omit functions to drop any countries that do not have a value for the average life satisfaction for all four waves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_life_sat = (\n",
    "    lifesat_data.groupby([\"S002EVS\", \"S003\"])[\"A170\"]  # groupby wave and country\n",
    "    .mean()  # take the mean of life satisfaction\n",
    "    .unstack()\n",
    "    .T.dropna(  # one row per country, one wave per column\n",
    "        how=\"any\", axis=\"index\"\n",
    "    )  # drop any rows with missing observations\n",
    ")\n",
    "avg_life_sat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a line chart for average life satisfaction*\n",
    "\n",
    "As the data are already in the format that **matplotlib** likes, namely as a matrix, we can almost use `dataframe.plot` directly. But as we want to make a few tweaks and add a few extras, we need to also create an overall axis (`ax`) and pass that to the dataframe plotting function (`dataframe.plot(ax=ax)` after `fig, ax = plt.suplots()`.\n",
    "\n",
    "The other settings do things like setting the y-label (`ax.set_ylabel`) and add a legend `ax.legend` that is outside of the graphed area `bbox_to_anchor=(1, 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "avg_life_sat.T.plot(ax=ax)\n",
    "ax.set_title(\"Life satisfaction across countries and survey waves\", loc=\"left\")\n",
    "ax.legend(bbox_to_anchor=(1, 1))\n",
    "ax.set_ylabel(\"Mean\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 8.3** Line chart of average life satisfaction across countries and survey waves.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.8\n",
    "\n",
    "**Creating dummy variables and calculating correlation coefficients**\n",
    "\n",
    "To obtain the correlation coefficients between variables, we have to make sure that all variables are numeric. However, the data on gender and employment status are coded using text, so we need to create two new variables (`gender` and `employment`). (We choose to create a new variable rather than overwrite the original variable so that even if we make a mistake, the raw data is still preserved).\n",
    "\n",
    "We can use the `np.where` function to make the value of the variable conditional on whether a logical statement (e.g. `x[\"X001\"] == \"Male\"`) is satisfied or not. As shown below, we can nest `np.where` statements to create more complex conditions, which is useful if the variable contains more than two values (an alternative is to create a categorical column using `.astype(\"category\")` and then `.cat.code` to turn discrete variables into numbers).\n",
    "\n",
    "We used two `np.where` statements for the unemployment variable (`\"X028\"`) so that the new variable will be 1 for full-time employed, 0 for unemployed, and NA if neither condition is satisfied.\n",
    "\n",
    "The first job is to ensure that all of the variables we'd like, \"X003\", \"X025A_num\", \"employment\", \"gender\", \"A009\", \"X047D\", \"X011_01\", \"inc_percentile\", \"A170\", \"work_ethic\", are numeric. We can check this for the variables we already have with `.info()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we need to convert one variable from `object` to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifesat_data[\"X003\"] = lifesat_data[\"X003\"].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the columns employment and gender don't exist yet; we'll create them soon\n",
    "\n",
    "cols_to_select = [\n",
    "    \"X003\",\n",
    "    \"X025A_num\",\n",
    "    \"employment\",\n",
    "    \"gender\",\n",
    "    \"A009\",\n",
    "    \"X047D\",\n",
    "    \"X011_01\",\n",
    "    \"inc_percentile\",\n",
    "    \"A170\",\n",
    "    \"work_ethic\",\n",
    "]\n",
    "\n",
    "corr_matrix = (\n",
    "    lifesat_data.loc[lifesat_data[\"S002EVS\"] == \"2008-2010\", :]\n",
    "    .assign(\n",
    "        gender=lambda x: np.where(x[\"X001\"] == \"Male\", 0, 1),\n",
    "        employment=lambda x: np.where(\n",
    "            x[\"X028\"] == \"Full time\", 1, np.where(x[\"X028\"] == \"Unemployed\", 0, np.nan)\n",
    "        ),\n",
    "    )\n",
    "    .loc[:, cols_to_select]\n",
    "    .corr()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `assign` method here. It can get confusing when performing operations on columns and rows because there are several different methods you can use: `assign`, `apply`, `transform`, and `agg`. Agg, apply, and transform are all methods that you use *after* a groupby operation.\n",
    "\n",
    "Here's a quick guide on when to use each of the three that follow a groupby:\n",
    "\n",
    "- Use `.agg` when you're using a groupby but you want your groups to become the new index (rows)\n",
    "- Use `.transform` when you're using a groupby but you want to retain your original index\n",
    "- Use `.apply` when you're using a groupby, but you want to perform operations that will leave neither the original index nor an index of groups\n",
    "\n",
    "Let's see examples of all three on some dummy data. First, let's create the dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_s = 1000\n",
    "prng = np.random.default_rng(42)  # prng=probabilistic random number generator\n",
    "s = pd.Series(\n",
    "    index=pd.date_range(\"2000-01-01\", periods=len_s, name=\"date\", freq=\"D\"),\n",
    "    data=prng.integers(-10, 10, size=len_s),\n",
    ")\n",
    "s.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the result of using each kind of the three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n`.agg` following `.groupby`: groups provide index\")\n",
    "print(s.groupby(s.index.to_period(\"M\")).agg(\"skew\").head())\n",
    "print(\"\\n`.transform` following `.groupby`: retain original index\")\n",
    "print(s.groupby(s.index.to_period(\"M\")).transform(\"skew\").head())\n",
    "print(\"\\n`.apply` following `.groupby`: index entries can be new\")\n",
    "print(s.groupby(s.index.to_period(\"M\")).apply(lambda x: x[x > 0].cumsum()).head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assign`, meanwhile, is used when you want to add new columns to a dataframe *in place*. It's sister function is the pure assignment by creating a new column directly. Let's see both of these on the dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make s a dataframe rather than just a series\n",
    "s = pd.DataFrame(s, columns=[\"number\"])\n",
    "# creating data directly\n",
    "s[\"new_column_directly\"] = 10\n",
    "s.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using assign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data using assign\n",
    "s = s.assign(new_column_indirectly=11)\n",
    "s.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the correlation matrix we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only interested in two columns, so select those\n",
    "corr_matrix.loc[:, [\"A170\", \"work_ethic\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.9\n",
    "\n",
    "**Calculating group means**\n",
    "\n",
    "*Calculate average life satisfaction and differences in average life satisfaction*\n",
    "\n",
    "We can achieve the tasks in Question 4(a) and (b) in one go using an approach similar to that used in Python Walkthrough 8.5, although now we are interested in calculating the average life satisfaction by country and employment type. Once we have tabulated these means, we can compute the difference in the average values. We will create two new variables: `D1` for the difference between the average life satisfaction for full-time employed and unemployed, and `D2` for the difference in average life satisfaction for full-time employed and retired individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the employment types that we wish to report\n",
    "employment_list = [\"Full time\", \"Retired\", \"Unemployed\"]\n",
    "\n",
    "df_employment = (\n",
    "    lifesat_data.loc[\n",
    "        (lifesat_data[\"S002EVS\"] == \"2008-2010\")\n",
    "        & (lifesat_data[\"X028\"].isin(employment_list)),  # row selection\n",
    "        :,  # col selection—all columns\n",
    "    ]  # select wave 4 and these specific emp types\n",
    "    .groupby([\"S003\", \"X028\"])  # group by country and employment type\n",
    "    .mean(numeric_only=True)[\n",
    "        \"A170\"\n",
    "    ]  # mean value of life satisfaction by country and employment\n",
    "    .unstack()  # reshape to one row per country (country is inner layer)\n",
    "    .assign(  # create the differences in means\n",
    "        D1=lambda x: x[\"Full time\"] - x[\"Unemployed\"],\n",
    "        D2=lambda x: x[\"Full time\"] - x[\"Retired\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "df_employment.round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Make a scatterplot sorted according to work ethic*\n",
    "\n",
    "In order to plot the differences ordered by the average work ethic, we first need to get all data from Wave 4 (using `.loc`), summarise the `work_ethic` variable by country (`groupby` then take the `mean`), and store the results in a temporary dataframe (`df_work_ethic`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work_ethic = (\n",
    "    lifesat_data.loc[\n",
    "        (lifesat_data[\"S002EVS\"] == \"2008-2010\"), [\"S003\", \"work_ethic\"]\n",
    "    ]  # select wave 4 and two columns only\n",
    "    .groupby([\"S003\"])  # group by country\n",
    "    .mean()  # mean value of work_ethic by country\n",
    ")\n",
    "\n",
    "df_work_ethic.head().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the mean `work_ethic` data with the table containing the difference in means (using an inner join to match the data correctly by country) and make a scatterplot using **matplotlib**. This process can be repeated for the difference in means between full-time employed and retired individuals by changing `y = D1` to `y = D2` in the function we create for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp_ethic_comb = pd.merge(df_employment, df_work_ethic, on=[\"S003\"], how=\"inner\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df_emp_ethic_comb[\"work_ethic\"], df_emp_ethic_comb[\"D1\"])\n",
    "ax.set_ylabel(\"Difference\")\n",
    "ax.set_xlabel(\"Work ethic\")\n",
    "ax.set_title(\n",
    "    \"Difference in wellbeing between the\\nfull-time employed and the unemployed vs work ethic\",\n",
    "    size=14,\n",
    ")\n",
    "ax.set_ylim(0, None)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Figure 8.5** Difference in life satisfaction (wellbeing) between the full-time employed and the unemployed vs average work ethic.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate correlation coefficients, use the `corr` function applied to a dataframe. You can see that the correlation between average work ethic and difference in life satisfaction is quite weak for employed vs unemployed, but moderate and positive for employed vs retired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp_ethic_comb.corr().loc[[\"D1\", \"D2\"], [\"work_ethic\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 8.10\n",
    "\n",
    "**Calculating confidence intervals and adding error bars**\n",
    "\n",
    "We will use Turkey, Spain, and Great Britain as example countries in the top, middle, and bottom third of work ethic scores respectively.\n",
    "\n",
    "In the tasks in Questions 1(a) and (b) we will obtain the means, standard errors, and 95% confidence intervals step-by-step, then for Question 1(c) we show how to use a shortcut to obtain confidence intervals from a single function.\n",
    "\n",
    "*Calculate confidence intervals manually*\n",
    "\n",
    "We obtained the difference in means in Python Walkthrough 8.9 (`D1` and `D2`), so now we can calculate the standard error of the means for each country of interest. We'll do this the long way round, using the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = [\"Turkey\", \"Spain\", \"Great Britain\"]\n",
    "\n",
    "df_emp_se = (\n",
    "    lifesat_data.loc[\n",
    "        (lifesat_data[\"S002EVS\"] == \"2008-2010\")\n",
    "        & (lifesat_data[\"X028\"].isin(employment_list))\n",
    "        & (lifesat_data[\"S003\"].isin(country_list)),\n",
    "        :,\n",
    "    ]  # select the relevant employment types, countries, and wave 4\n",
    "    .groupby([\"S003\", \"X028\"])  # groupby country and employment type\n",
    "    .apply(lambda x: x[\"A170\"].std() / np.sqrt(x[\"A170\"].count()))\n",
    "    # .std(ddof=0)[\"A170\"]  # find the standard dev of life satisfaction\n",
    "    .unstack()  # put the employment types along the columns\n",
    "    .assign(  # calculate the standard errors of the differences\n",
    "        D1_SE=lambda x: (x[\"Full time\"].pow(2) + x[\"Unemployed\"].pow(2)).pow(1 / 2),\n",
    "        D2_SE=lambda x: (x[\"Full time\"].pow(2) + x[\"Retired\"].pow(2)).pow(1 / 2),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_emp_se.round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine the standard errors with the difference in means, and compute the 95% confidence interval width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp_subset = (\n",
    "    df_employment.loc[df_employment.index.isin(country_list), [\"D1\", \"D2\"]]\n",
    "    .join(df_emp_se.loc[:, [\"D1_SE\", \"D2_SE\"]], how=\"inner\")\n",
    "    .assign(CI_1=lambda x: 1.96 * x[\"D1_SE\"], CI_2=lambda x: 1.96 * x[\"D2_SE\"])\n",
    ")\n",
    "\n",
    "df_emp_subset.round(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a table containing the difference in means, the standard error of the difference in means, and the confidence intervals for each of the two differences. (Recall that `D1` is the difference between the average life satisfaction for full-time employed and unemployed, and `D2` is the difference in average life satisfaction for full-time employed and retired individuals.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Calculate confidence intervals using a t-test function*\n",
    "\n",
    "We could obtain the confidence intervals directly by using the t-test function from the **pingouin** package. We already imported it at the start of this chapter, but if you didn't already, run `import pingouin as pg`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to prepare the data in two groups. In the following example we go through the difference in average life satisfaction for full-time employed and unemployed individuals in Turkey, but the process can be repeated for the difference between full-time employed and retired individuals by changing the code appropriately (also for your other two chosen countries).\n",
    "\n",
    "We start by selecting the data for full-time and unemployed people in Turkey, and storing it in two separate temporary matrices (arrays of rows and columns) called `turkey_full` and `turkey_unemployed` respectively, which is the format needed for the t-test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a boolean that's true for wave 4, Turkey, and full time\n",
    "full_boolean = (\n",
    "    (lifesat_data[\"S002EVS\"] == \"2008-2010\")\n",
    "    & (lifesat_data[\"S003\"] == \"Turkey\")\n",
    "    & (lifesat_data[\"X028\"] == \"Full time\")\n",
    ")\n",
    "\n",
    "turkey_full = (\n",
    "    lifesat_data.loc[full_boolean, [\"A170\"]]  # select the life satisfaction data\n",
    "    .astype(\"double\")  # ensure this is a floating point number\n",
    "    .values  # grab the values--this is needed for the t-test\n",
    ")\n",
    "\n",
    "# do the same for the unemployed\n",
    "unem_boolean = (\n",
    "    (lifesat_data[\"S002EVS\"] == \"2008-2010\")\n",
    "    & (lifesat_data[\"S003\"] == \"Turkey\")\n",
    "    & (lifesat_data[\"X028\"] == \"Unemployed\")\n",
    ")\n",
    "turkey_unemployed = lifesat_data.loc[unem_boolean, [\"A170\"]].astype(\"double\").values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, for full employment, we created a Boolean row filter based on three different columns using:\n",
    "\n",
    "```python\n",
    "lifesat_data[\"S002EVS\"] == \"2008-2010\" & lifesat_data[\"S003\"] == \"Turkey\" & lifesat_data[\"X028\"] == \"Full time\"\n",
    "```\n",
    "\n",
    "and put this into `.loc` using the `.loc[rows, columns]` syntax. `.loc` isn't the only way to select rows to operate on though: an alternative is to use `.query` and pass it a string (some text) that asks for particular column values. This is easiest to demonstrate with an example:\n",
    "\n",
    "```python\n",
    "turkey_full = (\n",
    "    lifesat_data\n",
    "    # select wave 4, Turkey, and full time\n",
    "    .query(\"S002EVS == '2008-2010' and S003 == 'Turkey' and X028 == 'Full time'\")\n",
    "    .loc[:, [\"A170\"]]  # select the life satisfaction data\n",
    "    .astype(\"double\")  # ensure this is a floating point number\n",
    "    .values  # grab the values--this is needed for the t-test\n",
    ")\n",
    "```\n",
    "\n",
    "You can see we still use a `.loc` after the query line, but only to select columns (we select all rows from the previous step using `:`).\n",
    "\n",
    "Sometimes `.query` can be shorter or clearer to write than a conditional statement; it varies depending on the case. Can you write the equivalent query for the unemployed? It's useful to know both but, if in doubt, simply use `.loc`.\n",
    "\n",
    "Let's move on now. We can use the `ttest` function from the **pingouin** package on the two newly created vectors. The default confidence interval level is 95% and can be changed via the `confidence=` keyword argument. Note that the `ravel` function turns an array like `[[1], [5], [0]]` into an array of the form `[1, 5, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest = pg.ttest(turkey_full.ravel(), turkey_unemployed.ravel()).round(3)\n",
    "ttest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the difference in means by finding the midpoint of the interval (`ttest[\"CI95%\"].iloc[0].mean()`is 0.74), which should be the same as the figures obtained in Question 1(b) (df.employment[3, 2] is 0.7374582)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add error bars to the column charts*\n",
    "\n",
    "We can now use these confidence intervals (and widths) to add error bars to our column charts. To do so, we use the geom_errorbar option, and specify the lower and upper levels of the confidence interval for the ymin and ymax options respectively. In this case it is easier to use the results from Questions 1(a) and (b), as we already have the values for the difference in means and the CI width stored as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(df_emp_subset.index, df_emp_subset[\"D1\"], yerr=df_emp_subset[\"CI_1\"])\n",
    "ax.set_ylabel(\"Difference in means\")\n",
    "ax.set_xlabel(\"Country\")\n",
    "ax.set_title(\"Difference in well-being (full-time vs unemployed)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 8.7* Difference in life satisfaction (well-being) between full-time employed and unemployed.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this can be repeated for the difference in life satisfaction between full-time employed and retired. Remember to change `df_emp_subset[\"D1\"]` to `df_emp_subset[\"D2\"]`, and change the error bars correspondingly too."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter used the following packages where *sys* is the Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "26c4b888616894cf8360ee3d370b6a41732ef00c8f1d61e869c42a8428cf1ac1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
