{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Walkthrough 1.1\n",
    "\n",
    "## Importing the datafile into Python\n",
    "\n",
    "We want to import the datafile called ‘NH.Ts+dSST.csv’ from NASA's website into Python using Visual Studio Code.\n",
    "\n",
    "We start by opening Visual Studio Code in the folder we'll be working in. Use File -> Open Folder to do this. We also need to ensure that the interactive Python window that we'll be using to run Python code opens in this folder. In Visual Studio Code, you can ensure that the interactive window starts in the folder that you have open by setting “Jupyter: Notebook File Root” to `${workspaceFolder}` in the Settings menu.\n",
    "\n",
    "Now create a new file (File -> New File in the menu) and name it `exercise_1.py`. In the new and empty file, right-click and select \"Run file in interactive window\". This will launch the interactive Python window.\n",
    "\n",
    "You will need to install the following packages: **pandas**, **numpy** and **matplotlib**. Packages are add-ons that extend the functionality of the Python language. To install packages, hit use the <kbd>⌘</kbd> + <kbd>\\`</kbd> keyboard shortcut (Mac) or <kbd>ctrl</kbd> + <kbd>\\`</kbd> (Windows/Linux), or click \"View > Terminal\". A new box, called the terminal, should appear at the bottom of the screen. To install packages, type `pip install packagename` into this box.\n",
    "\n",
    "**pandas**, **numpy** and **matplotlib** provide extra functionality for data analysis, numbers, and plotting respectively.\n",
    "\n",
    "We will download the data directly from the internet into our Python session using the **pandas** library (which provides data manipulation in Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n",
    "    skiprows=1,\n",
    "    na_values=\"***\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the `read_csv` function, we added two options. If you open the spreadsheet in Excel, you will see that the real data only starts in Row 2, so we use the `skiprows = 1` option to skip the first row when importing the data. When looking at the spreadsheet, you can see that missing temperature data is coded as `***`. In order to ensure that the missing temperature data are recorded as numbers, we tell **pandas** that `na_values = \"***\"` which imports those missing values as `NaN`, which means the number is missing.\n",
    "\n",
    "To check that the data have been imported correctly, you can use the `.head()` function to view the first five rows of the dataset, and confirm that they correspond to the columns in the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with this data, we use the `.info()` function to check that the data were read in as numbers (either real numbers, `float64`, or integers, `int`) rather than strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all variables are formatted as either `float64` or `int`, so Python correctly recognises that these data are numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 1.2\n",
    "\n",
    "**Drawing a line chart of temperature and time**\n",
    "\n",
    "We will now set the year as the *index* of the dataset. This will make plotting the time series of temperature easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"Year\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll make our chart. We'll use the **matplotlib** package for this. The built-in style isn't very attractive so we'll first switch to a prettier one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\n",
    "    \"https://github.com/aeturrell/coding-for-economists/raw/main/plot_style.txt\"\n",
    ")\n",
    "plt.rcParams[\"figure.figsize\"] = [6, 3]\n",
    "plt.rcParams[\"figure.dpi\"] = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these variables to draw line charts using the plot function. As an example, we will draw a line chart using data for January, `df[\"Jan\"]` for the years 1880—2016. \n",
    "\n",
    "The first line creates a variable for the month (in case we'd like to replot a different month later), the second creates an empty chart, the next two create a horizontal line and annotate it. Then `df[month].plot(ax=ax)` adds the temperature data to the chart. Finally, the title and y-axis label are set. We ensure that the month and year used in the title is always up to date and consistent with the data that we're pulling in by using the variables `month` and `df.index.max()` to write the month and maximum year into the title respectively. (This trick is called an f-string, or function string, and the string begins with an `f` before the opening of the quotation marks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = \"Jan\"\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(0, color=\"orange\")\n",
    "ax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\n",
    "df[month].plot(ax=ax)\n",
    "ax.set_title(\n",
    "    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n",
    ")\n",
    "ax.set_ylabel(\"Annual temperature anomalies\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different values for `color`, `xy`, and the first argument of `ax.axhline` in the plot function to figure out what these options do. (Note that `xycoords` set the behaviour of `xy`.)\n",
    "\n",
    "It is important to remember that all axis and chart titles should be enclosed in quotation marks (`\"\"`), as well as any words that are not options (for example, colour names or filenames)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 1.3\n",
    "\n",
    "**Producing a line chart for the annual temperature anomalies**\n",
    "\n",
    "This is where the power of programming languages becomes evident: to produce the same line chart for a different variable, we simply take the code used in Python walk-through 1.2 and replace the `\"Jan\"` with the name for the annual variable (`\"J-D\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = \"J-D\"\n",
    "fig, ax = plt.subplots()\n",
    "ax.axhline(0, color=\"orange\")\n",
    "ax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\n",
    "df[month].plot(ax=ax)\n",
    "ax.set_title(\n",
    "    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{df.index.max()})\"\n",
    ")\n",
    "ax.set_ylabel(\"Annual temperature anomalies\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 1.4\n",
    "\n",
    "**Creating frequency tables and histograms**\n",
    "\n",
    "Since we will be looking at data from different subperiods (year intervals) separately, we will create a categorical variable (a variable that has two or more categories) that indicates the subperiod for each observation (row). In Python this type of variable is called a ‘category’ or categorical. When we create a categorical column, we need to define the categories that this variable can take.\n",
    "\n",
    "We'll achieve this using a dictionary, a special type of object in Python that provides a mapping from one value to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Period\"] = pd.cut(\n",
    "    df.index,\n",
    "    bins=[1921, 1950, 1980, 2010],\n",
    "    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n",
    "    ordered=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a new variable called `\"Period\"` and defined the possible categories using the `labels=` keyword argument. Since we will not be using data for some years (before 1921 and after 2010), we want `\"Period\"` to take the value `NaN` (not a number) for these observations (rows), and the appropriate category for all the other observations. The `pd.cut` function does this automatically.\n",
    "\n",
    "Let's take a look at the last 20 entries using `.tail`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Period\"].tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd really like to combine the data from the three summer months. This is easy to do using the `.stack` function. Let's look at the first few rows of the data once stacked using `.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_months = [\"Jun\", \"Jul\", \"Aug\"]\n",
    "df[list_of_months].stack().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use all monthly anomalies from June, July, and August, but they are currently in three separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\n",
    "for ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n",
    "    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n",
    "    ax.set_title(period)\n",
    "plt.suptitle(\"Histogram of temperature anomalies\")\n",
    "axes[1].set_xlabel(\"Summer temperature distribution\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain what a histogram displays, let's take a look at the histogram for the period from 1921—1950. We will look first at the highest of the bars, which is centred at –0.15. This bar represents values of the temperature anomalies that fall in the interval from –0.2 to –0.1. The height of this bar is a representation of how many values fall into this interval, (23 observations, in this case). As it is the highest bar, this indicates that this is the interval in which the largest proportion of temperature anomalies fell for the period from 1921 to 1950. As you can see, there are virtually no temperature anomalies larger than 0.3. The height of these bars gives a useful overview of the distribution of the temperature anomalies.\n",
    "\n",
    "Now consider how this distribution changes as we move through the three distinct time periods. The distribution is clearly moving to the right for the period 1981–2010, which is an indication that the temperature is increasing; in other words, an indication of global warming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Python Walkthrough 1.5\n",
    "\n",
    "**Using the `quantile` function**\n",
    "\n",
    "First, we need to create a variable that contains all monthly anomalies in the years 1951—1980. Then, we'll use **pandas** `pd.qcut` function to find the required percentiles (0.3 and 0.7 refer to the 3rd and 7th deciles, respectively).\n",
    "\n",
    "Note: You may get slightly different values to those shown here if you are using the latest data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\n",
    "temp_all_months = df.loc[(df.index >= 1951) & (df.index <= 1980), \"Jan\":\"Dec\"]\n",
    "# Put all the data in stacked format and give the new columns sensible names\n",
    "temp_all_months = (\n",
    "    temp_all_months.stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n",
    ")\n",
    "# Take a look at this data:\n",
    "temp_all_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.3, 0.7]\n",
    "list_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n",
    "\n",
    "print(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\n",
    "print(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Walkthrough 1.6\n",
    "\n",
    "**Computing the proportion of anomalies at a given quantile using the `.mean()` function**\n",
    "\n",
    "Note: You may get slightly different values to those shown here if you are using the latest data.\n",
    "\n",
    "We repeat the steps used in Python walk-through 1.5, now looking at monthly anomalies in the years 1981—2010. We can simply change the year values in the code from Walkthrough 1.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)\n",
    "temp_all_months = df.loc[(df.index >= 1981) & (df.index <= 2010), \"Jan\":\"Dec\"]\n",
    "# Put all the data in stacked format and give the new columns sensible names\n",
    "temp_all_months = (\n",
    "    temp_all_months.stack()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n",
    ")\n",
    "# Take a look at the start of this data data:\n",
    "temp_all_months.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the monthly data for 1981—2010, we want to count the proportion of observations that are smaller than –0.1. We'll first create a *binary indicator* (ie it's True or False) that says, for each row (observation) in `temp_all_months`, whether the number is lower than the 0.3 quantile or not (given by `list_of_percentiles[0]`). Then we'll take the mean of this list of True and False values; when you take the mean of binary variables, each True evaluates to 1 and each False to 0, so the mean gives us the proportion of entries in `temp_all_months` that are lower than the 0.3 quantile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_less_than_q30 = temp_all_months[\"values\"] < list_of_percentiles[0]\n",
    "proportion_under_q30 = entries_less_than_q30.mean()\n",
    "print(f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we printed out the answer, we used some *number formatting*. This is written as `:.2f` within the curly brackets part of an f-string—this tells Python to display the number with two decimal places. You should also note that, as well as the mean given by `.mean()`, there are various other built-in functions like `.std()` for the standard deviation and `.var()` for the variance.\n",
    "\n",
    "Now we can assess that between 1951 and 1980, 30% of observations for the temperature anomaly were smaller than –0.10, but between 1981 and 2010 only about two per cent of months are considered cold. That is a large change.\n",
    "\n",
    "Let’s check whether we get a similar result for the number of observations that are larger than 0.11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_over_q70 = (temp_all_months[\"values\"] > list_of_percentiles[1]).mean()\n",
    "print(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 1.7\n",
    "\n",
    "**Calculating and understanding mean and variance**\n",
    "\n",
    "The process of computing the mean and variance separately for each period and season separately would be quite tedious. We would prefer a way to cover all of them at once. Let's re-stack the data in a form where `season` is one of the columns and could take the values `DJF`, `MAM`, `JJA`, or `SON`. Let's also have a peek at the structure of the data while we're at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_all_months = df.loc[:, \"DJF\":\"SON\"].stack().reset_index().rename(columns={\"level_1\": \"season\", 0: \"values\"})\n",
    "temp_all_months[\"Period\"] = pd.cut(\n",
    "    temp_all_months[\"Year\"],\n",
    "    bins=[1921, 1950, 1980, 2010],\n",
    "    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n",
    "    ordered=True,\n",
    ")\n",
    "temp_all_months.iloc[-135:-125]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take the mean and variance at once. We're going to *group* our data using a `groupby` operation that we pass a list of the variables we'd like to group together; here that will be `\"Period\"` and `\"season\"`. The variable we'd like to apply the grouping to is `\"values\"` so we then filter down to just the `\"values\"` column. Finally we're going to do an *aggregation* using the `.agg` function and we'll pass that a list of functions we'd like to apply. We'll apply `np.mean` and `np.var`, which, respectively, take the mean and variance of any values they are applied to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_mean_var = temp_all_months.groupby([\"season\", \"Period\"])[\"values\"].agg([np.mean, np.var])\n",
    "grp_mean_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recognise that the variances seem to remain fairly constant across the first two periods, but they do increase markedly for the 1981—2010 period.\n",
    "\n",
    "We can plot a line chart to see these changes graphically. (This type of chart is formally known as a ‘time-series plot’). One trick we can use here is that calling `.plot` on wide-format data (with many columns) is interpreted by **pandas** as you wanting to plot all of the columns you've selected. So, we can do a quick transform to our data and have **pandas** plot it in the way we'd like, with all of the different seasons represented.\n",
    "\n",
    "To do this, we'll set `\"Year\"` and `\"season\"` as the index but then unstack them so that season forms the columns of our data. We'll then only select the `\"values\"` entries as we're not using `\"Period\"` right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_format_data = temp_all_months.set_index([\"Year\", \"season\"]).unstack()[\"values\"]\n",
    "wide_format_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the data by calling `.plot` and passing that function the axis, here called `ax`, we would like it to use. We'll also add a horizontal line at zero and some labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axhline(0, color=\"black\", alpha=0.3)\n",
    "ax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\n",
    "wide_format_data.plot(linewidth=1, ax=ax)\n",
    "ax.set_title(\n",
    "    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{wide_format_data.index.max()})\"\n",
    ")\n",
    "ax.set_ylabel(\"Annual temperature anomalies\");"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13950ff975ea58bb356510d0d8f98cdd1bd1f12bd4ccce66a17a21f4f1a23379"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('core-python-wDU3726x-py3.9': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
