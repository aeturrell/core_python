{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Project 11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Settings\n",
    "\n",
    "Let's import the packages we'll need and also configure the settings we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "import pingouin as pg\n",
    "import warnings\n",
    "\n",
    "\n",
    "### You don't need to use these settings yourself\n",
    "### — they are just here to make the book look nicer!\n",
    "# Set the plot style for prettier charts:\n",
    "plt.style.use(\"plot_style.txt\")\n",
    "# Make seaborn work consistently with this\n",
    "so.Plot.config.theme.update(mpl.rcParams)\n",
    "# Ignore warnings to make nice output\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 11.1\n",
    "\n",
    "**Importing data and recoding variables**\n",
    "\n",
    "Before importing data in Excel or .csv format, open it to ensure you understand the structure of the data and check if any additional options are required for the `pd.read_excel` function in order to import the data correctly. In this case the data is in a worksheet called ‘Data’, there are no missing values to worry about, and the first row contains the variable names. This format should be straightforward to import *but* this file is in the Strict Open XML Spreadsheet (.xlsx) rather than Excel Workbook (.xlsx) and loading it with `pd.read_excel` will fail. You'll need to re-save it in \"xlsx: format and then import the data using the `pd.read_excel` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp = pd.read_excel(\n",
    "    Path(\"data/doing-economics-datafile-working-in-excel-project-11.xlsx\"),\n",
    "    sheet_name=\"Data\",\n",
    ")\n",
    "wtp.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse-code Variables\n",
    "\n",
    "The first task is to recode variables related to the respondents’ views on certain aspects of government behaviour and attitudes about global warming (`cog_2`, `cog_5`, `scepticism_6`, and `scepticism_7`). This coding makes the interpretation of high and low values consistent across all questions, since the survey questions do not have this consistency.\n",
    "\n",
    "To recode all of these values (across several variables) in one go, we use dictionary mapping: that is, using the `map` function to convert specific values to new values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {1: 5, 2: 4, 3: 3, 4: 2, 5: 1}\n",
    "vars_of_interest = [\"cog_2\", \"cog_5\", \"scepticism_6\", \"scepticism_7\"]\n",
    "for col in vars_of_interest:\n",
    "    wtp[col] = wtp[col].map(map_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new variables containing WTP amounts\n",
    "\n",
    "Although we could employ the same technique as above to recode the value for the minimum and maximum willingness to pay variables, an alternative is to use the `pd.merge` function. This function allows us to combine two dataframes via values given in a particular variable.\n",
    "\n",
    "We start by creating a new dataframe (`category_amount`) that has two variables: the original category value and the corresponding new euro amount. We then apply the `pd.merge` function to the `wtp` dataframe and the new dataframe, specifying the variables that link the data in each dataframe together. We're going to match on the `WTP_plmin` and, separately, the `WTP_plmax` options, to put in new columns for the min WTP and max WTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector containing the euro amounts\n",
    "wtp_euro_levels = [48, 72, 84, 108, 156, 192, 252, 324, 432, 540, 720, 960, 1200, 1440]\n",
    "\n",
    "# create a dataframe from this vector\n",
    "category_amount = pd.DataFrame({\"original\": range(1, 15), \"new\": wtp_euro_levels})\n",
    "\n",
    "# creating a new column for min WTP\n",
    "wtp = pd.merge(\n",
    "    category_amount, wtp, how=\"right\", left_on=\"original\", right_on=\"WTP_plmin\"\n",
    ").rename(columns={\"new\": \"WTP_plmin_euro\"})\n",
    "\n",
    "# creating a new column for max WTP\n",
    "wtp = pd.merge(\n",
    "    category_amount, wtp, how=\"right\", left_on=\"original\", right_on=\"WTP_plmax\"\n",
    ").rename(columns={\"new\": \"WTP_plmax_euro\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 11.2\n",
    "\n",
    "**Creating indices from multiple columns**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create all of the required indices in three steps using column-aggregation operations; here the `.mean`. In each step we use the relevant function with `axis=1`, ie aggregation over columns. We then insert the single, created column back into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp[\"climate\"] = wtp[[\"scepticism_2\", \"scepticism_6\", \"scepticism_7\"]].mean(axis=1)\n",
    "wtp[\"gov_intervention\"] = wtp[\n",
    "    [\"cog_1\", \"cog_2\", \"cog_3\", \"cog_4\", \"cog_5\", \"cog_6\"]\n",
    "].mean(axis=1)\n",
    "wtp[\"pro_environment\"] = wtp[[\"PN_1\", \"PN_2\", \"PN_3\", \"PN_4\", \"PN_6\", \"PN_7\"]].mean(\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 11.3\n",
    "\n",
    "**Calculating correlation coefficients**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate correlation coefficients and Cronbach's alpha\n",
    "\n",
    "We covered calculating correlation coefficients in Python walkthrough 10.1. In this case, since there are no missing values we can use the `.corr` method without any additional options.\n",
    "\n",
    "For the questions on climate change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp[[\"scepticism_2\", \"scepticism_6\", \"scepticism_7\"]].corr().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the questions on government behaviour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp[[\"cog_1\", \"cog_2\", \"cog_3\", \"cog_4\", \"cog_5\", \"cog_6\"]].corr().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, the questions on personal behaviour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp[[\"PN_1\", \"PN_2\", \"PN_3\", \"PN_4\", \"PN_6\", \"PN_7\"]].corr().round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cronbach's alpha\n",
    "\n",
    "It is straightforward to compute the Cronbach’s alpha using the `cronbach_alpha` function from the **pingouin** package. This function calculates Cronbach’s alpha. Let's look at it for these three sets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "pg.cronbach_alpha(data=wtp[[\"scepticism_2\", \"scepticism_6\", \"scepticism_7\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.cronbach_alpha(data=wtp[[\"cog_1\", \"cog_2\", \"cog_3\", \"cog_4\", \"cog_5\", \"cog_6\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.cronbach_alpha(data=wtp[[\"PN_1\", \"PN_2\", \"PN_3\", \"PN_4\", \"PN_6\", \"PN_7\"]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 11.4\n",
    "\n",
    "**Using loops to obtain summary statistics**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two different formats (DC and TWPL) are recorded in the variable `abst_format`, and take the values `ref` and `ladder` respectively. We will store all the variables of interest into a list called `variables`, and use a \"for\" loop to calculate summary statistics for each variable and present it in a table (using `pd.crosstab`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\"sex\", \"age\", \"kids_nr\", \"hhnetinc\", \"member\", \"education\"]\n",
    "\n",
    "for var in variables:\n",
    "    print(pd.crosstab(wtp[var], wtp[\"abst_format\"], normalize=\"columns\").round(2))\n",
    "    print(\"————\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above gives the required tables, but is not easy to read. You may want to tidy up the results, for example by translating (from German to English) and reordering the options in the household net income variable (`hhnetinc`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 11.5\n",
    "\n",
    "**Calculating summary statistics**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agg` function can provide multiple statistics for a number of variables in one command. You will need to provide a list of the variables you want to summarise (after your groupby) and then use the `agg` option to specify the summary statistics you need. Here, we need the mean, standard deviation, mean, and max for the variables `climate`, `gov_intervention`, and `pro_environment`. Finally, `.stack` puts the data into a longer (and here more readable) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp.groupby(\"abst_format\").agg([\"mean\", \"std\", \"min\", \"max\"])[\n",
    "    [\"climate\", \"gov_intervention\", \"pro_environment\"]\n",
    "].stack()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 11.6\n",
    "\n",
    "**Summarising willingness to pay variables**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create column charts for minimum and maximum WTP\n",
    "\n",
    "Before we can plot a column chart, we need to compute frequencies (number of observations) for each value of the willingness to pay (1–14). We do this separately for the minimum and maximum willingness to pay.\n",
    "\n",
    "In each case we select the relevant variable and remove any observations with missing values using the `.dropna()` method. We can then separate the data by level (WTP amount) of the `WTP_plmin_euro` or `WTP_plmax_euro` variables (using `groupby`), then obtain a frequency count using the [todo] function.\n",
    "\n",
    "Once we have the frequency count stored as a dataframe, we can plot the column charts.\n",
    "\n",
    "For the minimum willingness to pay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn.objects as so\n",
    "\n",
    "df_plmin = (\n",
    "    wtp[[\"WTP_plmin_euro\"]]\n",
    "    .dropna()\n",
    "    .astype(\"int\")\n",
    "    .astype(\"category\")\n",
    "    .value_counts()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "(\n",
    "    so.Plot(df_plmin, x=\"WTP_plmin_euro\", y=0)\n",
    "    .add(so.Bar())\n",
    "    .label(x=\"Minimum WTP (euros)\", y=\"Frequency\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 11.4 Minimum WTP (euros).**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do the same for the maximum willingness to pay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plmax = (\n",
    "    wtp[[\"WTP_plmax_euro\"]]\n",
    "    .dropna()\n",
    "    .astype(\"int\")\n",
    "    .astype(\"category\")\n",
    "    .value_counts()\n",
    "    .sort_values()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "(\n",
    "    so.Plot(df_plmax, x=\"WTP_plmax_euro\", y=0)\n",
    "    .add(so.Bar())\n",
    "    .label(x=\"Maximum WTP (euros)\", y=\"Frequency\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 11.5 Maximum WTP (euros).**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate average WTP for each individual\n",
    "\n",
    "We can use the `mean` function to obtain the average of the minimum and maximum willingness to pay (combining the two columns at each row using the `axis=1` keyword argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp[\"wtp_average\"] = wtp[[\"WTP_plmin_euro\", \"WTP_plmax_euro\"]].mean(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean and median WTP across individuals\n",
    "\n",
    "The mean and median of this average value can be obtained using the `mean` and `median` functions. Note that invalid entries, such a NaNs, are omitted by default. And it's also worth noting that there's only a single remaining dimension to take the mean and median over here: so no need to specify `axis=0`, which is the default in any case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp[\"wtp_average\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp[\"wtp_average\"].median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate correlation coefficients\n",
    "\n",
    "We showed how to obtain a matrix of correlation coefficients for a number of variables in Python walkthrough 8.8. We use the same process here, storing the coefficients in an object called `M_corr`.\n",
    "\n",
    "WTP %>%\n",
    "  # Create the gender variable\n",
    "  mutate(gender = \n",
    "    as.numeric(ifelse(sex == \"female\", 0, 1))) %>%\n",
    "  select(WTP_average, education, gender,\n",
    "    climate, gov_intervention, pro_environment) %>%\n",
    "  cor(., use = \"pairwise.complete.obs\") -> M\n",
    "\n",
    "M[, \"WTP_average\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_corr = (\n",
    "    wtp.assign(gender=lambda x: np.where(x[\"sex\"] == \"female\", 0, 1))\n",
    "    .loc[\n",
    "        :,\n",
    "        [\n",
    "            \"wtp_average\",\n",
    "            \"education\",\n",
    "            \"gender\",\n",
    "            \"climate\",\n",
    "            \"gov_intervention\",\n",
    "            \"pro_environment\",\n",
    "        ],\n",
    "    ]\n",
    "    .corr()\n",
    ")\n",
    "\n",
    "M_corr[\"wtp_average\"].round(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 11.7\n",
    "\n",
    "**Summarising Dichotomous Choice (DC) variables**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create frequency table for DC_ref_outcome\n",
    "\n",
    "We can group by `costs` and `DC_ref_outcome` to obtain the number of observations for each combination of amount and vote response. We can also recode the voting options to ‘Yes’, ‘No’, and ‘Abstain’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recoding_dict = {\n",
    "    \"do not support referendum and no pay\": \"No\",\n",
    "    \"support referendum and pay\": \"Yes\",\n",
    "    \"would not vote\": \"Abstain\",\n",
    "}\n",
    "\n",
    "wtp_dc = (\n",
    "    wtp.dropna(subset=[\"costs\", \"DC_ref_outcome\"])\n",
    "    .assign(DC_ref_outcome=lambda x: x[\"DC_ref_outcome\"].map(recoding_dict))\n",
    "    .groupby([\"costs\", \"DC_ref_outcome\"])[\"id\"]\n",
    "    .count()\n",
    "    .unstack()\n",
    ")\n",
    "\n",
    "wtp_dc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add column showing proportion voting yes or no\n",
    "\n",
    "We can extend the table from Question 2(a) to include the proportion voting yes or no (to obtain percentages, multiply the values by 100). We *chain* methods in the below; using `assign` to create new columns and `round` to round the values in the float columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_dc = wtp_dc.assign(\n",
    "    total=lambda x: x[\"Abstain\"] + x[\"No\"] + x[\"Yes\"],\n",
    "    prop_no=lambda x: (x[\"Abstain\"] + x[\"No\"]) / x[\"total\"],\n",
    "    prop_yes=lambda x: x[\"Yes\"] / x[\"total\"],\n",
    ").round(2)\n",
    "\n",
    "wtp_dc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a line chart of WTP\n",
    "\n",
    "Using the dataframe generated for Questions 2(a) and (b) (`wtp_dcz`), we can plot the ‘demand curve’ as a scatterplot with connected points using **seaborn**. Adding the extra option `x=so.Continuous().tick(every=200)` under the scale option changes the default labelling on the horizontal axis to display ticks at every 200 euros, enabling us to read the chart more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn.objects as so\n",
    "\n",
    "(\n",
    "    so.Plot(wtp_dc, x=\"costs\", y=\"prop_yes\")\n",
    "    .add(so.Line())\n",
    "    .add(so.Dots())\n",
    "    .label(\n",
    "        x=\"Amount (Euros)\",\n",
    "        y=\"Fraction voting 'yes'\",\n",
    "    )\n",
    "    .scale(\n",
    "        x=so.Continuous().tick(every=200),\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 11.6 Demand curve (in euros), DC method.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate new proportions and add them to the table and chart\n",
    "\n",
    "It is straightforward to calculate new proportions and add them to the existing dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtp_dc = wtp_dc.assign(\n",
    "    total_ex=lambda x: x[\"No\"] + x[\"Yes\"],\n",
    "    prop_no_ex=lambda x: (x[\"No\"]) / x[\"total_ex\"],\n",
    "    prop_yes_ex=lambda x: x[\"Yes\"] / x[\"total_ex\"],\n",
    ").round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're going to plot them too. Because **seaborn** expects long-format data, we do need to re-orient the dataframe, however. We'll put the results of this long format data in a new dataframe called `demand_curve`. We'll clean this up a bit too: by renaming the ref outcome variable to `vote` and the entries in that variable to more relevant names for the chart. Finally, we'll plot the chart using lines and dots as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_curve = (\n",
    "    pd.melt(\n",
    "        wtp_dc.reset_index(), id_vars=[\"costs\"], value_vars=[\"prop_yes\", \"prop_yes_ex\"]\n",
    "    )\n",
    "    .rename(columns={\"DC_ref_outcome\": \"vote\"})\n",
    "    .assign(\n",
    "        vote=lambda x: x[\"vote\"].map(\n",
    "            {\"prop_yes\": \"counted as yes\", \"prop_yes_ex\": \"excluded\"}\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    so.Plot(demand_curve, x=\"costs\", y=\"value\", color=\"vote\")\n",
    "    .add(so.Line())\n",
    "    .add(so.Dots())\n",
    "    .label(\n",
    "        x=\"Amount (Euros)\",\n",
    "        y=\"Fraction voting 'yes'\",\n",
    "    )\n",
    "    .scale(\n",
    "        x=so.Continuous().tick(every=200),\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 11.7 Demand curve from DC respondents, under different treatments for 'Abstain' responses.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Walkthrough 11.8\n",
    "\n",
    "**Calculating confidence intervals for differences in means**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the difference in means, standard deviations, and number of observations\n",
    "\n",
    "We first create two vectors that will contain the `wtp` values for each of the two question methods. For the DC format, willingness to pay is recorded in the `costs` variable, so we select all observations where the `DC_ref_outcome` variable indicates the individual voted ‘yes’ and drop any missing observations. For the TWPL format we use the `wtp_average` variable that we created in Pythgon walkthrough 11.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_wtp = wtp.loc[\n",
    "    wtp[\"DC_ref_outcome\"] == \"support referendum and pay\", \"costs\"\n",
    "].dropna()\n",
    "\n",
    "dc_wtp.agg([\"mean\", \"std\", \"count\"]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twpl_wt = wtp.loc[~wtp[\"wtp_average\"].isna(), \"wtp_average\"].dropna()\n",
    "\n",
    "twpl_wt.agg([\"mean\", \"std\", \"count\"]).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate 95% confidence intervals\n",
    "\n",
    "Using the `ttest` function from the **pingouin** package to obtain 95% confidence intervals was covered in Python walkthroughs 8.10 and 10.6. As we have already separated the data for the two different question formats in Question 3(a), we can obtain the confidence interval directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.ttest(dc_wtp, twpl_wt, confidence=0.05)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate median WTP for the DC format\n",
    "\n",
    "In Python walkthrough 11.6 we obtained the median WTP for the TWPL format. We now obtain the WTP using the DC format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_wtp.median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter used the following packages where *sys* is the Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "26c4b888616894cf8360ee3d370b6a41732ef00c8f1d61e869c42a8428cf1ac1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
